{
  "questions": [
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What is MongoDB?",
      "explanation": "MongoDB is a document-based database that is highly scalable, and offers better performance."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Define a replica set?",
      "explanation": "The group of instances that host a similar data set is known as a replica set. Two nodes are present in a replica set, one is secondary and the other is primary, where data is replicated from the primary and sent to the secondary node.\n\n> _If you want to enrich your career and become a professional in **MongoDB**, then visit **Mindmajix** \\- a global **online training** platform: **\"[MongoDB Training](../../mongodb-training \"MongoDB Training\")\"** This course will help you to achieve excellence in this domain._"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What is the role of a profiler in MongoDB?",
      "explanation": "The role of a MongoDB profiler is to show the performance and analyze the characteristics of every operation of the database. By using the profiler, you will find all the queries which are slower than usual."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What are the different types of NoSQL databases? Give some examples.",
      "explanation": "The NoSQL database is classified into four basic types. The following is the classification of NoSQL database:\n\n*   Column store \n*   Document store \n*   Key-value store \n*   Graph base \n\nThe following are the few examples of NoSQL database\n\n*   MongoDB\n*   Cassandra\n*   CouchDB\n*   HBASE\n\n**Related Blog: \\[[MongoDB VS DynamoDB](../../mongodb-vs-dynamodb \"MongoDB VS DynamoDB\")\\]**"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What are the key features of MongoDB?",
      "explanation": "The following are the core features of MongoDB:\n\n*   High performance\n*   Automatic scaling\n*   Rich query language\n*   High availability"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What is the advantage of MongoDB?",
      "explanation": "The following are a few advantages of MongoDB database:\n\n*   SchemalesS\n*   Easy to scale-out\n*   No complex joins\n*   Structure of a single object is clear"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Which programming languages can be used with MongoDB?",
      "explanation": "MongoDB accepts all the programming languages. The following is a list of a few languages:\n\n*   C\n*   C++\n*   C#\n*   Java\n*   Node.js\n*   Perl\n*   PHP\n*   Python\n*   Ruby\n*   Scala\n*   Go\n*   Erlang.\n\n#### [How to create Database in MonogoDB?](../../mongodb-create-database)"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What is an embedded document?",
      "explanation": "Embedded documents specify the relationship between data that is written inside the body. The documents are received while the related data body is small."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Do a MongoDB database support foreign-key and primary-key relationship?",
      "explanation": "No, by default, MongoDB doesn't support foreign key or a primary key relationship."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Define the storage engine in MongoDB with an example.",
      "explanation": "A storage engine is a part of the database and is used to manage data storage on the disk. For instance, if there are two storage engines, one engine might offer support for read-heavy workloads, and another storage engine might offer higher-throughput for writing operations."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to move an old file into the moveChunk directory?",
      "explanation": "All the old files are converted into backup files and are moved into moveChunk directory at a time once the function is done."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How does MongoDB offer consistency?",
      "explanation": "To provide consistency, MongoDB makes use of reader-writer locks to allow readers to simultaneously access any collection like a database but it always offers private access to single writers."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What query is used in MongoDB to create and drop a collection?",
      "explanation": "The following are the queries used to create and drop a collection:\n\n    db.createCollection(name,options) // create a collection\n    db.collection.drop() // drop a collection"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "In MongoDB what is Objecld composed of?",
      "explanation": "The Objectld is composed of the following parameters:\n\n*   Client machine ID\n*   Timestamp\n*   3 byte incremented counter\n*   Client process ID"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What does sharding mean in MongoDB?",
      "explanation": "Sharding is a process of storing data records among one or more machines applied by MongoDB to meet the demands of data growth. It forms a horizontal partition in the database and each partition is known as database shard or a shard."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What is CRUD?",
      "explanation": "Mongodb offers best CRUD operations to perform better database operations. The following are the operations:\n\n*   Create\n*   Read\n*   Update\n*   Delete"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "In MongoDB, which command can be used to provide all information of a query plan?",
      "explanation": "The explain() command is used to provide information of all the query plans. The possible models are as follows:\n\n*   'queryPlanner', \n*   'executionStats'\n*   'allPlansExecution'.\n\nMore information about [MongoDB find queries](../../mongodb-find-queries)"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Define GridFS and its functionality in MongoDB?",
      "explanation": "In MongoDB, **GridFS** is a special specification for storing and retrieving files which exceed the BSON-document size limit (16MB). The major functionality of this grid is to divide a file into smaller segments, and stores each of those segments as a separate document instead of storing then into a single document."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "what command is used to create a MongoDB collection?",
      "explanation": "db.createCollection (name, options) is a command used to create collection MongoDB."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What feature in MongoDB is used to do safe backups?",
      "explanation": "To save backups of the old files “Journaling” feature is used in MongoDB databases."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What’s a good way to get a list of all unique tags for a collection of documents millions of items large when we doesn't have access to mongodb’s new “distinct” command ?",
      "explanation": "The normal way of doing tagging seems to be indexing multikeys. Even if your MongoDB driver doesn’t implement distinct, we can implement. \n\nIn JavaScript you can write something like this:\n\n    result = db.$cmd.findOne({“distinct” : “collection_name”, “key” : “tags”})\n\nYou do a findOne on the “$cmd” collection of whatever database you’re using. Pass it the collection name and the key you want to run distinct on.\n\nIf you ever need a command your driver doesn’t provide a helper for, you can look at the below link for a complete list of database commands.\n\nhttps://docs.mongodb.com/manual/reference/command/"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to get the names of all the keys in a MongoDB collection?",
      "explanation": "For example, from this:\ndb.things.insert( { type : \\[‘dog’, ‘cat’\\] } );\ndb.things.insert( { egg : \\[‘cat’\\] } );\ndb.things.insert( { type : \\[\\] } );\ndb.things.insert( { hello : \\[\\]  } );\nHow to get the unique keys: type, egg, hello\n\nWe could do this with MapReduce:\n\n    mr = db.runCommand({\n    “mapreduce” : “my_collection”,\n    “map” : function() {\n    for (var key in this) { emit(key, null); }\n    },\n    “reduce” : function(key, stuff) { return null; },\n    “out”: “my_collection” + “_keys”\n    })\n\nThen run distinct on the resulting collection so as to find all the keys:\n\n    db[mr.result].distinct(“_id”)\n    [“foo”, “bar”, “baz”, “_id”, …]"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to browse or query live MongoDB data?",
      "explanation": "Below is some utilities available in the market.\n\n*   MongoHub is moved to a native mac version, please check  [https://github.com/bububa/MongoHub-Mac](https://github.com/bububa/MongoHub-Mac).\n*   [https://github.com/Imaginea/mViewer](https://github.com/Imaginea/mViewer) This is awesome with tree and document views.\n*   genghisapp\n\nIt is a web-based GUI that is clean, light-weight, straight-forward, offers keyboard shortcuts, and works awesomely. It also supports GridFS.\n\nBest of all, it’s a single script.\n\n    To install it\n                $ gem install genghisapp bson_ext\n                (bson_ext is optional but will greatly improve the performance of the gui)\n    \n                To run it (this will automatically open your web browser and navigate to the app as well)\n                genghisapp\n    \n                To stop it\n                genghisapp –kill"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to use map/reduce to handle more than 10000 unique keys for grouping in MongoDB?",
      "explanation": "Use Version 2.2. The db.collection.group() method's returned array can contain at most 20,000 elements; i.e. at most 20,000 unique groupings. For group by operations that result in more than 20,000 unique groupings, use mapReduce. Previous versions had a limit of 10,000 elements."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Can anyone share some insight on the index/RAM relationship and what happens when both an individual index and all of my indexes exceed the size of available RAM?",
      "explanation": "MongoDB keeps what it can of the indexes in RAM. They’ll be swapped out on an LRU basis. You’ll often see documentation that suggests you should keep your “working set” in memory: if the portions of index you’re actually accessing fit in memory, you’ll be fine.\n\nIt is the working set size plus MongoDB’s indexes that should ideally reside in RAM at all times i.e. the amount of available RAM should ideally be at least the working set size plus the size of indexes plus what the rest of the OS (Operating System) and other software running on the same machine needs. If the available RAM is less than that, LRUing is what happens and we might therefore get significant slowdown.\n\nOne thing to keep in mind is that in an index btree buckets are cached, not individual index keys i.e. if we had a uniform distribution of keys in an index including for historical data, we might need more of the index in RAM compared to when we have a compound index on time plus something else. With the latter, keys in the same btree bucket are usually from the same time era, so this caveat does not happen. Also, we should keep in mind that our field names in BSON are stored in the records (but not the index) so if we are under memory pressure they should be kept short.\n\nMongoDB Interview Questions and Answers for Experienced \n--------------------------------------------------------"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What does an appropriate db.ClockTime.update() statement look like to convert these text based values to a date datatype?",
      "explanation": "This code should do it:\n\n    > var cursor = db.ClockTime.find()\n    > while (cursor.hasNext()) {\n    … var doc = cursor.next();\n    … db.ClockTime.update({_id : doc._id}, {$set : {ClockInTime : new Date(doc.ClockInTime)}})\n    … }\n    I have exactly the same situation as Jeff Fritz.\n    In my case I have succeed with the following simpler solution:\n    db.ClockTime.find().forEach(function(doc) {\n    doc.ClockInTime=new Date(doc.ClockInTime);\n    db.ClockTime.save(doc);\n    })"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Updating a specific key/value inside of an array field with MongoDB",
      "explanation": "As a preface, I’ve been working with MongoDB for about a week now, so this may turn out to be a pretty simple answer.\n\nI have data already stored in my collection, we will call this collection content, as it contains articles, news, etc. Each of these articles contains another array called author which has all of the author’s information (Address, Phone, Title, etc).\n\n**The Goal –** I am trying to create a query that will update the author’s address on every article that the specific author exists in, and only the specified author block (not others that exist within the array).\n\nSort of a “Global Update” to a specific article that affects his/her information on every piece of content that exists.\n\nHere is an example of what the content with the author looks like.\n\n    {\n    “_id” : ObjectId(“4c1a5a948ead0e4d09010000”),\n    “authors” : [\n    {\n    “user_id” : null,\n    “slug” : “joe-somebody”,\n    “display_name” : “Joe Somebody”,\n    “display_title” : “Contributing Writer”,\n    “display_company_name” : null,\n    “email” : null,\n    “phone” : null,\n    “fax” : null,\n    “address” : null,\n    “address2” : null,\n    “city” : null,\n    “state” : null,\n    “zip” : null,\n    “country” : null,\n    “image” : null,\n    “url” : null,\n    “blurb” : null\n    },\n    {\n    “user_id” : null,\n    “slug” : “jane-somebody”,\n    “display_name” : “Jane Somebody”,\n    “display_title” : “Editor”,\n    “display_company_name” : null,\n    “email” : null,\n    “phone” : null,\n    “fax” : null,\n    “address” : null,\n    “address2” : null,\n    “city” : null,\n    “state” : null,\n    “zip” : null,\n    “country” : null,\n    “image” : null,\n    “url” : null,\n    “blurb” : null\n    },\n    ],\n    “tags” : [\n    “tag1”,\n    “tag2”,\n    “tag3”\n    ],\n    “title” : “Title of the Article”\n    }\n\nI can find every article that this author has created by running the following command:\n\n    db.content.find({authors: {$elemMatch: {slug: ‘joe-somebody’}}});\n\nSo theoretically I should be able to update the author's record for the slug joe-somebody but notjane-somebody (the 2nd author), I am just unsure exactly how you reach in and update every record for that author.\n\nI thought I was on the right track, and here’s what I’ve tried.\n\n    {\n    “_id” : ObjectId(“4c1a5a948ead0e4d09010000”),\n    “authors” : [\n    {\n    “user_id” : null,\n    “slug” : “joe-somebody”,\n    “display_name” : “Joe Somebody”,\n    “display_title” : “Contributing Writer”,\n    “display_company_name” : null,\n    “email” : null,\n    “phone” : null,\n    “fax” : null,\n    “address” : null,\n    “address2” : null,\n    “city” : null,\n    “state” : null,\n    “zip” : null,\n    “country” : null,\n    “image” : null,\n    “url” : null,\n    “blurb” : null\n    },\n    {\n    “user_id” : null,\n    “slug” : “jane-somebody”,\n    “display_name” : “Jane Somebody”,\n    “display_title” : “Editor”,\n    “display_company_name” : null,\n    “email” : null,\n    “phone” : null,\n    “fax” : null,\n    “address” : null,\n    “address2” : null,\n    “city” : null,\n    “state” : null,\n    “zip” : null,\n    “country” : null,\n    “image” : null,\n    “url” : null,\n    “blurb” : null\n    },\n    ],\n    “tags” : [\n    “tag1”,\n    “tag2”,\n    “tag3”\n    ],\n    “title” : “Title of the Article”\n    }\n\nSolution:\n\nThis is what finally worked for me!\n\n    db.content.update({‘authors.slug’:’joe-somebody’},{$set:{‘authors.$.address’:’Address That I wanted’}},false,true);\n\nIt updates all the records properly, thanks!\n\nMaybe you can use the $ operator (positional-operator)?"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB – simulate join or subquery",
      "explanation": "I’m trying to figure out the best way to structure my data in Mongo to simulate what would be a simple join or subquery in SQL.\n\nSay I have the classic Users and Posts example, with Users in one collection and Posts in another. I want to find all posts by users who’s city is “london”.\n\nI’ve simplified things in this question, in my real world scenario storing Posts as an array in the User document won’t work as I have 1,000’s of “posts” per user constantly inserting."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Can Mongos $in operator help here? Can $in handle an array of 10,000,000 entries?",
      "explanation": "Honestly, if you can’t fit “Posts” into “Users”, then you have two options.\n\n1.  Denormalize some User data inside of posts. Then you can search through just the one collection.\n2.  Do two queries. (one to find users the other find posts)\n\nBased on your question, you’re trying to do #2.\n\nTheoretically, you could build a list of User IDs (or refs) and then find all Posts belonging to a User$in that array. But obviously that approach is limited."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Can $in handle an array of 10,000,000 entries?",
      "explanation": "Look, if you’re planning to “query” your posts for all users in a set of 10,000,000 Users you are well past the stage of “query”. You say yourself that each User has 1,000s of posts so you’re talking about a query for “Users with Posts who live in London” returning 100Ms of records.  \n**100M records isn’t a query, that’s a dataset!**\n\nIf you’re worried about breaking the $in command, then I highly suggest that you use map/reduce. The Mongo Map/Reduce will create a new collection for you. You can then trim down or summarize this dataset as you see fit.\n\n$in can handle 100,000 entries. I’ve never tried 10,000,000 entries but the query (a query is also a document) has to be smaller than 4mb (like every document) so 10,0000,0000 entries isn’t possible.\n\nWhy don’t you include the user and its town in the Posts collection? You can index this town because you can index properties of embedded entities. You no longer have to simulate a join because you can query the Posts on the towns of its embedded users.\n\nThis means that you have to update the Posts when the town of a user changes but that doesn’t happen very often. This update will be fast if you index the UserId in the Posts collection."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongo complex sorting?",
      "explanation": "I know how to sort queries in MongoDB by multiple fields, e.g.,db.coll.find().sort({a:1,b:-1})."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Can I sort with a user-defined function; e.g., supposing a and b are integers, by the difference between a and b (a-b)?",
      "explanation": "I don’t think this is possible directly; the sort documentation certainly doesn’t mention any way to provide a custom compare function.\n\nYou’re probably best off doing the sort in the client, but if you’re really determined to do it on the server you might be able to use db.eval() to arrange to run the sort on the server (if your client supports it).\n\n    Server-side sort:\n    db.eval(function() {\n    return db.scratch.find().toArray().sort(function(doc1, doc2) {\n    return doc1.a – doc2.a\n    })\n    });\n    \n    Versus the equivalent client-side sort:\n    db.scratch.find().toArray().sort(function(doc1, doc2) {\n    return doc1.a – doc2.b\n    });\n\nNote that it’s also possible to sort via an aggregation pipeline and by the $orderby operator (i.e. in addition to .sort()) however neither of these ways lets you provide a custom sort function either. Why don’t create the field with this operation and sort on it ?\n\n**Related Page: [MongoDB Vs PostgreSQL Comparison](../../mongodb-vs-postgresql)**"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to set a primary key in MongoDB?",
      "explanation": "I want to set one of **my fields as the primary key.** I am using MongoDB as my NoSQL.  \nThe \\_id field is reserved for primary key in mongodb, and that should be an unique value. If you don’t set anything to \\_id it will automatically fill it with “MongoDB Id Object”. But you can put any unique info into that field. Additional info: [https://www.mongodb.org/display/DOCS/BSON](https://www.mongodb.org/display/DOCS/BSON) Hope it helps."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Delete everything in a MongoDB database",
      "explanation": "I’m doing development on MongoDB. For totally non-evil purposes, I sometimes want to blow away everything in a databasethat is, to delete every single collection, and whatever else might be lying around, and start from scratch. Is there a single line of code that will let me do this? Bonus points for giving both a MongoDB console method and a MongoDB Ruby driver method.\n\n    use [database];\n    db.dropDatabase();\n    Ruby code should be pretty similar.\n    Also, from the command line:\n    mongo [database] –eval “db.dropDatabase();”Use\n    [databaseName]\n    db.Drop+databaseName();\n    drop collection\n    use databaseName\n    db.collectionName.drop();"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to $set sub-sub-array items in MongoDB",
      "explanation": "I’m developing a webapp which has a portal-ish component to it (think like multiple panels that can be drug around from column to column and added or removed). I’m using MongoDB to store this info with a format like so…  \nSkip code block\n\n    {\n    _id: ObjectId(…),\n    title: ‘My Layout’,\n    columns: [\n    {\n    order: 1,\n    width: 30,\n    panels: [\n    { title: ‘Panel Title’, top: 100, content: ‘…’ },\n    { title: ‘Panel Title’, top: 250, content: ‘…’ },\n    ]\n    },\n    {\n    … multiple columns …\n    }\n    ]\n    }\n    \n    I’m attempting to use atomic/modifier operations with update() and this is getting confusing. If I wanted to just update one property of a specific panel, how do I reference that?\n    update(\n    { _id: ObjectId(…) },\n    { $set: { columns.[???].panels.[???].top: 500 }\n    )\n    \n    If you know the index in the array you can access the array element directly using dot notation.\n    update(\n    { _id: ObjectId(xxxx) },\n    { $set: { ‘columns.0.panels.0.top’ : 125}}\n    )\n\nMake sure you encase the dot notated path in quotes as a string.\n\n**Edit:**\n\nTo give more detail on how this could work dynamically, I’ll give an example in PHP:\n\n    $action = array(“columns.$colNum.panels.$panelNum” => $newValue);\n\nYes there is the positional operator, but it does not appear to be advanced enough to change arrays within arrays, this may change in MongoDB 1.7.0  \nThere is an alternative you can do instead of trying to stuff this information into a nested document. Try to flatten it out. You can create a collection that has panel & column objects:  \ncolumn object:\n\n    {\n    _id: // MongoId\n    type: ‘column’,\n    user: ‘username’,\n    order: 1,\n    width: 30,\n    }\n    panel object:\n    {\n    _id: //MongoId\n    type: ‘panel’,\n    user: ‘username’,\n    parentColumn: //the columns _id string\n    top: 125,\n    left: 100\n    }\n\nThen you can find all columns that belong to a user by doing:\n\n    find({ type: ‘column’, user:’username’});\n    You can find all panels for a specific column by doing:\n    find({type: ‘panel’, columnOwner:’ownerID’});\n    Since each column and panel will have a unique ID given by MongoDB to it, you can easily query and atomically set options.\n    update({‘_id’: ObjectId(‘idstring’)}, {$set : { ‘top’ : 125}});"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Some beginner’s questions about MongoDB",
      "explanation": "I’m a beginner with MongoDB and I’ve some questions:\n\n1.  When I’m connected to Mongo, and i executeshow dbs I see 2 databases: admin and local. What’s their role? Then if I execute an insert command likedb.foo.insert({“value”:”mongo”}), the test database appears. Why? How can i specify a custom name for a database?\n2.  Withshow dbs I get the databases (somehow like show databases in sql), how can I then list the collections inside a database (I would use show tables in sql)?\n3.  When executing a command, the MongoDB tutorial always uses thedb object. Is it the main object (a sort of “connection” object) that has to used for executing commands or it’s something else?\n\nThanks!\n\n*   Adminand local contain various settings local to the server, like users who are authenticated to connect. Under beginner usage, you shouldn’t need to worry about them at all. By default you connect to a database named test. To connect to a new database, just use databasename from the mongo command line, or mongo databasename from your OS shell.\n*   Use \\[database\\_name\\]and then show collections\n*   Thedb object is your root handle to the currently-selected database on the mongo command line. The command line is really just a Javascript command line, and there are various mongodb-specific objects and functions exposed that let you do stuff. Try help() for a full listing.\n\n**Related Page: [MongoDB GUI - Top 7 MongoDB GUI Tools](../../mongodb-gui-tools)**"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to update based on existing data in mongo",
      "explanation": "Sort of a mongo noob, and I have a feeling I am tackling this problem from the wrong direction.\n\nI have about a 7 million document collection. Each document has two fields that I want to modify(not replace), basically they are big strings that have , and I want to replace those with n.  \nI spent about an hour trying to find a way to “backreference” the object returned by the query, which totally doesn’t exist. What is the best approach for something like this?  \nYou’ll have to query for all the documents and update them one by one. If you do it in JavaScript, it would be something like:\n\n    mydb = db.getSisterDB(“whateverDBYoureUsing”);\n    var cursor = mydb.foo.find();\n    while (cursor.hasNext()) {\n    var x = cursor.next();\n    /* replace with n in x’s strings … */\n    db.foo.update({_id : x._id}, x);\n    }\n\nYou can copy this into a .js file (say, replace.js), change the db and collection names, and run it as a script from the shell:  \nmongo replace.js"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to print out more than 20 items (documents) in MongoDB’s shell?",
      "explanation": "db.foo.find().limit(300)\n    won’t do it… it still prints out 20\n    db.foo.find().toArray()\n    db.foo.find().forEach(printjson)\n\nwill both print out very expanded view of each document instead of the 1-line version for find():  \nDBQuery.shellBatchSize = 300  \nwill do.  \nMongoDB Docs – Getting Started with the mongo Shell – Executing Queries  \nYou can use it inside of the shell to iterate over the next 20 results. Just type it if you see “has more” and you will see the next 20 items.  \nfrom the shell if you want to show all results you could do db.collection.find().toArray() to get all results without it  \nCould always do:\n\n    db.foo.find().forEach(function(f){print(tojson(f, ”, true));});\n    To get that compact view.\n    Also, I find it very useful to limit the fields returned by the find so:\n    db.foo.find({},{name:1}).forEach(function(f){print(tojson(f, ”, true));});\n\nwhich would return only the \\_id and name field from foo."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to store timestamps? Are created and updated fields available automatically?",
      "explanation": "What’s the best way to store timestamps in MongoDB?\n\nWhich format is best:\n\n    # “created”: { “d” : “2010-03-29”, “t” : “20:15:34” }\n    # “created”: “12343545234” # seconds since epoc\n    # “created”: “2010-03-14T21:20:14+0000”\n\nIs there a way to have MongoDB automatically setcreated and updated fields?\n\nWhich format is best  \nBest for what?  \nIs there a way to have MongoDB automatically set created and updated fields?  \nCreated time is in the ObjectId but, as far as I know, you will have to update a updated field manually.\n\n**Example:**\n\n    ObjectId(“538141a9615760fd04ffef5f”).getTimestamp()\n\n1\\. The format you need to process it with best performance in your application should be preferred. Note that as default every document in MongoDB gets a created timestamp\n\n(https://www.mongodb.org/display/DOCS/Object+IDs#ObjectIDs-DocumentTimestamps)\n\n2\\. See 1) + I think you need to manually set the “update” field.  \nIf you do following on mongo shell it shows you time stamp that represents when that documents inserted using mongoId. For ex. ObjectId(“51f3dee5ee49f9b91e0db133”).getTimestamp(), then it returns ISODate."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB: Updating documents using data from the same document",
      "explanation": "I have a list of documents, each with lat and lon properties (among others).\n\n    { ‘lat’: 1, ‘lon’: 2, someotherdata […] }\n    { ‘lat’: 4, ‘lon’: 1, someotherdata […] }\n    […]\n    I want to modify it so that it looks like this:\n    { ‘coords’: {‘lat’: 1, ‘lon’: 2}, someotherdata […]}\n    { ‘coords’: {‘lat’: 4, ‘lon’: 1}, someotherdata […]}\n    […]\n\nSo far I’ve got this:\n\n    db.events.update({}, {$set : {‘coords’: {‘lat’: db.events.lat, ‘lon’: db.events.lon}}}, false, true)\n\nBut it treats the db.events.lat and db.events.lon as strings. How can I reference the document’s properties? Cheers. The $rename operator (introduced a month after this question was posted) makes it really easy to do these kinds of things where you don’t need to modify the values.\n\n**Insert some test documents**\n\n    db.events.insert({ ‘lat’: 1, ‘lon’: 2, someotherdata: [] })\n    db.events.insert({ ‘lat’: 4, ‘lon’: 1, someotherdata: [] })\n\n  \n**use the $rename operator**\n\n    db.events.update({}, {$rename: {‘lat’: ‘coords.lat’, ‘lon’: ‘coords.lon’}}, false, true)\n\n  \n**Results**\n\n    Skip code block\n    db.events.find()\n    {\n    “_id” : ObjectId(“5113c82dd28c4e8b79971add”),\n    “coords” : {\n    “lat” : 1,\n    “lon” : 2\n    },\n    “someotherdata” : [ ]\n    }\n    {\n    “_id” : ObjectId(“5113c82ed28c4e8b79971ade”),\n    “coords” : {\n    “lat” : 4,\n    “lon” : 1\n    },\n    “someotherdata” : [ ]\n    }\n\n**Update**: If all you have to do is change the structure of a document without changing the values, see gipset’s answer for a nice solution.  \nAccording to a (now unavailable) comment on the Update documentation page, you cannot reference the current document’s properties from within an update().  \nYou’ll have to iterate through all the documents and update them like this:\n\n    Skip code block\n    db.events.find().snapshot().forEach(\n    function (e) {\n    // update document, using its own properties\n    e.coords = { lat: e.lat, lon: e.lon };\n    // remove old properties\n    delete e.lat;\n    delete e.lon;\n    // save the updated document\n    db.events.save(e);\n    }\n    )\n\nSuch a function can also be used in a map-reduce job or a server-side db.eval() job, depending on your needs."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "In MongoDB how do you use $set to update a nested value?",
      "explanation": "In MongoDB how do you use $set to update a nested value?\n\nUsing the dot notation:\n\n    db.people.update({ }, { $set: { “address.street”: “Main Street” } })"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongorestore of a db causing me trouble",
      "explanation": "I’m new to MongoDB and I have hard time to backup my local DB and restore it on my server. I found the link on Mongo’s website:\n\nhttps://www.mongodb.org/display/DOCS/Import+Export+Tools but I still have problems with the restore.\n\nWhen I do my backup I call  \nmongodump –db Gen  \nThen I see that all the collections are dump in /bin/dump/Gen folder  \nI copy-paste from local to the server in the same folder the call  \nmongorestore –db Gen –drop –dbpath dump/Gen  \nBut I get the following : Error : root directory must be a dump of a single database when specifying a db name with –db  \nWhat am I doing wrong?  \nThanks for the help!  \nOk I find out what I’m doing wrong :  \nI was doing  \nmongorestore –db Gen –drop –dbpath dump/Gen  \nBut without the –dbpath it works just fine!  \nmongorestore –db Gen –drop dump/Gen  \nThanks everyone!\n\nMongoDB advanced interview questions and answers\n------------------------------------------------"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to create user accounts in MongoDB?",
      "explanation": "I wonder what’s ‘correct’ way to create user accounts in MongoDB and actions like register/login. Do I have to create a specific collection for users (Username,Email,Password) or MongoDB has something built-in already for users?\n\nIf I have to create users collection manually, how to deal with password encryption? Thanks.\n\nYou’ll probably have to create and manage the collection of users manually.\n\nAs for encrypting passwords, the common approach is to hash the passwords using a suitable hash-function before you store them in the db. Later, when a user tries to login you use the same hash-function on the string they entered in the password field and compare that result to the password entry in your db. That way you never store the actual passwords and if someone hacks you they won’t get your users passwords, just their hashes.\n\nFor extra security (against dictionary attacks) you should also salt the hashed passwords. Read about it here"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How update the \\_id of one MongoDB Document?",
      "explanation": "want update an \\_id MongoDB of one document. I know it’s not a really good pratice. But with some technical reason, I need update it. But If I try to update it I have :\n\n    > db.clients.update({‘_id’:ObjectId(“4cc45467c55f4d2d2a000002”)}, {‘$set’:{‘_id’:ObjectId(“4c8a331bda76c559ef000004”)}});\n\nMod on \\_id not allowed  \nAnd the update is not made. How I can really update it ?  \nYou cannot update it. You’ll have to save the document using a new \\_id, and then remove the old document.\n\nSkip code block\n\n    // store the document in a variable\n    doc = db.clients.findOne({_id: ObjectId(“4cc45467c55f4d2d2a000002”)})\n    // set a new _id on the document\n    doc._id = ObjectId(“4c8a331bda76c559ef000004”)\n    // insert the document, using the new _id\n    db.clients.insert(doc)\n    // remove the document with the old _id\n    db.clients.remove({_id: ObjectId(“4cc45467c55f4d2d2a000002”)})\n    To do it for your whole collection you can also use a loop (based on Niels example):\n    db.status.find().forEach(function(doc){ var id=doc._id; doc._id=doc.UserId; db.status.insert(doc); db.status.remove({_id:id}); })\n\nIn this case UserId was the new ID I wanted to use\n\n**Related Page: [MongoDB Query & Examples](../../mongodb-query-and-examples)**"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Get MongoDB Databases in a Javascript Array?",
      "explanation": "I know that in the MongoDB terminal, I can run “show dbs” to see the available databases. I want to get them programmatically so that I can iterate over them and delete some based upon a regular expression.  \nI have tried db.runCommand(“show dbs”) but that doesn’t work.  \nThanks in advance.\n\n    > db.getMongo().getDBNames()\n    [\n    “test”,\n    “admin”,\n    “local”\n    ]\n    > db.getMongo().getDBNames\n    function () {\n    return this.getDBs().databases.map(function (z) {return z.name;});\n    }\n\nBased upon this answer\n\nI was able to code up a solution.\n\n    use admin\n    \n    dbs = db.runCommand({listDatabases: 1})\n    dbNames = []\n    for (var i in dbs.databases) { dbNames.push(dbs.databases[i].name) }\n    Hopefully this will help someone else.\n    The below will create an array of the names of the database:\n    var connection = new Mongo();\n    var dbNames = connection.getDBNames();"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB and “joins”",
      "explanation": "I’m sure MongoDB doesn’t officially support “joins”. What does this mean? Does this mean “We cannot connect two collections(tables) together.”?  \nI think if we put the value for \\_id in collection A to the other\\_id in collection B, can we simply connect two collections?  \nIf my understanding is correct, MongoDB can connect two tables together, say, when we run a query. This is done by “Reference” written in\n\n[https://www.mongodb.org/display/DOCS/Schema+Design](https://www.mongodb.org/display/DOCS/Schema+Design).\n\nThen what does “joins” really mean?  \nI’d love to know the answer because this is essential to learn MongoDB schema design.\n\n[https://www.mongodb.org/display/DOCS/Schema+Design](https://www.mongodb.org/display/DOCS/Schema+Design)\n\nIt’s no join since the relationship will only be evaluated when needed. A join (in a SQL database) on the other hand will resolve relationships and return them as if they were a single table (you “join two tables into one”). You can read more about DBRef here:\n\n[https://docs.mongodb.org/manual/applications/database-references/](https://docs.mongodb.org/manual/applications/database-references/)\n\nThere are two possible solutions for resolving references. One is to do it manually, as you have almost described. Just save a document’s \\_id in another document’s other\\_id, then write your own function to resolve the relationship. The other solution is to use DBRefs as described on the manual page above, which will make MongoDB resolve the relationship client-side on demand. Which solution you choose does not matter so much because both methods will resolve the relationship client-side (note that a SQL database resolves joins on the server-side).\n\nThe database does not do joins — or automatic “linking” between documents. However you can do it yourself client side. If you need to do 2, that is ok, but if you had to do 2000, the number of client/server turnarounds would make the operation slow.\n\nIn MongoDB a common pattern is embedding. In relational when normalizing things get broken into parts. Often in mongo these pieces end up being a single document, so no join is needed anyway. But when one is needed, one does it client-side.\n\nConsider the classic ORDER, ORDER-LINEITEM example. One order and 8 line items are 9 rows in relational; in MongoDB we would typically just model this as a single BSON document which is an order with an array of embedded line items. So in that case, the join issue does not arise. However the order would have a CUSTOMER which probably is a separate collection – the client could read the cust\\_id from the order document, and then go fetch it as needed separately.\n\nThere are some videos and slides for schema design talks on the mongodb.org web site I belive.\n\nThe first example you link to shows how MongoDB references behave much like lazy loading not like a join. There isn’t a query there that’s happening on both collections, rather you query one and then you lookup items from another collection by reference.\n\n### Mongo interface"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What are some GUIs to use with Mongo, and what features do they offer? I’m looking for facts here, not opinions on which interface is best.",
      "explanation": "**Official List from MongoDB**\n\n[https://docs.mongodb.com/ecosystem/](https://docs.mongodb.com/ecosystem/)\n\n**Web Based**\n\nFor PHP, I’d recommend Rock Mongo. Solid, lots of great features, easy setup.\n\n[https://github.com/iwind/rockmongo](https://github.com/iwind/rockmongo)\n\nIf you don’t want to install anything … you can use MongoHQ’s web interface (even if your MongoDB isn’t on MongoHQ - which is now named as Compose.)\n\n[https://www.compose.com/](https://www.compose.com/)\n\n**Mac OS X**\n\nWhile MongoHub had been a decent option for a while it’s bugs make it virtually unusable at this point …  \nThere is a more up-to-date (and less buggy) fork of the MongoHub project available:\n\nhttps://github.com/fotonauts/MongoHub-Mac you can download a binary here.\n\n**Windows**\n\nBy far, the best UI (for Windows) currently out there is MongoVUE.\n\n[https://blog.mongovue.com/](https://blog.mongovue.com/)\n\nLooks great, lots of features, and if you are new it will really help you get going.\n\n[https://blog.mongovue.com/features/](https://blog.mongovue.com/features/)\n\nHere’s a Q&A with the author too if you are interested\n\nOn Mac there is MongoHub. On Windows you could try MongoVUE.  \nAlso see Do any visual tools exist for MongoDB (for Windows)?  \nScreenshot of MongoHub:  \nHere’s the official page of Admin UIs.  \nI have not really used any of them. But it looks like there is quite a bit of coverage there.\n\n**Web**\n\nAt the shop where I work we use the Prudence platform for some stuff, and also MongoDB, so we of course use MongoVision a lot. Browser based, tabbed collection views, pretty-printed document editor, and three themes OOB. Open source.\n\n[https://code.google.com/p/mongo-vision/](https://code.google.com/p/mongo-vision/)\n\n**OS X**\n\nMongoHub was as reliable as MongoVision.\n\nhttps://mongohub.todayclose.com/"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Why not MongoDB?",
      "explanation": "I recently used MongoDB for the first time and found it exceptionally easy to use and high-performing. Which leads to my question – why not MongoDB?  \nLets say I am implementing a Q & A app. My approach would be to implement the User data in a MySQL database and then use MongoDB for the question and answer storage – one collection storing a question and all responses."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Is there anything wrong with this approach?",
      "explanation": "MongoDB sounds like a fine application for your problem, but there are plenty of reasons why you would not use it.\n\nMongoDB would not be well suited for applications that need:\n\n1.  Multi-Object Transactions: MongoDB only supports ACID transactions for a single document.\n2.  SQL: SQL is well-known and a lot of people know how to write very complex queries to do lots of things. This knowledge is transferrable across a lot of implementations where MongoDB’s queries language are specific to it.\n3.  Strong ACID guarantees: MongoDB allows for things like inconsistent reads which is fine in some applications, but not in all.\n4.  Traditional BI: A lot of very powerful tools exist that allow for OLAP and other strong BI applications and those run against traditional SQL database.\n\n**Possible downsides:**\n\n**Ans:**\n\n1.  You work in an organization that has only used SQL relational databases. You have no approval or support for using a NoSQL database yet.\n2.  You’ve never administered a MongoDB cluster; there’s a learning curve, as with all technologies.\n3.  Your data is really relational (e.g., one User has many Questions; a Question has many Answers), and you’ve overlooked the possibility.\n\nMondoDB is a fine solution, a good alternative for those situations where it applies. If you can use it, why not?  \nMongoDB is a brilliant database and I enjoy using it. That said, it has a few gotchas if you come from the world of SQL.  \nApart from ACID and other things that are well documented (and in other answers too), these things have caught us by surprise:\n\n1\\. MongoDB expects you to have memory. **Lots of memory.** If you can’t fit your working set in memory, you can forget about it. This is different from most relational DBs which use memory only as cache! **To be more specific:** MongoDB uses RAM as primary storage and “swaps” the unneeded parts out to disk (Mongo leaves the decision over which parts get “swapped” to kernel). Traditional RDBMS work the other way around – they use disk as primary storage and use RAM as caching mechanism. So in general MongoDB uses more RAM. This is not a bad thing by itself, but as a consequence “real” RAM consumption is difficult to predict, which can lead to serious and unexpected degradation of performance once the working set grows over the (hard to predict) limit.\n\n2\\. **Storage does not auto-shrink** when you remove records. The space that is allocated per collection stays allocated until you either repair DB or drop the collection. And it is allocated in huge chunks on a DB level (data files), which are then allocated to collections when needed (extents). That said, inside the collection’s allocated space the documents that are removed DO release their space for other documents in the same collection. This is a good explanation of concepts: [https://www.10gen.com/presentations/storage-engine-internals](https://www.10gen.com/presentations/storage-engine-internals)\n\n3\\. As a contrast to SQL which is parsed server-side, in Mongo you pass the data structures to query and CRUD functions. The consequence is that **each driver provides a different syntax**, which is a bit annoying. For instance, PyMongo uses a list of tuples instead of a dictionary (probably because dict in Python does not preserve order of keys) to specify which fields will be returned byfind(): (to be fair, that was probably the only sane way to do it – but it is a consequence of not using string-based language such as SQL)  \na.   MongoDB shell: db.test.find({}, {a:1})  \nb.   PyMongo: db.find({}, fields=\\[(a,1,)\\]\n\nThis should not be viewed as a criticism of MongoDB – I enjoy using it and it has proven to be a reliable and performant tool. But to use it properly you need to learn about its space management."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Track MongoDB performance?",
      "explanation": "Is there a way to track ‘query’ performance in MongoDB? Specially testing indexes or subdocuments?  \nIn sql you can run queries, see execution time and other analytic metrics.  \nI have a huge mongoDB collection and want to try different variations and indexes, not sure how to test it, would be nice to see how long did it take to find a record.. (I am new in MongoDB). Thanks\n\nThere are two things here that you’ll likely be familiar with.  \n1\\. Explain plans  \n2\\. Slow Logs\n\n**Explain Plans**\n\nHere are some basic docs on explain. Running explain is as simple asdb.foo.find(query).explain(). (note that this actually runs the query, so if your query is slow this will be too)  \nTo understand the output, you’ll want to check some of the docs on the slow logs below. You’re basically given details about “how much index was scanned”, “how many are found”, etc. As is the case with such performance details, interpretation is really up to you. Read the docs above and below to point you in the right direction.\n\n**Slow Logs**\n\nBy default, slow logs are active with a threshold of 100ms. Here’s a link to the full documentation on profiling. A couple of key points to get you started:  \nGet/Set profiling:\n\n    db.setProfilingLevel(2); // 0 => none, 1 => slow, 2 => all\n    db.getProfilingLevel();\n    See slow queries:\n    db.system.profile.find()"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongodb can’t start",
      "explanation": "today I updated my Mongo.. mongodb-stable (from 10gen repo)  \nbut my service has down. the following command not working\n\n    $ sudo service mongodb start\n    $ start: Unknown job: mongodb\n\neven this command not working\n\n    $ sudo /etc/init.d/mongodb start\n    $ Rather than invoking init scripts through /etc/init.d, use the service(8)\n    utility, e.g. service mongodb start\n\nSince the script you are attempting to invoke has been converted to an  \nUpstart job, you may also use the start(8) utility, e.g. start mongodb  \nstart: Unknown job: mongodb  \nthere is no mongo process running\n\n    $ ps -ef|grep mongo\n    $ user  9689  8121  0 13:01 pts/1    00:00:00 grep –color=auto mongo\n\nlog is here  \nSkip code block\n\n    tail /var/log/mongodb/mongodb.log\n    Fri Dec 10 11:24:35 [conn4] end connection 127.0.0.1:54217\n    Fri Dec 10 11:25:35 [initandlisten] connection accepted from 127.0.0.1:54229 #5\n    Fri Dec 10 11:26:25 [initandlisten] connection accepted from 127.0.0.1:54243 #6\n    Fri Dec 10 11:26:30 [conn6] end connection 127.0.0.1:54243\n    Fri Dec 10 11:30:13 got kill or ctrl c or hup signal 15 (Terminated), will terminate after current cmd ends\n    Fri Dec 10 11:30:13 [interruptThread] now exiting\n    Fri Dec 10 11:30:13 dbexit:\n    Fri Dec 10 11:30:13 [interruptThread] shutdown: going to close listening sockets…\n    Fri Dec 10 11:30:13 [interruptThread] closing listening socket: 5\n    Fri Dec 10 11:30:13 [interruptThread] closing listening socket: 6\n    Fri Dec 10 11:30:13 [interruptThread] closing listening socket: 7\n    Fri Dec 10 11:30:13 [interruptThread] closing listening socket: 8\n    Fri Dec 10 11:30:13 [interruptThread] shutdown: going to flush oplog…\n    Fri Dec 10 11:30:13 [interruptThread] shutdown: going to close sockets…\n    Fri Dec 10 11:30:13 [interruptThread] shutdown: waiting for fs preallocator…\n    Fri Dec 10 11:30:13 [interruptThread] shutdown: closing all files…\n    Fri Dec 10 11:30:13     closeAllFiles() finished\n    Fri Dec 10 11:30:13 [interruptThread] shutdown: removing fs lock…\n    Fri Dec 10 11:30:13 dbexit: really exiting now\n    for now, I’m running Mongo through this command just for a while, creating process manually\n    $ sudo mongod -f /etc/mongodb.conf\n\nany idea? or has anyone updated Mongo-stable via Update manager?\n\n**Edits**:\n\nThis mongodb version was v1.6.5 and it seems mongo team released it as stable with a bug. And they fixed it immediately at v1.7.4. You can see the major priority issue.  \nBug has been reported and fixed.\n\n    https://jira.mongodb.org/browse/SERVER-2200\n    $ sudo apt-get purge mongodb-stable\n    $ sudo apt-get install mongodb-stable\n    (remove the lock file if present in /var/lib/mongodb)\n    $ sudo init 6\n    Then edit /etc/init/mongodb.conf removing the line “limit nofile 20000”\n    $ sudo vi /etc/init/mongodb.conf\n    $ sudo service mongodb start\n    mongodb start/running, process 2351\n    Worked.\n\nRunning **Ubuntu 11.10** confirm you have the latest version of MongoDB:\n\n    $ mongod –version\n    db version v2.2.0\n\n(If you don’t have the latest version of MongoDB, follow the **Ubuntu Installation instructions** at \n\nFirst confirm that the **mongodb** user/group has permission to write to the data directory:\n\n    $ sudo chown -R mongodb:mongodb /var/lib/mongodb/.\n    Start up MongoDB as a Daemon (background process) using the following command:\n    $ mongod –fork –dbpath /var/lib/mongodb/ –smallfiles –logpath /var/log/mongodb.log –logappend\n\nTo Shut Down MongoDB enter the Mongo CLI, access the admin and issue the shutdown command:\n\n    $ ./mongo\n    > use admin\n    > db.shutdownServer()\n    \n    \n\nSee: https://www.mongodb.org/display/DOCS/Starting+and+Stopping+Mongo"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB: Unique Key in Embedded Document",
      "explanation": "Is it possible to set a unique key for a key in an embedded document?  \nI have a Users collection with the following sample documents:\n\n    Skip code block\n    {\n    Name: “Bob”,\n    Items: [\n    {\n    Name: “Milk”\n    },\n    {\n    Name: “Bread”\n    }\n    ]\n    },\n    {\n    Name: “Jim”\n    },\n\nIs there a way to create an index on the property Items.Name?  \nI got the following error when I tried to create an index:\n\n    > db.Users.ensureIndex({“Items.Name”: 1}, {unique:true});\n\nE11000 duplicate key error index: GroceryGuruApp.Users.$Items.Name\\_1  dup key: {  \n: null }  \nAny suggestions? Thank you!\n\nUnique indexes exist only across collection. To enforce uniqueness and other constraints across document you must do it in client code. (Probably virtual collections would allow that, you could vote for it.)\n\nWhat are you trying to do in your case is to create index on key Items.Name which doesn’t exist in any of the documents (it doesn’t refer to embedded documents inside array Items), thus it’s null and violates unique constraint across collection.\n\nThe index will be across all Users and since you asked it for ‘unique’, no user will be able to have two of the same named item AND no two users will be able to have the same named Item.  \nIs that what you want?\n\nFurthermore, it appears that it’s objecting to two Users having a ‘null’ value for Items.Name, clearly Jim does, is there another record like that?  \nIt would be unusual to require uniqueness on an indexed collection like this.\n\nMongoDB does allow unique indexes where it indexes only the first of each value, see https://www.mongodb.org/display/DOCS/Indexes#Indexes-DuplicateValues, but I suspect the real solution is to not require uniqueness in this case.\n\nIf you want to ensure uniqueness only within the Items for a single user you might want to try the $addToSet option. See\n\n[https://www.mongodb.org/display/DOCS/Updating#Updating-%24addToSet](https://www.mongodb.org/display/DOCS/Updating#Updating-%24addToSet)\n\nYou can create a **unique compound sparse index** to accomplish something like what you are hoping for. It may not be the best option (client side still might be better), but it can do what you’re asking depending on specific requirements.\n\nTo do it, you’ll need to create another field on the same level as Name: Bob that is unique to each top-level record (could do FirstName + LastName + Address, we’ll call this key Identifier).\n\nThen create an index like this:\n\n    ensureIndex({‘Identifier’:1, ‘Items.name’:1},{‘unique’:1, ‘sparse’:1})\n\nA sparse index will ignore items that don’t have the field, so that should get around your NULL key issue. Combining your unique Identifier and Items.name as a compound unique index should ensure that you can’t have the same item name twice per person.\n\nAlthough I should add that I’ve only been working with Mongo for a couple of months and my science could be off. This is not based on empirical evidence but rather observed behaviour.\n\n**More on MongoDB Indexes**  \n1\\. Compound Keys Indexes  \n2\\. Sparse Indexes\n\n**Related Page: [MongoDB Vs Elasticsearch Comparison](../../mongodb-vs-elasticsearch)**"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Creating a database in Mongo: can’t connect, get “connect failed”",
      "explanation": "I want to create a new database in Mongo. However, I’m having trouble connecting:\n\n    :~$ mongo\n    MongoDB shell version: 1.6.5\n    connecting to: test\n    Tue Dec 21 18:16:25 Error: couldn’t connect to server 127.0.0.1 (anon):1154\n    exception: connect failed\n\nHow can I connect to mongo in order to create a new database? Alternatively, can I create a new database from the command line?  \nSlightly surprisingly, the Mongo docs don’t seem to cover how to create a database.  \nThanks.\n\nIn order to open Mongo JavaScript shell, a Listener should be initialized first.  \nSo, first run mongod.exe before running mongo.exe. Both are in the same location(/bin).\n\nThere is no separate commands to create a db in mongodb. Just type “use dbname;” in console. Now you have created a db of the name ‘dbname’. Now, if you type ‘show databases’ you cannot see the db name you just created. Because, mongo will not create any db, util you create collection and insert a document into that collection.  \nHope this is useful to you!\n\n1.  cd /var/lib/mongodb/\n2.  Remove mongod.lock file from this folder\n3.  Sudo start mongodb (in console)\n4.  Mongo (in console)\n\nAnd it runs fine. First you’ll need to run mongod on one terminal. Then fire up another terminal and type mongo. This shall open the mongo shell. You also need to create /data/db/ where mongo will store your databases."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How can I use MongoDB to find all documents which have a field, regardless of the value of that field?",
      "explanation": "For example, I have collection with documents, where documents can have field “url” (but most of them doesn’t). How can I find all documents, which have field “url” (regardless of value of this field)? To find if a key/field exists in your document use the $exists operator.  \nVia the MongoDB shell …\n\n    > db.things.find( { url : { $exists : true } } );"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongodb Query To select records having a given key",
      "explanation": "let the records in database are\n\n    {“_id”:”1?,”fn”:”sagar”,”ln”:”Varpe”}\n    {“_id”:”1?,”fn”:”sag”,”score”:”10?}\n    {“_id”:”1?,”ln”:”ln1?,” score”:”10?}\n    {“_id”:”1?,”ln”:”ln2?}\n\nI need to design a MongoDB query to find all records who has a given key  \nlike if i pass “ln” as a parameter to query it shold return all records in which “ln”is a Key , the results fo are\n\n    {“_id”:”1?,”fn”:”sagar”,”ln”:”Varpe”}\n    {“_id”:”1?,”ln”:”ln1?,”score”:”10?}\n    {“_id”:”1?,”ln”:”ln2?}\n\nTo find if a key/field exists in your document use the $exists operator.  \nVia the MongoDB shell …\n\n    db.things.find( { ln : { $exists : true } } );\n\nPossible duplicate of this question: MongoDB queries"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "“Field name duplication not allowed with modifiers” on update",
      "explanation": "I get a “Field name duplication not allowed with modifiers” error while trying to update a field(s) in Mongo. An example:\n\n    > db.test.insert({test: “test1”, array: [0]});\n    > var testFetch = db.test.findOne({test: “test1”});\n    > db.test.update(testFetch,\n    {$push: {array: 1}, //push element to end of key “array”\n    $pop:  {array: -1} //pop element from the start of key “array”\n    });\n\nField name duplication not allowed with modifiers  \nIs there no way to perform this atomic operation? I don’t want to be doing two separate updates for this.  \nThere’s an outstanding issue for this on Mongo’s ticket system: https://jira.mongodb.org/browse/SERVER-1050  \nLooks like it’s scheduled for this year. Your scenario is definitely a sensible scenario, but it’s also tied to a bunch of edge cases. What if you $push and $pop on an empty array? What’s expected? What do you want if you $push and $pull?  \nI don’t want to be doing two separate updates for this.  \nI know that doing this really has “code smell”, but is it a complete blocker for using this solution? Is the “double-update” going to completely destroy server performance?"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "In mongoDb, how do you remove an array element by its index",
      "explanation": "Skip code block\n    {\n    “_id” : ObjectId(“4d1cb5de451600000000497a”),\n    “name” : “dannie”,\n    “interests” : [\n    “guitar”,\n    “programming”,\n    “gadgets”,\n    “reading”\n    ]\n    }\n\nIn the example above, assume the above document is in the **db.people** collection. How to remove the 3rd element of the **interests** array by it’s **index**?\n\nEdit:  \nThis is my current solution:\n\n    var interests = db.people.findOne({“name”:”dannie”}).interests;\n    interests.splice(2,1)\n    db.people.update({“name”:”dannie”}, {“$set” : {“interests” : interests}});\n\nIs there a more direct way?\n\nThere is no straight way of pulling/removing by array index. In fact, this is an opened issue\n\n[https://jira.mongodb.org/browse/SERVER-1014,](https://jira.mongodb.org/browse/SERVER-1014,)\n\nyou may vote for it.\n\n    The workaround is using $unset and then $pull:\n    db.lists.update({}, {$unset : {“interests.3” : 1 }})\n    db.lists.update({}, {$pull : {“interests” : null}})\n\nUpdate: as mentioned in some of the comments this approach is not atomic and can cause some race conditions if other clients read and/or write between the two operations. If we need the operation to be atomic, we could:\n\n1.  Read the document from the database\n2.  Update the document and remove the item in the array\n3.  Replace the document in the database. To ensure the document has not changed since we read it, we can use the update if current pattern describedin the mongo docs.\n\nYou can use $pull modifier of update operation for removing a particular element in an array. In case you provided a query will look like this:  \ndb.people.update({“name”:”dannie”}, {‘$pull’: {“interests”: “guitar”}})\n\nAlso, you may consider using $pullAll for removing all occurrences. More about this on the official documentation page –\n\n[https://www.mongodb.org/display/DOCS/Updating#Updating-%24pull](https://www.mongodb.org/display/DOCS/Updating#Updating-%24pull)\n\nThis doesn’t use index as a criteria for removing an element, but still might help in cases similar to yours. IMO, using indexes for addressing elements inside an array is not very reliable since mongodb isn’t consistent on an elements order as fas as I know."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB custom and unique IDs",
      "explanation": "I’m using MongoDB, and I would like to generate unique and cryptical IDs for blog posts (that will be used in restful URLS) such as s52ruf6wst or xR2ru286zjI.\n\nWhat do you think is best and the more scalable way to generate these IDs?\n\nI was thinking of following architecture :\n\nA periodic (daily?) batch running to generate a lot of random and uniques IDs and insert them in a dedicated MongoDB collection with InsertIfNotPresent\n\nAnd each time I want to generate a new blog post, I take an ID from this collection and mark it as “taken” with UpdateIfCurrent atomic operation WDYT? This is exactly why the developers of MongoDB constructed their ObjectID’s (the \\_id) the way they did … to scale across nodes, etc.\n\nA BSON ObjectID is a 12-byte value consisting of a 4-byte timestamp (seconds since epoch), a 3-byte machine id, a 2-byte process id, and a 3-byte counter. Note that the timestamp and counter fields must be stored big endian unlike the rest of BSON. This is because they are compared byte-by-byte and we want to ensure a mostly increasing order. Here’s the schema:\n\n0123   456      78    91011\n\ntime   machine  pid   inc  \nTraditional databases often use monotonically increasing sequence numbers for primary keys. In MongoDB, the preferred approach is to use Object IDs instead. Object IDs are more synergistic with sharding and distribution.\n\n[https://www.mongodb.org/display/DOCS/Object+IDs](https://www.mongodb.org/display/DOCS/Object+IDs)\n\nSo I’d say just use the ObjectID’s\n\nThey are not that bad when converted to a string (these were inserted right after each other) …\n\nFor example:\n\n4d128b6ea794fc13a8000001  \n4d128e88a794fc13a8000002\n\nThey look at first glance to be “guessable” but they really aren’t that easy to guess …\n\n4d128 b6e a794fc13a8000001  \n4d128 e88 a794fc13a8000002\n\nAnd for a blog, I don’t think it’s that big of a deal … we use it production all over the place.  \nWhat about using UUIDs?\n\nhttps://www.famkruithof.net/uuid/uuidgen as an example."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB shell run() function",
      "explanation": "run(“/test.js”)\n\nin mongo shell. i always get error\n\nrun(“/test.js”) shell: started program /test.js sh2516| Unable to start program /test.js errno:13 Permission denied 255\n\nfor test i add 777 permission on this file i doesn’t have any passwords on DB  \ntest.js is empty\n\nWhat am I doing wrong?\n\ntry using load() rather than run(). load is for javascript while run is for system binaries."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "“system.” In a Collection Name in MongoDB",
      "explanation": "I just discovered a bizarre behavior exhibited by MongoDB.  \nApparently, any collection name with the string “system.” anywhere in it will just not function correctly.  \nTo make matters worse, it won’t even tell you anything is wrong!\n\nIt’s really more a matter of curiosity, but does anybody have any idea why this would happen? Is it documented somewhere?  \nMy assumption is that it uses “”system.\\*” collections to store things internally (like indexes) and doesn’t want you messing with them, but this doesn’t seem like the correct behavior to me.  \nYou are correct “system.\\*” is a reserved collection namespace used by MongoDB in each DB.  \nIt is used to store indexes and users, etc.\n\nSQL Server has many such tables too, and I don’t believe they warn you not to use them either :)  \nBut you could always put in a request for such functionality:\n\n[https://jira.mongodb.org/](https://jira.mongodb.org/)\n\nYou can see them by running …  \n\\> show collections  \nand you’ll see something like …  \nsystem.indexes  \nsystem.users  \nSo, you can see your indexes for example:  \n\\> db.system.indexes.find()  \nFrom the MongoDB docs:\n\nThe **.system.**\\* namespaces in MongoDB are special and contain database system information. System collections include:  \n1\\. **system.namespaces** lists all namespaces.  \n2\\. **system.indexes** lists all indexes.  \n3\\. Additional namespace / index metadata exists in the database.ns files, and is opaque.  \n4\\. **system.profile** stores database profiling information.  \n5. **system.users** lists users who may access the database.  \n6\\. **local.sources** stores replica slave configuration data and state.  \n7\\. Information on the structure of a stored object is stored within the object itself. See BSON .\n\nThere are several restrictions on manipulation of objects in the system collections. Inserting in system.indexes adds an index, but otherwise that table is immutable (the special drop index command updates it for you). **system.users** is modifiable. **system.profile** is droppable.\n\nhttps://docs.mongodb.org/manual/reference/system-collections/"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB Shell – access collection with period in name?",
      "explanation": "I have found a collection in one of our MongoDB databases with the name my.collection.  \nIs there a way to access this collection from the MongoDB shell, despite it having a point in the name?  \n\\> db.my.collection.findOne();  \nnull  \nI’m pretty sure that that is not correct.  \ntry this instead:  \ndb\\[“my.collection”\\].findOne();\n\nyou run into the same issue with hyphens or any other name that does not match on \\[a-zA-Z\\_$\\]\\[0-9a-zA-Z\\_$\\]  \nThis limitation comes from valid named for javascript object properties.\n\nif collection name is “my.collection”  \ndb.my.collection.findOne(); // OK  \nnull  \nif collection name is “my.1.collection”  \ndb.my.1.collection.findOne(); // Not OK  \nSyntaxError: missing ; before statement  \nFix:  \ndb\\[“my.1.collection”\\].findOne(); // Now is OK  \nnull"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How do I insert a record from one mongo database into another?",
      "explanation": "As far as I see all commands operate on the same database in mongodb. I want to do something like this:  \ndb.mySourceCollection.find().forEach( function(x){ db.theDestinationCollection.save(x)} );  \nwhere mySourceCollection is on liveDatabase and theDestinationCollection is ontestDatabase.  \nUse use :-)\n\n    > var documents = db.mySourceCollection.find()\n    > use testDatabase\n\nswitched to db testDatabase\n\n    > documents.forEach(function(x){ db.theDestinationCollection.insert(x) })\n\ndb is used to refer to the currently connected database, however you can switch databases on the fly using the use command, as I’ve shown above.  \nCheck out the help command in the shell — it mentions this command and much more!\n\nuse dbname doesn’t work in scripted mode (i.e. when scripting the shell with javascript), so you should use the db.getSiblingDB() method instead to reassign the ‘db’ variable, e.g.:  \ndb = db.getSiblingDB(“otherdb”)\n\nMore info here: https://www.mongodb.org/display/DOCS/Scripting+the+shell\n\n**Related Page: [MongoDB Vs. CouchDB](../../mongodb-vs-couchdb)**"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "find inside a hash mongodb",
      "explanation": "I have this struct in my collection:  \n{foo : 1, bar : 4, baz : {a : 1, b : 2 ,c : “fafofu”}}  \nHow do I find “a” and “b” inside baz ? It does not works db.my\\_collection.find({baz : {a : 1, b : 2});  \nI don’t care about if “c” is “fafofu” or “cacocu” does not matters.  \nYou can use . to reach into the baz object.  \ndb.my\\_collection.find({“baz.a” : 1, “baz.b” : 2});"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Query IDE for MongoDB?",
      "explanation": "I’m wondering if there is an IDE for MongoDB that allows you to run queries and see the results? This would behave like query analyzer in SQL Server Management Studio. The issue I’m having right now is that I have to do queries, such as “db.MyTable.find()” from command prompt, which isn’t a good solution.  \nIf the answer is no, is there a more mature “no sql” solution like MongoDB that does have an IDE?\n\n**Web Based**\n\nFor PHP, I’d recommend Rock Mongo. Solid, lots of great features, easy setup.\n\n[https://code.google.com/p/rock-php/wiki/rock\\_mongo](https://code.google.com/p/rock-php/wiki/rock_mongo)\n\nIf you don’t want to install anything … you can use MongoHQ’s web interface (even if you your MongoDB isn’t on MongoHQ.)\n\n[https://mongohq.com/home](https://mongohq.com/home)\n\n**Windows**\n\nBy far, the best UI (for Windows) currently out there is MongoVUE.\n\n[https://blog.mongovue.com/](https://blog.mongovue.com/)\n\nLooks great, lots of features, and if you are new it will really help you get going\n\n[https://blog.mongovue.com/features/](https://blog.mongovue.com/features/)\n\nHere’s a Q&A with the author too if you are interested\n\nhttps://learnmongo.com/posts/qa-ishann-kumar-creator-of-mongovue/\n\nThere is an official list of admin tools here:\n\n[https://www.mongodb.org/display/DOCS/Admin+UIs](https://www.mongodb.org/display/DOCS/Admin+UIs)\n\nAnother contender : https://www.robomongo.org/ Robomongo give you a shell-like an interface but outputs your results in the GUI. Its available for windows, mac(dmg, zip) and linux (deb, rpm, tar.gz) as a desktop application. Currently its free!\n\nRobomongo prints the results in a treeView or Json text representation and supports the generation of UUID (.NET-,Python-,Java-Encoding). It has autocomplete, shows multiple results at once and has a query history."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB: Geting “Client Cursor::yield can’t unlock b/c of recursive lock” warning when use findAndModify in two process instances",
      "explanation": "I’m using: MongoDB 1.6.4, Python 2.6.6, PyMongo 1.9, Ubuntu 10.10  \nI’m getting “Client Cursor::yield can’t unlock b/c of recursive lock” warning in my logs very often when use findAndModify in two process instances. When i use only one process warning doesn’t appear.  \nHow can i fix this?  \n\\*Update 8 March 2013 \\*  \nIs there a fix to this problem as of now?  \nthis is usually means you are missing indexes on fields used in query.  \nI don’t know tech details of this warning but from my experience adding index on the query field helps. check you have index on fields that used in query part of findAndModify. also run db.collection.find().explain() to check if it uses the index.  \nThanks to the pingw33n who help solve this question."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "how to query child objects in mongodb",
      "explanation": "I’m new to mongodb and am trying to query child objects. I have a collection of States, and each State has child Cities. One of the Cities has a Name property that is null, which is causing errors in my app. How would I query the State collections to find child Cities that have a name == null?\n\nIf it is exactly null (as opposed to not set):\n\n    db.states.find({“cities.name”: null})\n\n(but as javierfp points out, it also matches documents that have no cities array at all, I’m assuming that they do).\n\nIf it’s the case that the property is not set:\n\n    db.states.find({“cities.name”: {“$exists”: false}})\n\nI’ve tested the above with a collection created with these two inserts:\n\n    ]db.states.insert({“cities”: [{name: “New York”}, {name: null}]})\n    db.states.insert({“cities”: [{name: “Austin”}, {color: “blue”}]})\n\nThe first query finds the first state, the second query findsthe second. If you want to find them both with one query you can make an $or query:\n\n    db.states.find({“$or”: [\n    {“cities.name”: null},\n    {“cities.name”: {“$exists”: false}}\n    ]})\n    Assuming your “states” collection is like:\n    {“name” : “Spain”, “cities” : [ { “name” : “Madrid” }, { “name” : null } ] }\n    {“name” : “France” }\n    The query to find states with null cities would be:\n    db.states.find({“cities.name” : {“$in” : [null], “$exists” : true}});\n\nIt is a common **mistake** to query for nulls as:\n\n    db.states.find({“cities.name” : null});\n\n  \nbecause this query will return all documents lacking the key (in our example it will return Spain and France). So, unless you are sure the key is always present you must check that the key exists as in the first qeury."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB 1.6.5: how to rename field in collection",
      "explanation": "$rename function is available only in development version 1.7.2. How to rename field in 1.6.5?  \nThe simplest way to perform such an operation is to loop through the data set re-mapping the name of the field. The easiest way to do this is to write a function that performs the re-write and then use the .find().forEach() syntax in the shell.  \nHere’s a sample from the shell:\n\n    Skip code block\n    db.foo.save({ a : 1, b : 2, c : 3});\n    db.foo.save({ a : 4, b : 5, c : 6});\n    db.foo.save({ a : 7, b : 8 });\n    db.foo.find();\n    remap = function (x) {\n    if (x.c){\n    db.foo.update({_id:x._id}, {$set:{d:x.c}, $unset:{c:1}});\n    }\n    }\n\n    db.foo.find().forEach(remap);\n    db.foo.find();\n\nIn the case above I’m doing an $unset and a $set in the same action. MongoDB does not support transactions across collections, but the above is a single document. So you’re guaranteed that the set and unset will be atomic (i.e. they both succeed or they both fail).\n\nThe only limitation here is that you’ll need to manage outside writers to keep the data consistent. My normal preference for this is simply to turn off writes while this updates. If this option is not available, then you’ll have to figure out what level of consistency you want for the data. (I can provide some ideas here, but it’s really going to be specific to your data and system)  \ndb.collection\\_name.update({}, {$rename: {“oldname”: “newname”}}, false, true);\n\nThis will rename the column for each row in the collection.  \nAlso, I discovered that if your column (the one you’re renaming) appears within the index catalog (db.collection\\_name.getIndexes()), then you’re going to have to drop and recreate the index (using the new column name) also."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Is it normal size for MongoDB?",
      "explanation": "I just import my **Mysql** database (size 2,8 Mio) in my new **Mongo** database with very simple php script i build, import was ok without error but when i look my **Mongo** database (with RockMongo) i can see this : Data Size 8.01m, Storage Size 13.7m.\n\n**MongoDB** is bigger than **Mysql** for the same amount of data, is this normal?  \nThanks for your help and sorry for my english.\n\nYes, it’s normal that the “same” data will take up more space in mongodb. There’s a few things you need to take into account:  \n1) the document \\_id that’s stored for each document (unless you are specifying your own value for that) is 12 bytes per doc  \n2) you’re storing the key for each key-value pair in each document, whereas in MySql the column name is not stored for every single row so you have that extra overhead in your mongodb documents too. One way to reduce this is to use shortened key names (“column names”) in your docs  \n3) mongodb automatically adds padding to allow documents to grow  \nIn similar tests, loading data from SQL Server to MongoDB, with shortened 2 character document key names instead of the full names as per SQL Server, I see about 25-30% extra space being used in MongoDB."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Maximum number of databases supported by MongoDB",
      "explanation": "I would like to create a database for each customer. But before, I would like to know how many databases can be created in a single instance of MongoDB ?  \nThere’s no explicit limit, but there are probably some implicit limits due to max number of open file handles / files in a directory on the host OS/filesystem.  \nsee: https://groups.google.com/group/mongodb-user/browse\\_thread/thread/01727e1af681985a?fwc=2"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongo $in operator performance",
      "explanation": "Is it slow / poor form to use the $in operator in MongoDB with a large array of possibilities?\n\n    posts.find({\n    author : {\n    $in : [‘friend1',’friend2',’friend3’….’friend40’]\n    }\n    })\n\nApp Engine, for example, won’t let you use more than 30 because they translate directly to one query per item in the IN array, and so instead force you into using their method for handling fan out. While thats probably the most efficient method in Mongo too, the code for it is significantly more complex so I’d prefer to just use this generic method.  \nWill Mongo execute these $in queries efficiently for reasonable-sized datasets?\n\nIt can be fairly efficient with small lists (hard to say what small is, but at least into the tens/hundreds) for $in. It does not work like app-engine since mongodb has actual btree indexes and isn’t a column store like bigtable.\n\nWith $in it will skip around in the index to find the matching documents, or walk through the whole collection if there isn’t an index to use.  \nIf you build an index (ensureIndex) on the list element, it should be pretty quick.\n\nHave you tried using explain()? Its a good, built-in way to profile your queries: https://www.mongodb.org/display/DOCS/Indexing+Advice+and+FAQ#IndexingAdviceandFAQ-Use%7B%7Bexplain%7D%7D."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongo Query question $gt,$lt",
      "explanation": "I have a query below. I want get items between 4 and 6 so only a:1 should match because it has the value 5 in b.\n\n    > db.test.find({ b : { $gt :  4  }, b: {$lt : 6}});\n    { “_id” : ObjectId(“4d54cff54364000000004331”), “a” : 1, “b” : [ 2, 3, 4, 5 ] }\n    { “_id” : ObjectId(“4d54d0074364000000004332”), “a” : 2, “b” : [ 2, 4, 6, 8 ] }\n    >\n\nCan someone tell be why a:2 is matching this query? I can’t really see why it is being returned.  \nI also tried what was specified in the tutorial but id did not seem to work:\n\n    > db.test.find({ b : { $gt :  4, $lt : 6}});\n    { “_id” : ObjectId(“4d54cff54364000000004331”), “a” : 1, “b” : [ 2, 3, 4, 5 ] }\n    { “_id” : ObjectId(“4d54d0074364000000004332”), “a” : 2, “b” : [ 2, 4, 6, 8 ] }\n    >\n\n#### Subscribe to our youtube channel to get new updates..!\n\nAnd this one to avoid any confusion regarding GT/GTE\n\n    > db.test.find({b: {$gt: 4.5, $lt: 5.5}});\n    { “_id” : ObjectId(“4d54cff54364000000004331”), “a” : 1, “b” : [ 2, 3, 4, 5 ] }\n    { “_id” : ObjectId(“4d54d0074364000000004332”), “a” : 2, “b” : [ 2, 4, 6, 8 ] }\n    >\n\nonly a:1 should be returned.  \nAs suggested, I gave $elemMatch a try but it did not appear to work either (objectIds are different because I am on a different machine)\n\n    > db.test.find();\n    { “_id” : ObjectId(“4d5a24a5e82e00000000433f”), “a” : 1, “b” : [ 2, 3, 4, 5 ] }\n    { “_id” : ObjectId(“4d5a24bbe82e000000004340”), “a” : 2, “b” : [ 2, 4, 6, 8 ] }\n    > db.test.find({b: {$elemMatch: {$gt : 4, $lt: 6 }}});\n    >\n\nNo documents were returned.  \nThis is a really confusing topic. I work at 10gen and I had to spend a while wrapping my head around it  \nLet’s walk through how the query engine processes this query.  \nHere’s the query again:\n\n    > db.test.find({ b : { $gt :  4, $lt : 6}});\n\nWhen it gets to the record that seems like it shouldn’t match…\n\n    { “_id” : ObjectId(“4d54cff54364000000004331”), “a” : 1, “b” : [ 2, 4, 6, 8 ] }\n\nThe match is not performed against each element of the array, but rather against the array as a whole.  \nThe comparison is performed in three steps:\n\n**Step 1:** Find all documents where b has a value greater than 4  \nb: \\[2,4,6,8\\] matches because 6 & 8 are greater than 4\n\n**Step 2**: Find all documents where b has a value less than 6  \nb: \\[2,4,6,8\\] matches because 2 & 4 are less than 6\n\n**Step 3:** Find the set of documents that matched in both step 1 & 2.  \nThe document with b: \\[2,4,6,8\\] matched both steps 1 & 2 so it is returned as a match. Note that results are also de-duplicated in this step, so the same document won’t be returned twice.\n\nIf you want your query to apply to the individual elements of the array, rather than the array as a whole, you can use the $elemMatch operator. For example\n\n    > db.temp.find({b: {$elemMatch: {$gt: 4, $lt: 5}}})\n    > db.temp.find({b: {$elemMatch: {$gte: 4, $lt: 5}}})\n    { “_id” : ObjectId(“4d558b6f4f0b1e2141b66660”), “b” : [ 2, 3, 4, 5, 6 ] }\n\n**Related Page: [MongoDB Show Collections](../../mongodb-show-collections)**"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB: How to change the type of a field?",
      "explanation": "There is a question already in Stackoverflow, very similar with my question. The thing is that the answer for that questions was for a Java Driver, I am trying to do it in the shell.  \nI am doing this…\n\n    db.meta.update({‘fields.properties.default’: { $type : 1 }}, {‘fields.properties.default’: { $type : 2 }})\n\nThis is not working  \nThe only way to change the $type of the data is to perform an update on the data where the data has the correct type.\n\nIn this case, it looks like you’re trying to change the $type from 1 (double) to 2 (string).\n\nhttps://www.mongodb.org/display/DOCS/Advanced+Queries#AdvancedQueries-%24type\n\nSo simply load the document from the DB, perform the cast (new String(x)) and then save the document again.\n\nIf you need to do this programmatically and entirely from the shell, you can use thefind(…).forEach(function(x) {}) syntax.  \nIn response to the second comment below. Change the field bad from a number to a string in collection foo.\n\n    db.foo.find( { ‘bad’ : { $type : 1 } } ).forEach( function (x) {\n    x.bad = new String(x.bad); // convert field to string\n    db.foo.save(x);\n    });\n    // String to Integer\n    db.db-name.find({field-name : {$exists : true}}).forEach( function(obj) { obj.field-name = new NumberInt( obj.field-name ); db.db-name.save(obj); } );\n    // Integer to String\n    db.db-name.find({field-name : {$exists : true}}).forEach( function(obj) { obj.field-name = “”+obj.field-name; db.db-name.save(obj); } );\n    This is what I used for string to int conversion.\n    db.my_collection.find().forEach( function(obj) {\n    obj.my_value= parseInt(obj.my_value);\n    db.my_collection.save(obj);\n    });\n    db.coll.find().forEach(function(data) {\n    db.coll.update({_id:data._id},{$set:{myfield:parseInt(data.myfield)}});\n    })"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Many to many update in MongoDB without transactions",
      "explanation": "I have two collections with a many-to-many relationship. I want to store an array of linked ObjectIds in both documents so that I can take Document A and retrieve all linked Document B’s quickly, and vice versa.\n\nCreating this link is a two step process  \n1\\. Add Document A’s ObjectId to Document B  \n2\\. Add Document B’s ObjectId to Document A\n\nAfter watching a MongoDB video I found this to be the recommended way of storing a many-to-many relationship between two collections  \nI need to be sure that both updates are made. What is the recommended way of robustly dealing with this crucial two step process without a transaction?  \nI could condense this relationship into a single link collection, the advantage being a single update with no chance of Document B missing the link to Document A. The disadvantage being that I’m not really using MongoDB as intended. But, because there is only a single update, it seems more robust to have a link collection that defines the many-to-many relationship.\n\nShould I use safe mode and manually check the data went in afterwards and try again on failure? Or should I represent the many-to-many relationship in just one of the collections and rely on an index to make sure I can still quickly get the linked documents?  \nAny recommendations? Thanks\n\n**@Gareth**, you have multiple legitimate ways to do this. So they key concern is how you plan to query for the data, (i.e.: what queries need to be fast)  \nHere are a couple of methods.\n\n**Method #1: the “links” collection**  \nYou could build a collection that simply contains mappings between the collections.\n\nPros:  \n1\\. Supports atomic updates so that data is not lost\n\nCons:  \n2\\. Extra query when trying to move between collections\n\n**Method #2: store copies of smaller mappings in larger collection**\n\nFor example: you have millions of Products, but only a hundred Categories. Then you would store the Categories as an array inside each Product.\n\nPros:  \n1\\. Smallest footprint  \n2\\. Only need one update\n\nCons:  \n1\\. Extra query if you go the “wrong way”\n\n**Method #3: store copies of all mappings in both collections**  \n(what you’re suggesting)  \nPros:  \n1\\. Single query access to move between either collection  \nCons:  \n1\\. Potentially large indexes  \n2\\. Needs transactions (?)  \nLet’s talk about “needs transactions”. There are several ways to do transactions and it really depends on what type of safety you require.  \nShould I use safe mode and manually check the data went in afterwards and try again on failure?  \nYou can definitely do this. You’ll have to ask yourself, what’s the worst that happens if only one of the saves fails?\n\n**Method #4: queue the change**\n\nI don’t know if you’ve ever worked with queues, but if you have some leeway you can build a simple queue and have different jobs that update their respective collections.  \nThis is a much more advanced solution. I would tend to go with #2 or #3."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB log file growth",
      "explanation": "Currently my log file sits at 32 meg. Did I miss an option that would split the log file as it grows?  \nRotate the logs yourself\n\n[https://www.mongodb.org/display/DOCS/Logging](https://www.mongodb.org/display/DOCS/Logging)\n\nor use ‘logrotate’ with an appropriate configuration.  \nYou can use logrotate to do this job for you.  \nPut this in /etc/logrotate.d/mongod (assuming you use Linux and have logrotated installed):  \nSkip code block\n\n    /var/log/mongo/*.log {\n    daily\n    rotate 30\n    compress\n    dateext\n    missingok\n    notifempty\n    sharedscripts\n    copytruncate\n    postrotate\n    /bin/kill -SIGUSR1 `cat /var/lib/mongo/mongod.lock 2> /dev/null` 2> /dev/null || true\n    endscript\n    }\n\nIf you think that 32 megs is too large for a log file, you may also want to look inside to what it contains.  \nIf the logs seem mostly harmless (“open connection”, “close connection”), then you may want to start mongod with the –quiet switch. This will reduce some of the more verbose logging."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB field order and document position change after update",
      "explanation": "I am learning MongoDB and I noticed that whenever I do an update on a document the field being updated is pushed to the end of the order, so if I had something like:\n\n    db.collection.save({field1: value1, field2: value2, …, field 10: value10});\n    db.collection.update({field1: value1}, {$set: {field2: new_value}});\n    then if you do:\n    db.collection.find();\n    it will display:\n    { “field1?:”value1”, …, “field10?:”value10”, “field2?:”new_value”}\n\nYou can see how the field order changes where the updated field is being pushed to the end of the document. In addition, the document itself is being pushed to the end of the collectoin. I know that it’s a “schema-less” DB and it may not be a huge problem, but it just doesn’t look “pretty” :). Is there a way to do an in-place update without changing the order?  \nMongoDB allocates space for a new document based on a certain padding factor. If your update increases the size of the document beyond the size originally allocated the document will be moved to the end of the collection. The same concept applies to fields in a document.\n\nBoth document structure and collection structure in MongoDB based on JSON principles. JSON is a **set** of key/value pairs (in particular fieldName/fieldValue for document and index/document for collection). From this point of view it doesn’t seem that you can relay on order at all.\n\nIn the case of the documents if the field size changes, it writes out a new document with the fields sorted by field name. This behavior can be seen with the following statements  \nCase 1: No change in size of field, so no change in field order\n\n    db.testcol.find() \n    db.testcol.save({a:1,c:3,b:2}) \n    db.testcol.find()\n     { “_id” : ObjectId(“4d5efc3bec5855af36834f5a”), “a” : 1, “c” : 3, “b” : 2 } \n    db.testcol.update({a:1},{$set:{c:22}}) \n    db.testcol.find() { “_id” : ObjectId(“4d5efc3bec5855af36834f5a”), “a” : 1, “c” : 22, “b” : 2 }\n\nCase 2: Field size changes and the fields are reodered\n\n    db.testcol.find() db.testcol.save({a:1,c:”foo”,b:2,d:4}) \n    db.testcol.find()\n    { “_id” : ObjectId(“4d5efdceec5855af36834f5e”), “a” : 1, “c” : “foo”, “b” : 2, “d” : 4 } \n    db.testcol.update({a:1},{$set:{c:”foobar”}}) \n    db.testcol.find() \n    { “_id” : ObjectId(“4d5efdceec5855af36834f5e”), “a” : 1, “b” : 2, “c” : “foobar”, “d” : 4 }\n\nIs there a particular reason why you do not want the fields reordered? The above was using 1.8.0\\_rc0 on OS X  \nFYI, in MongoDB 2.6 updates will preserve field order, with the following exceptions:  \n1\\. The \\_id field is always the first field in the document.  \n2\\. Updates that include renaming of field names may result in the reordering of fields in the document.\n\n**Related Page: [MongoDB Find Queries](../../mongodb-find-queries)**"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Limit MongoDB database size?",
      "explanation": "We’re interested in deploying MongoDB, but we need to know if we can limit database/table sizes?\n\nFor example:\n\n    db.user1.find() db.user2.find()\n\n  \nAs you can see from the above, each user will have their own database. We want to limit each user’s database so we don’t have any one user eating up all our hard drive space.\n\nIs this possible with MongoDB?  \nThanks.\n\nYou can create a database per user and enable the –quota option. This will allow you keep any user from using too much space.\n\nhttps://www.mongodb.org/display/DOCS/Command+Line+Parameters  \nhttps://www.mongodb.org/display/DOCS/Excessive+Disk+Space"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "While Creating New User in MongoDB, it has to be created under admin?",
      "explanation": "I want to create a new user in MongoDB, So i have to do that by login to admin by(use admin) and then i have to use the command add user to create new user is it?  \nThanks,  \nIf no users created you can create new user without any authentification, but if you have created admin user for specific database you should authentifcate, and then perform any operation.\n\n**Documentation:**  \nIf no users are configured in admin.system.users, one may access the database from the localhost interface without authenticating. Thus, from the server running the database (and thus on localhost), run the database shell and configure an administrative user:\n\n    $ ./mongo\n    > use admin\n    > db.addUser(“theadmin”, “anadminpassword”)\n\nWe now have a user created for database admin. Note that if we have not previously authenticated, we now must if we wish to perform further operations, as there is a user in admin.system.users.\n\n    > db.auth(“theadmin”, “anadminpassword”)\n\nWe can view existing users for the database with the command:\n\n    > db.system.users.find()\n\nNow, let’s configure a “regular” user for another database.\n\n    > use projectx\n    > db.addUser(“joe”, “passwordForJoe”)"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "When to embed documents in Mongo DB",
      "explanation": "I’m trying to figure out how to best design Mongo DB schemas. The Mongo DB documentation recommends relying heavily on embedded documents for improved querying, but I’m wondering if my use case actually justifies referenced documents.  \nA very basic version of my current schema is basically: (Apologies for the psuedo-format, I’m not sure how to express Mongo schemas)  \nSkip code block\n\n    users {\n    email (string)\n    }\n    games {\n    user (reference user document)\n    date_started (timestamp)\n    date_finished (timestamp)\n    mode (string)\n    score: {\n    total_points (integer)\n    time_elapsed (integer)\n    }\n    }\n\nGames are short (about 60 seconds long) and I expect a lot of concurrent writes to be taking place.\n\nAt some point, I’m going to want to calculate a high score list, and possibly in a segregated fashion (e.g., high score list for a particular game.mode or date)  \nIs embedded documents the best approach here? Or is this truly a problem that relations solves better? How would these use cases best be solved in Mongo DB?  \n… is this truly a problem that relations solves better?\n\nThe key here is less about “is this a relation?” and more about “how am I going to access this?”\n\nMongoDB is not “anti-reference”. MongoDB does **not** have the benefits of joins, but it **does** have the benefit of embedded documents.  \nAs long as you understand these trade-offs then it’s perfectly fair to use references in MongoDB. It’s really about how you plan to query these objects.  \nIs embedded documents the best approach here?\n\nMaybe. Some things to consider.  \n1\\. Dogames have value outside of the context of the user?  \n2\\. How manygames will a single user have?  \n3\\. Isgames transactional in nature?  \n4\\. How are you going to accessgames? Do you always need all of a user’s games?\n\nIf you’re planning to build leaderboards and a user can generate hundreds of game documents, then it’s probably fair to have games in their own collection. Storing ten thousand instances of “game” inside of each users isn’t particularly useful.\n\nBut depending on your answers to the above, you could really go either way. As the litmus test, I would try running some Map / Reduce jobs (i.e. build a simple leaderboard) to see how you feel about the structure of your data."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Why Is MongoDB So Fast",
      "explanation": "I was showing my co-worker performance benchmarks of MongoDB vs SQL 2008 and while he believes MongoDB is faster, he doesn’t understand how its possible. His logic, was that SQL has been around for decades, and has some of the smartest people working on it, and how can MongoDB; a relatively new kid on the block be so superior in performance? I wasn’t able to really provide a solid and technical answer, and I was hoping you guys could assist.\n\nMongoDB isn’t like a traditional relational database. It’s noSQL or document based, it provides weak consistency guarantees, and it doesn’t have to guarantee consistency like SQL.  \nMongoDB is fast because its web scale!\n\nIts a fun video and well worth everyone watching, but it does answer your question – that most of the noSQL engines like MongoDB are not robust and not resilient to crashes and other outages. This security is what they sacrifice to gain speed.\n\nSQL has to do quite a lot, Mongo just has to drop bits onto disk (almost)"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to remove an element from a doubly-nested array in a MongoDB document",
      "explanation": "I have a document structure something along the lines of the following:\n\n    {\n    “_id” : “777”,\n    “someKey” : “someValue”,\n    “someArray” : [\n    {\n    “name” : “name1”,\n    “someNestedArray” : [\n    {\n    “name” : “value”\n    },\n    {\n    “name” : “delete me”\n    }\n    ]\n    }\n    ]\n    }\n\nI want to delete the nested array element with the value “delete me”.\n\nI know I can find documents which match this description using nested $elemMatch expressions. What is the query syntax for removing the element in question?  \nTo delete the item in question you’re actually going to use an update. More specifically you’re going to do an update with the $pull command which will remove the item from the array.\n\n    db.temp.update({ _id : “777” }, {$pull : { “someArray.0.someNestedArray” : {“name”:”delete me”} } } )\n\nThere’s a little bit of “magic” happening here. Using .0 indicates that we know that we are modifying the 0th item of someArray. Using {“name”:”delete me”} indicates that we know the exact data that we plan to remove.\n\nThis process works just fine if you load the data into a client and then perform the update. This process works less well if you want to do “generic” queries that perform these operations.  \nI think it’s easiest to simply recognize that updating arrays of sub-documents generally requires that you have the original in memory at some point.\n\nIn response to the first comment below, you can probably help your situation by changing the data structure a little\n\nSkip code block\n\n    “someObjects” : {\n    “name1”:  {\n    “someNestedArray” : [\n    {\n    “name” : “value”\n    },\n    {\n    “name” : “delete me”\n    }\n    ]\n    }\n    }\n\nNow you can do {$pull : { “someObjects.name1.someNestedArray” : …\n\nHere’s the problem with your structure. MongoDB does not have very good support for manipulating “sub-arrays”. Your structure has an array of objects and those objects contain arrays of more objects.\n\nIf you have the following structure, you are going to have a difficult time using things like $pull:\n\n    array [\n    { subarray : array [] },\n    { subarray : array [] },\n    ]\n\nIf your structure looks like that and you want to update subarray you have two options: 1. Change your structure so that you can leverage $pull. 2. Don’t use $pull. Load the entire object into a client and use findAndModify.\n\n**Related Page: [MongoDB Port](../../mongodb-port)**"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Can MongoDB work when size of database larger then RAM? or when index larger then RAM?",
      "explanation": "Yes it can work. To what level it will perform is more of an “It Depends”\n\nThe key thing is to ensure your **working set** can fit in RAM. So if you have 16GB of RAM and 20GB database (inc. indexes) for example, if you need to only access half of all the data as the other half is older/never actually queried then you’ll be fine as only half of your database needs to be in RAM (10GB).\n\nWorking set is key here. For example, if you have a logging application outputting to MongoDB, it may be that your working set is the amount of data (and indexes) from the past 3 months and that all data before that you don’t access.\n\nWhen your working set exceeds the amount of RAM, then it will carry on working but with noticeably degraded performance as things will then be constantly having to go to disk which is far less performant. If you’re in this situation of exceeding RAM constraints on a machine, then this is where sharding comes into play – so you can balance the data out over a number of machines therefore increasing the amount of data that can be kept in RAM."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Multiple $inc updates in MongoDB",
      "explanation": "**Is it possible to update a single document by passing two $inc operators in a single update document?**\n\nFor example, I am trying to increment two different fields in a given document using the following update document:\n\n    {\n    “$inc” : { “ViewAggregates.4d75b891842f2d3930cf7674” : 1 },\n    “$inc” : { “ViewAggregates.Total” : 1 }\n    }\n\nNo errors are thrown and the document is updated but only one of the fields has been incremented. It is as if the server disregarded the first $inc operator and only the second was actually applied.\n\nIs this the intendedcorrect behavior or is there something I am missing?  \nThis is an interesting side-effect of dictionary keys being unique — the second $inc overwrites the first.\n\n    However, it’s still possible to increment more than one field:\n    {\n    “$inc”: {\n    “ViewAggregates.4d75b891842f2d3930cf7674” : 1,\n    “ViewAggregates.Total” : 1\n    }\n    }\n\nThis works for many other operators too :-)"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB : Indexes order and query order must match?",
      "explanation": "This question concern the internal method to manage indexes and serching Bson Documents.  \nWhen you create a multiple indexes like “index1”, “index2”, “index3?…the index are stored to be used during queries, but what about the order of queries and the performance resulting.  \nsample\n\n    index1,index2,index3—-> query in the same order index1,index2,index3 (best case) index1,index2,index3—-> query in another order index2,index1,index3 (the order altered)\n\nMany times you use nested queries including this 3 index and other items or more indexes. The order of the queries would implicate some time lost?. Must passing the queries respecting the indexes order defined or the internal architecture take care about this order search? I searching to know if i do take care about this or can make my queries in freedom manier.  \nThanks.\n\nThe order of the conditions in your query does not affect whether it can use an index or no.\n\n    e.g. typical document structure:\n    {\n    “FieldA” : “A”,\n    “FieldB” : “B”\n    }\n\nIf you have an compound index on A and B :\n\n    db.MyCollection.ensureIndex({FieldA : 1, FieldB : 1})\n\nThen both of the following queries will be able to use that index:\n\n    db.MyCollection.find({FieldA : “A”, FieldB : “B”})\n    db.MyCollection.find({FieldB : “B”, FieldA : “A”})\n\nSo the ordering of the conditions in the query do not prevent the index being used – which I think is the question you are asking.\n\nYou can easily test this out by trying the 2 queries in the shell and adding .explain() after the find. I just did this to confirm, and they both showed that the compound index was used.  \nhowever, if you run the following query, this will NOT use the index as FieldA is not being queried on:\n\n    db.MyCollection.find({FieldB : “B”})\n\nSo it’s the ordering of the fields in the index that defines whether it can be used by a query and not the ordering of the fields in the query itself (this was what Lucas was referring to).\n\nIf you have a compound index on multiple fields, you can use it to query on the beginning subset of fields. So if you have an index on  \na,b,c  \nyou can use it query on  \na  \na,b  \na,b,c  \nSo yes, order matters. You should clarify your question a bit if you need a more precise answer."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB :: are MongoIds unique across collections?",
      "explanation": "I was wondering: can Mongo IDs have the same value in different collections in the same database?  \nThank you,  \nThe uniqueness constraint for \\_id is per collection, so yes – one and the same ID can occur once per Collection.  \nIt’s however very unlikely, if not impossible, for the same ID to be generated twice. So in order for this to happen you would have to manually insert duplicate IDs."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "like query in mongoDB",
      "explanation": "I am working on mongodb . In which i Want to use like query. my collection structure is as follows.\n\n    { “name” : “xy” , “age” : 34 , “location” : “sss”}\n    { “name” : “xyx” , “age” : 45 , “location” : “sshs”}\n    { “name” : “x” , “age” : 33 , “location” : “shhss”}\n    { “name” : “pq” , “age” : 23 , “location” : “hhh”}\n    { “name” : “pqr” , “age” : 12 , “location” : “sss”}\n\ni want to find records matching to “name” : “x”.  \nso query will return all three records matching xy ,xyz,x.  \nIs it possible in mongo.  \nif any one knows plz reply.  \nThanks  \nYou can use regular expressions to do this:  \ndb.customers.find( { name : /^x/i } );  \nYou will probably want to have some indexes on the name field.  \nRead more at the MongoDB Documetation site."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to check if an array field contains a unique value or another array in MongoDB?",
      "explanation": "I am using mongodb now.  \nI have blogpost collection, and blogpost has a tags filed which is an array, e.g.\n\n    blogpost1.tags = [‘tag1’, ‘tag2’, ‘tag3’, ‘tag4’, ‘tag5’]\n    blogpost2.tags = [‘tag2’, ‘tag3’]\n    blogpost3.tags = [‘tag2’, ‘tag3’, ‘tag4’, ‘tag5’]\n    blogpost4.tags = [‘tag1’, ‘tag4’, ‘tag5’]\n\nHow can I do these searchs\n\n    contains ‘tag1’\n    contains [‘tag1',’tag2’],\n    contains any of [‘tag3’, ‘tag4’]\n\nTry this out:\n\n    db.blogpost.find({ ‘tags’ : ‘tag1’}); //1\n    db.blogpost.find({ ‘tags’ : { $all : [ ‘tag1’, ‘tag2’ ] }}); //2\n    db.blogpost.find({ ‘tags’ : { $in : [ ‘tag3’, ‘tag4’ ] }}); //3\n\nMy experience is that for (2) the following solution is much faster than the one with “$all”:\n\ndb.blogpost.find({ $and: \\[ {tags: ‘tag1’} ,{tags: ‘tag2’} \\] });\n\nbut to be honest, I do not not why. I would be interested in, if anyone knows."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "create secure database in mongodb",
      "explanation": "I want to create the database in mongodb and thats secure.  \nhere ..secure mean .application has to pass username/password to connect my database in mongodb.  \njavaamtho  \nFrom Mongo Java Tutorial\n\nMongoDB can be run in a secure mode where access to databases is controlled through name and password authentication. When run in this mode, any client application must provide a name and password before doing any operations. In the Java driver, you simply do the following with the connected mongo object :  \nboolean auth = db.authenticate(myUserName, myPassword);\n\nIf the name and password are valid for the database, auth will be true. Otherwise, it will be false. You should look at the MongoDB log for further information if available.  \nMost users run MongoDB without authentication in a trusted environment.\n\n**Configuring Authentication and Security**\n\nAuthentication is stored in each database’s system.users collection. For example, on a database projectx, projectx.system.users will contain user information.  \nWe should first configure an administrator user for the entire db server process. This user is stored under the special admin database.\n\nIf no users are configured in admin.system.users, one may access the database from the localhost interface without authenticating. Thus, from the server running the database (and thus on localhost), run the database shell and configure an administrative user:\n\n    $ ./mongo\n    > use admin\n    > db.addUser(“theadmin”, “anadminpassword”)\n\nWe now have a user created for database admin. Note that if we have not previously authenticated, we now must if we wish to perform further operations, as there is a user in admin.system.users.\n\n    > db.auth(“theadmin”, “anadminpassword”)\n\nWe can view existing users for the database with the command:\n\n    > db.system.users.find()\n\nNow, let’s configure a “regular” user for another database.\n\n    > use projectx\n    > db.addUser(“joe”, “passwordForJoe”)\n\nFinally, let’s add a readonly user. (only supported in 1.3.2+)\n\n    > use projectx\n    > db.addUser(“guest”, “passwordForGuest”, true)\n\n**Related Page: [MongoDB Create Collection](../../mongodb-create-collection)**"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How do I copy a database from one MongoDB server to another?",
      "explanation": "I have two mongodbs in different server, both start with –auth. Now I want to copy a db from one server to another.\n\n    > mongo\n    > use admin\n    > db.copyDatabase(“mydb”,”mydb”,”another_server”)\n    It shows:\n    { “errmsg” : “”, “ok” : 0 }\n    And:\n    > db.getLastError()\n    null\n\nSeems no error, but the copy is not successful. What’s the correct command to use?  \nIf you are using **–auth**, you’ll need to include your username/password in there…  \nAlso you must be on the “destination” server when you run the command.  \ndb.copyDatabase(, , , , );  \nIf all that doesn’t work, you might want to try something like creating a slave of the database you want to copy …"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Case-insensitive query on mongodb",
      "explanation": "Is there a way to query for a case-insensitive value on mongo without using map/reduce?  \nSuppose you have document that contains tag field and you want search on it\n\n    Tags\n    {\n    tag,\n    …\n    }\n    First option is use regex(but it work slow as @RestRisiko said):\n    db.tags.find( { “tag” : { “$regex” : “C#”, “$options” : “-i” } })\n    Second option is create another, lower case field( and in mongodb it best way):\n    Tags\n    {\n    tag,\n    tagLower,\n    ..\n    }\n    And use find as usual:\n    db.tags.find( { “tagLower” : “c#”})\n\nIt will work faster, because above code can use index for search.  \nYou have to normalize the data to be queried. Using a regular expression for case-insensitive search might work as well it won’t use indexes. So your only option is to normalize. If you need to preserve the original state then you need to denormalize the data and store the normalized values in a dedicated column of the document."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB load balancing",
      "explanation": "I was looking at best load balancing option for concurrent users with Mongo DB. I have looked at Master Slave replication but don’t think this will load balance. Are there any open source DB load balancers for Mongo DB? I have looked at Sequoia but looks like that project is no longer actively supported.\n\nPlease note: The data is not very huge & also not use case for sharding. both Master Slave and Replica Sets will load balance in MongoDB, if you set slaveOK in your driver.  \nWhen slaveOK is enabled MongoDB drivers direct all reads to secondaries/slaves. This provides relatively effective read balancing; for write balancing your only option.would be sharding."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB: unconditional updates?",
      "explanation": "This seems like a silly question but I haven’t yet found the answer. If I simply wanted to add the same field->value to EVERY record in a MongoDB collection, what would be the appropriate shell command to do so? I tried doing a multi update with a blank query ({}) but that resulted in this error:\n\n**multi update only works with $ operators**\n\nI’m a bit puzzled about how to get around this. Any suggestions?  \nThe error says it all: You can only modify multiple documents using the $ modifier operators. You probably had something like this:\n\n    > db.coll.update({ }, { a: ‘b’ }, false, true);\n\nWhich would normally replace the first object in the collection with { a: ‘b’ } if multi was false. You wouldn’t want to replace all the objects in your collection with the same document!  \nUse the $set operator instead:\n\n    > db.coll.update({ }, { ‘$set’: { a: ‘b’ } }, false, true);\n\nThis will set the a property of every document (creating it as necessary) to ‘b’."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongo: find items that don’t have a certain field",
      "explanation": "Is it possible in Mongo to search for documents in a collection that are missing a certain field?  \nYeah, it’s possible using $exists:\n\n    db.things.find( { a : { $exists : false } } ); // return if a is db.things.find( { a : { $exists : false } } ); // return if a is missing\n\nIf you don’t care if the field is missing or null (or if it’s never null) then you can use the slightly shorter and safer:\n\ndb.things.find( { a : null } ); // return if a is missing or null\n\nIt’s safer because $exists will return true even if the field is null, which often is not the desired result and can lead to an NPE."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Is there a sample MongoDB Database along the lines of world for MySql?",
      "explanation": "As someone new to Mongo, I am looking for a sample MongoDB database that I can import and play with. Something along the lines of world for mysql or Northwind for MSSQL.  \nIs there one? (I couldn’t find any reference to one at https://www.mongodb.org nor did my googling help)  \nFor Windows users: Please follow the following steps to import from the json file if you are using windows 7:\n\n1.  Download the above mentionedJSON file and place it inside a folder (say d:sample)\n2.  Open a command prompt, start the mongo server by going in to the bin directory and typing in mongoD\n3.  Now take another command prompt and go to the bin directory again and write following command C:mongodbbin>mongoimport –db test –collection zips –file d:samplezips.json\n    \n4.  The import would start working immediatly and at the end it would show some thing like this Thu Dec 19 17:11:22 imported 29470 objects\n\nThat’s it!  \nI found this you can import the json data with:  \nmongoimport –db scratch –collection zips –file zips.json  \nI guess that you can import any json data that you find, it also supports csv and tsv. Hope this helps.  \nThis doesn’t have everything, but it is a nice step towards getting Northwind on MongoDB:  \nhttps://github.com/tmcnab/northwind-mongo"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB query help: $elemMatch in nested objects",
      "explanation": "> db.test.insert({“a” : { “b” : { “c” : { “d1” : [ “e1” ],\n    “d2” : [ “e2” ],\n    “d3” : [ “e3”, “e4” ],\n    “d4” : [ “e5”, “e6” ] } } } })\n    > db.test.find({‘a.b.c’ : {$exists : true}})\n    { “_id” : ObjectId(“4daf2ccd697ebaacb10976ec”), “a” : { “b” : { “c” : { “d1” : [ “e1” ], “d2” : [ “e2” ], “d3” : [ “e3”, “e4” ], “d4” : [ “e5”, “e6” ] } } } }\n\nBut none of these work:\n\n    > db.test.find({‘a.b’: “c”})\n    > db.test.find({‘a.b’: {$elemMatch : {“c” : {$exists: true}}}})\n    > db.test.find({‘a.b’: {$elemMatch : {$elemMatch : {$all : [“e1”] }}}})\n\nSuppose I don’t know what the values of c and d1…d4 are. Is there a generic way to search the nested-objects’ structure for particular values?  \nI thought that was what $elemMatch was for.  \nThank you.\n\nI thought that was what $elemMatch was for…  \nFrom the docs: Using the $elemMatch query operator, you can match an entire document within an array.  \nThis does not sounds like what you’re looking for.\n\nIs there a generic way to search the nested-objects’ structure for particular values?  \nIt sounds like you want to search “everything in object ‘c’ for an instance of ‘e1'”.  \nMongoDB supports two related features, but the features not quite what you’re looking for.  \n1\\. Reach into objects, dot notation:db.test.find({‘a.b.c.d1’ : ‘e1’})  \n2\\. Read through arrays: \\`db.test.find({‘a.b.c.d4’ : ‘e5’})  \nIt sounds like you’re looking for the ability to do both at the same time. You want to “reach into objects” and “read through arrays” in the same query.  \nUnfortunately, I do not know of such a feature. You may want to file a feature request for this.\n\n**Related Page: [MongoDB Create Index](../../mongodb-create-index)**"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB Cursor Timeouts while doing a lot of writes",
      "explanation": "We have a cluster of 2 replica Sets, with 3 servers per set. With a single collection being sharded. We also have a quite a few more(8+) collections that we use on a daily basis. With the majority of the data being in the sharded collection with close to 100 Million records in it.\n\nRecently we have added the requirement to obtain 100x the data that we had been getting previously, and we need to write this to mongodb. A daemon has been set in place to perform the writes necessary to keep the database up to date. The script performs at over 200 writes a second, with the majority going to the all separate collections.  \nWith this quantity of writes, we have been unable to perform large reads for analytical purposes. Receiving a combination of Cursor Timeouts client-side and server-side(“Cursor Not Found”).\n\nWe have attempted to do limit/skip schemes on the reads, but the problem persists. What is the best course of action to remedy this as we require both a large amount of writes, with few, but large reads?\n\nTypically, in a case like this you want to start looking at the queries causing the time. Then you want to look at the hardware to see what’s being stressed.\n\n1.  Are these queries correctly indexed?\n2.  How big are the indexes? Do they fit in RAM?\n3.  Can you provide some details on where the bottlenecks are?\n4.  Are you locked on on IO?\n5.  Are your processors running at full speed?\n\nAlso, is there anything unusual in the logs?  \nBasically we need to ensure that you have: 1. Correctly built the system to handle the query 2. Correctly provisioned the system to handle the data volumes"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Is moving documents between collections a good way to represent state changes in MongoDB?",
      "explanation": "I have two collections, one (**A)** containing items to be processed (relatively small) and one (**B**) with those already processed (fairly large, with extra result fields).  \nItems are read from **A**, get processed and save()’d to **B**, then remove()’d from **A**.\n\nThe rationale is that indices can be different across these, and that the “incoming” collection can be kept very small and fast this way.  \nI’ve run into two issues with this:\n\n1.  if either remove() or save() time out or otherwise fail under load, I lose the item completely, or process it twice\n2.  if both fail, the side effects happen but there is no record of that I can sidestep the double-failure case with findAndModify locks (not needed otherwise, we have a process-level lock) but then we have stale lock issues and partial failures can still happen. There’s no way to atomically remove+save to different collections, as far as I can tell (maybe by design?)  \n    Is there a Best Practice for this situation?\n3.  There’s no way to atomically remove+save to different collections, as far as I can tell (maybe by design?)  \n    Yes this is by design. MongoDB explicitly does not provides joins or transactions. Remove + Save is a form of transaction.  \n    Is there a Best Practice for this situation?  \n    You really have two low-complexity options here, both involve findAndModify.  \n    **Option #1: a single collection**  \n    Based on your description, you are basically building a queue with some extra features. If you leverage a single collection then you use findAndModify to update the status of each item as it is processing.  \n    Unfortunately, that means you will lose this: …that the “incoming” collection can be kept very small and fast this way.  \n    **Option #2: two collections**  \n    The other option is basically a two phase commit, leveraging findAndModify.  \n    Take a look at the docs for this here.  \n    Once an item is processed in A you set a field to flag it for deletion. You then copy that item over toB. Once copied to B you can then remove the item from A."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "For Mongodb is it better to reference an object or use a natural String key?",
      "explanation": "I am building a corpus of indexed sentences in different languages. I have a collection of Languages which have both an ObjectId and the ISO code as a key. Is it better to use a reference to the Language collection or store a key like “en” or “fr”?  \nI suppose it’s a compromise between:\n\n1.  ease of referencing the Language\n2.  object in that collection\n3.  speed in doing queries where the sentence has a certain language\n4.  the size of the data on disk\n\nAny best practices that I should know of?  \nIn the end, it really comes down to personal choice and what will work best for your application.  \nThe only requirement that MongoDB imposes upon \\_id is that it be unique. It can be an ObjectId (which is provided by default), a string, even an embedded document (As I recall it cannot be an Array though).\n\nIn this case, you can likely guarantee ISO Code is a unique value and it may be an ideal value. You have a ‘known’ primary key which is also useful in itself by being identifiable, so using that instead of a generated ID is probably a more sensible bet. It also means anywhere you ‘reference’ this information in another collection you can save the ISO Code instead of an Object ID; those browsing your raw data can immediately identify what information that reference points at.\n\nAs an aside:  \nThe two big benefit of ObjectId is that they can be generated uniquely across multiple machines, processes and threads without needing any kind of central sequence tracking by the MongoDB server. They also are stored as a special type in MongoDB that only uses 12 bytes (as opposed to the 24 byte representation of the string version of an ObjectID)"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Does the MongoDB stats() function return bits or bytes?",
      "explanation": "When using MongoDB’s .stats() function to determine document size, are the values returned in bits or bytes?  \nBytes of course. Unless you pass in a scale as optional argument.  \nRunning the collStats command – db.collection.stats() – returns all sizes in bytes, e.g.\n\n    > db.foo.stats()\n    {\n    “size” : 715578011834,  // total size (bytes)\n    “avgObjSize” : 2862,    // average size (bytes)\n    }\n\nHowever, if you want the results in another unit then you can also pass in a scale argument.  \nFor example, to get the results in KB:\n\n    > db.foo.stats(1024)\n    {\n    “size” : 698806652,  // total size (KB)\n    “avgObjSize” : 2,    // average size (KB)\n    }\n    Or for MB:\n    > db.foo.stats(1024 * 1024)\n    {\n    “size” : 682428,    // total size (MB)\n    “avgObjSize” : 0,   // average size (MB)\n    }"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Storing Friend Relationships in MongoDB?",
      "explanation": "I was wondering what the best way of storing friend relationship data using MongoDB is?  \nComing from mysql I had a separate table with friend relationships that had two foreign keys, each pointing to a friend in the “friendship” however, with MongoDB its possible to have arrays of references or even embedded documents..so whats the best way to store these relationships  \nImmediate first reaction was that I’d store an array of friend object ids in each user, however, that troubles me, because then, in order to remove a “friendship” I would have to do deletes on both friend’s documents whereas if I stored the relationship in a separate collection (a la SQL) i could remove or add a relationship by just modifying one collection.  \nThanks!\n\nKeeping a list of friend\\_ids in a user, is what I’ll recommend. Few reasons,  \n1.You query a user, and you have list of all friends upfront available.  \n2.The requests (pending, accepted) can be handled well, by seeing that a respective ids should be present in both the user’s friends list. And I can get list of actual and accepted friends by querying\n\n    my_id, my_friends = user._id, user.friends\n    db.users.find({‘_id’:{‘$in’: my_friends}, ‘friends’: my_id})\n\nYes, while removing a friendship , you have to $pull from both the friend’s list of both users, but frequency of that would be much less. But you query less in getting the friend-list, which would be used frequently."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB – file size is huge and growing",
      "explanation": "I have an application that use mongo for storing short living data. All data older than 45 minutes is removed by script something like:  \noldSearches = \\[list of old searches\\]  \nconnection = Connection()  \ndb = connection.searchDB  \nres = db.results.remove{‘search\\_id’:{“$in”:oldSearches}})  \nI’ve checked current status –\n\n    >db.results.stats()\n    {\n    “ns” : “searchDB.results”,\n    “count” : 2865,\n    “size” : 1003859656,\n    “storageSize” : 29315124464,\n    “nindexes” : 1,\n    “ok” : 1\n    }\n\nSo, according to this 1gb of data occupies 29GB of storage. Data folder looks like this(You may see that many files are very old – last accessed in the middle of may):  \nSkip code block\n\n    ls -l /var/lib/mongodb/\n    total 31506556\n    -rwxr-xr-x 1 mongodb nogroup          6 2011-06-05 18:28 mongod.lock\n    -rw——- 1 mongodb nogroup   67108864 2011-05-13 17:45 searchDB.0\n    -rw——- 1 mongodb nogroup  134217728 2011-05-13 14:45 searchDB.1\n    -rw——- 1 mongodb nogroup 2146435072 2011-05-20 20:45 searchDB.10\n    -rw——- 1 mongodb nogroup 2146435072 2011-05-28 00:00 searchDB.11\n    -rw——- 1 mongodb nogroup 2146435072 2011-05-27 13:45 searchDB.12\n    -rw——- 1 mongodb nogroup 2146435072 2011-05-29 16:45 searchDB.13\n    -rw——- 1 mongodb nogroup 2146435072 2011-06-07 13:50 searchDB.14\n    -rw——- 1 mongodb nogroup 2146435072 2011-06-06 01:45 searchDB.15\n    -rw——- 1 mongodb nogroup 2146435072 2011-06-07 13:50 searchDB.16\n    -rw——- 1 mongodb nogroup 2146435072 2011-06-07 13:50 searchDB.17\n    -rw——- 1 mongodb nogroup 2146435072 2011-06-06 09:07 searchDB.18\n    -rw——- 1 mongodb nogroup  268435456 2011-05-13 14:45 searchDB.2\n    -rw——- 1 mongodb nogroup  536870912 2011-05-11 00:45 searchDB.3\n    -rw——- 1 mongodb nogroup 1073741824 2011-05-29 23:37 searchDB.4\n    -rw——- 1 mongodb nogroup 2146435072 2011-05-13 17:45 searchDB.5\n    -rw——- 1 mongodb nogroup 2146435072 2011-05-18 17:45 searchDB.6\n    -rw——- 1 mongodb nogroup 2146435072 2011-05-16 01:45 searchDB.7\n    -rw——- 1 mongodb nogroup 2146435072 2011-05-17 13:45 searchDB.8\n    -rw——- 1 mongodb nogroup 2146435072 2011-05-23 16:45 searchDB.9\n    -rw——- 1 mongodb nogroup   16777216 2011-06-07 13:50 searchDB.ns\n    -rw——- 1 mongodb nogroup   67108864 2011-04-23 18:51 test.0\n    -rw——- 1 mongodb nogroup   16777216 2011-04-23 18:51 test.ns\n    According to “top” mongod uses 29G of virtual memory ( and 780Mb of RSS)\n\nWhy do i have such abnormal values? Do i need to run something additional to .remove() function to clean up database from old values?\n\n**Virtual memory size** and resident size will appear to be very large for the mongod process. This is benign: virtual memory space will be just larger than the size of the datafiles open and mapped; resident size will vary depending on the amount of memory not used by other processes on the machine.\n\n[https://www.mongodb.org/display/DOCS/Caching](https://www.mongodb.org/display/DOCS/Caching)\n\nWhen you remove an object from MongoDB collection, the space it occupied is not automatically garbage collected and new records are only appended to the end of data files, making them grow bigger and bigger. This explains it all:\n\n[https://www.mongodb.org/display/DOCS/Excessive+Disk+Space](https://www.mongodb.org/display/DOCS/Excessive+Disk+Space)\n\nFor starters, simply use:\n\n    db.repairDatabase()"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongo does not have a max() function, how do I work around this?",
      "explanation": "I have a MongoDB collection and need to find the max() value of a certain field across all docs. This value is the timestamp and I need to find the latest doc by finding the largest timestamp. Sorting it and getting the first one gets inefficient really fast. Shall I just maintain a ‘maxval’ separately and update it whenever a doc arrives with a larger value for that field? Any better suggestions? Thanks much.\n\nFor sure if it will be big collection and if you need always display max timestamp you may need create separate collection and store statistic data there instead of order big collection each time.\n\n    statistic\n    {\n    _id = 1,\n    id_from_time_stamp_collection = ‘xxx’,\n    max_timestamp: value\n    }\n\nAnd whenever new doc come just update statistic collection with id = 1(with $gt condition in query, so if new timestamp will be greater than max\\_timestamp then max\\_timestamp will be updated, otherwise – no).  \nAlso probably you can store and update other statistic data within statistic collection.  \nif you have an index on the timestsamp field, finding the highest value is efficientl something like  \ndb.things.find().sort({ts:-1}).limit(1)  \nbut if having an index is too much overhead storing the max in a separate collection might be good."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongodb drop every database",
      "explanation": "I would like to know if there’re a command to drop every databases from my mongodb ?  \nI know if I want to drop only one datatable, I just need to type the name of the database like the code below but I dont want to have to specify it.  \nmongo DB\\_NAME –eval ‘db.dropDatabase();’  \nAny help would be appreciated.  \nThanks.  \nJohn  \nyou can create a javascript loop that do the job and then execute it in the mongoconsole.\n\n    var dbs = db.getMongo().getDBNames()\n    for(var i in dbs){\n    db = db.getMongo().getDB( dbs[i] );\n    print( “dropping db ” + db.getName() );\n    db.dropDatabase();\n    }\n    save it to dropall.js and then execute:\n    mongo dropall.js\n    Try this command:\n    mongo –quiet –eval ‘db.getMongo().getDBNames().forEach(function(i){db.getSiblingDB(i).dropDatabase()})’\n    You can do it easy through c# official driver:\n    var _mongoServer = MongoServer.Create(“mongodb://localhost:27020”);\n    var names = _mongoServer.GetDatabaseNames();\n    foreach (var name in names)\n    {\n    _mongoServer.DropDatabase(name);\n    }"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "mongodb find by comparing field values",
      "explanation": "Is it possible to express the following SQL query in mongodb:  \nSELECT \\* FROM table AS t WHERE t.field1 > t.filed2;\n\n**edit:** To summarize:.  \nusing a third field storing “field1 – field2” is almost perfect, but requires a little extra maintenance.  \n$where will load and eval in JavaScript and won’t use any indexes. No good for large data.  \nmap/reduce has the same problem and will go trough all records even if we need only one  \nYou can do this using $where:  \ndb.coll.find( { $where: “this.field1 > this.field2” } );  \n**But:**  \nJavascript executes more slowly than the native operators, but it is very flexible  \nIf performance is an issue better to go with way suggested by @yi\\_H.  \nYou could store in your document field1 – field2 as field3, then search for { field3: { $gt: 0 } }  \nIt also possible to get matching documents with mapreduce."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "insert to specific index for mongo array",
      "explanation": "Mongo supports arrays of documents inside documents. For example, something like\n\n    {_id: 10, “coll”: [1, 2, 3] }\n    Now, imagine I wanted to insert an arbitrary value at an arbitrary index\n    {_id: 10, “coll”: [1, {name: ‘new val’}, 2, 3] }\n\nI know you can update values in place with $ and $set, but nothing for insertion. it kind of sucks to have to replace the entire array just for inserting at a specific index.  \nNow you can achieve this with a new upcoming version 2.6. To do this you have to use $positionoperator. For your particular example:\n\n    db.students.update(\n    { _id: 10},\n    { $push: {\n    coll: {\n    $each: [ {name: ‘new val’} ],\n    $position: 1\n    }\n    }}\n    )"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Where does MongoDB store its documents?",
      "explanation": "I have inserted and fetched data using MongoDB, in PHP. Is there an actual copy of this data in a document somewhere?  \nMongoDB stores it’s data in the data directory specified by –dbpath. It uses a database format so it’s not actual documents, but there are multiple documents in each file and you cannot easily extract the data from this format yourself.\n\nTo read and/or update a document you need to use a MongoDB client, in the same way that you send SQL queries to MySQL through a MySQL client. You probably want to do it programmatically by using one of the client libraries for your programming language, but there is also a command-line client if you need to do manual updates.\n\nBy default Mongo stores its data in the directory /data/db.  \nYou can specify a different directory using the –dbpath option.  \nIf youre running Mongo on Windows it will be C:datadb (the driver letter will be the same as the working directory). To avoid confusion, Id recommend that you always specify a data directory using–dbpath."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What does it mean to fit “working set” into RAM for MongoDB?",
      "explanation": "MongoDB is fast, but only when your working set or index can fit into RAM. So if my server has 16G of RAM, does that mean the sizes of all my collections need to be less than or equal to 16G? How does one say “ok this is my working set, the rest can be “archived?”\n\n“Working set” is basically the amount of data AND indexes that will be active/in use by your system.  \nSo for example, suppose you have 1 year’s worth of data. For simplicity, each month relates to 1GB of data giving 12GB in total, and to cover each month’s worth of data you have 1GB worth of indexes again totalling 12GB for the year.\n\nIf you are always accessing the last 12 month’s worth of data, then your working set is: 12GB (data) + 12GB (indexes) = 24GB.  \nHowever, if you actually only access the last 3 month’s worth of data, then your working set is: 3GB (data) + 3GB (indexes) = 6GB. In this scenario, if you had 8GB RAM and then you started regularly accessing the past 6 month’s worth of data, then your working set would start to exceed past your available RAM and have a performance impact.  \nBut generally, if you have enough RAM to cover the amount of data/indexes you expect to be frequently accessing then you will be fine.  \nEdit: Response to question in comments\n\nI’m not sure I quite follow, but I’ll have a go at answering. Firstly, the calculation for working set is a “ball park figure”. Secondly, if you have a (e.g.) 1GB index on user\\_id, then only the portion of that index that is commonly accessed needs to be in RAM (e.g. suppose 50% of users are inactive, then 0.5GB of the index will be more frequently required/needed in RAM). In general, the more RAM you have, the better especially as working set is likely to grow over time due to increased usage. This is where sharding comes in – split the data over multiple nodes and you can cost effectively scale out. Your working set is then divided over multiple machines, meaning the more can be kept in RAM. Need more RAM? Add another machine to shard on to.\n\nThe working set is basically the stuff you are using most (frequently). If you use index A for collection B to search for a subset of documents then you could consider that your working set. As long as the most commonly used parts of those structures can fit in memory then things will be exceedingly fast. As parts no longer fit in your working set, like many of the documents then that can slow down. Generally things will become much slower if your indexes exceed your memory.\n\nYes, you can have lots of data, where most of it is “archived” and rarely used without affecting the performance of our application or impacting your working set (which doesn’t include that archived data)."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB: Updating a document in an array",
      "explanation": "I have a collection with documents of this schema:\n\n    {\n    _id: something,\n    recipients: [{id:1, name:”Andrey”, isread:false}, {id:2, name:”John”, isread:false}]\n    }\n    Now, I want to update “isread” for John (id = 2) using findAndModify(), because I also need to get the original document.\n    I’m trying this command:\n    db.messages.findAndModify({query:{‘recipients.id’:2}, update:{‘recipients.$.isread’:true}})\n    but what it does, it just replaces the whole “recipients” field with ‘recipients.$.isread’, so the document now looks like:\n    {\n    _id: someid,\n    ‘recipients.$.isread’:true\n    }\n\nWhat am I doing wrong?  \nTry to use $set like this:\n\ndb.messages.findAndModify({query:{‘recipients.id’:2}, update:{$set:{‘recipients.$.isread’:true}}})"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to limit number of updating documents in mongodb",
      "explanation": "How to implement somethings similar to db.collection.find().limit(10) but while updating documents?  \nNow I’m using something really crappy like getting documents withdb.collection.find().limit() and then updating them.  \nIn general I wanna to return given number of records and change one field in each of them.  \nThanks.\n\nUnfortunately the workaround you have is the only way to do it AFAIK. There is a boolean flagmulti which will either update all the matches (when true) or update the 1st match (when false).\n\nYou can use:\n\n    db.collection.find().limit(NUMBER_OF_ITEMS_YOU_WANT_TO_UPDATE).forEach(\n    function (e) {\n    e.fieldToChange = “blah”;\n    ….\n    db.collection.save(e);\n    }\n    );\n\n(Credits for forEach code: MongoDB: Updating documents using data from the same document)  \nWhat this will do is only change the number of entries you specify. So if you want to add a field called “newField” with value 1 to only half of your entries inside “collection”, for example, you can put in\n\n    db.collection.find().limit(db.collection.count() / 2).forEach(\n    function (e) {\n    e.newField = 1;\n    db.collection.save(e);\n    }\n    );\n\nIf you then want to make the other half also have “newField” but with value 2, you can do an update with the condition that newField doesn’t exist:  \ndb.collection.update( { newField : { $exists : false } }, { $set : { newField : 2 } }, {multi : true} );"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Compound indices for OR+Sort query in mongodb",
      "explanation": "For this OR query:\n\n    db.messages.find({ $or: [ { to: { $ne: null }, from: “xyz” }, { to: “xyz” } ] }).sort({_id: -1}).limit(50)\n\nwith following indices:\n\n    {to:1, from: 1, _id:-1} and {from:1, to:1, _id:-1}\n\nmongo is always doing a full scan.  \nI was hoping that mongo could use these two indices and merge the results.  \nDo I need to split this into two queries (one for each OR clause) and merge myself? Or is there some other index that can help?\n\nThis is a known issue, https://jira.mongodb.org/browse/SERVER-1205, which you can vote for if it is very important for your use case."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB: Unique index on array element’s property",
      "explanation": "I have a structure similar to this:\n\n    class Cat {\n    int id;\n    List kittens;\n    }\n    class Kitten {\n    int id;\n    }\n\nI’d like to prevent users from creating a cat with more than one kitten with the same id. I’ve tried creating an index as follows:  \ndb.Cats.ensureIndex({‘id’: 1, ‘kittens.id’: 1}, {unique:true})  \nBut when I attempt to insert a badly-formatted cat, Mongo accepts it.  \nAm I missing something? can this even be done?  \nAs far as I know, unique indexes only enforce uniqueness across different documents, so this would throw a duplicate key error:\n\n    db.cats.insert( { id: 123, kittens: [ { id: 456 } ] } )\n    db.cats.insert( { id: 123, kittens: [ { id: 456 } ] } )\n\nBut this is allowed:\n\n    db.cats.insert( { id: 123, kittens: [ { id: 456 }, { id: 456 } ] } )\n\nI’m not sure if there’s any way enforce the constraint you need at the Mongo level, maybe it’s something you could check in the application logic when you insert of update?"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Updating embedded document property in Mongodb",
      "explanation": "I have a document that looks like this:  \nSkip code block\n\n    {\n    “_id”: 3,\n    “Slug”: “slug”,\n    “Title”: “title”,\n    “Authors”: [\n    {\n    “Slug”: “slug”,\n    “Name”: “name”\n    }\n    ]\n    }\n\nI want to update all Authors.Name based on Authors.Slug. I tried this but it didn’t work:\n\n    .update({“Authors.Slug”:”slug”}, {$set: {“Authors.Name”:”zzz”}});\n\nWhat am I doing wrong here?\n\n    .update(Authors:{$elemMatch:{Slug:”slug”}}, {$set: {‘Authors.$.Name’:”zzz”}});"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What is the best way to store dates in MongoDB?",
      "explanation": "I am just starting to learn about MongoDB and hoping to slowly migrate from MySQL.\n\n[\n\n### MongoDB Certification Training !\n\nExplore Curriculum\n\n](https://mindmajix.com/mongodb-training#curriculum)\n\nIn MySQL, there are two different data types – DATE (‘0000-00-00’) and DATETIME (‘0000-00-00 00:00:00’). In my MySQL, I use the DATE type, but I am not sure how to transfer them into MongoDB. In MongoDB, there is a Date object, which is comparable to DATETIME. It seems it would be most appropriate to use Date objects, but that would be wasting space, since hours, min, sec are not utilized. On the other hand, storing dates as strings seems wrong.\n\nIs there a golden standard on storing dates (‘0000-00-00’) in MongoDB?\n\nBSON (the storage data format used by mongo natively) has a dedicated date type UTC datetime which is a 64 bit (so, 8 byte) signed integer denoting milliseconds since Unix time epoch. There are very few valid reasons why you would use any other type for storing dates and timestamps.\n\nIf you’re desperate to save a few bytes per date (again, with mongo’s padding and minimum block size and everything this is only worth the trouble in very rare cases) you can store dates as a 3 byte binary blob by storing it as an unsigned integer in YYYYMMDD format, or a 2 byte binary blob denoting “days since January 1st of year X” where X must be chosen appropriately since that only supports a date range spanning 179 years.\n\nEDIT: As the discussion below demonstrates this is only a viable approach in very rare circumstances. **Basically; use mongo’s native date type**   \nI’m actually in the process of converting a MongoDB database where dates are stored as proper Date() types to instead store them as strings in the form yyyy-mm-dd. Why, considering that every other answerer says that this is a horrible idea? Simply put, because of the neverending pain I’ve been suffering trying to work with dates in JavaScript, which has no (real) concept of timezones.\n\nI had been storing UTC dates in MongoDB, i.e. a Date() object with my desired date and the time set as midnight UTC, but it’s unexpectedly complicated and error-prone to get a user submitted date correctly converted to that from whatever timezone they happen to be in. I’ve been struggling to get my JavaScript “whatever local timezone to UTC” code to work (and yes, I’m aware of Sugar.js and Moment.js) and I’ve decided that simple strings like the good old MySQL standard yyyy-mm-dd is the way to go, and I’ll parse into Date() objects as needed at runtime on the client side.\n\nIncidentally, I’m also trying to sync this MongoDB database with a FileMaker database, which also has no concept of timezones. For me the simplicity of simply not storing time data, especially when it’s meaningless like UTC midnight, helps ensure less-buggy code even if I have to parse to and from the string dates now and then."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDb : Avoid excessive disk space",
      "explanation": "I am having a database which has been allocated a space of 85GB. I got this size using the show dbs command. But when I use the db.stats(), i get the storage size as 63GB. After going through the docs I found dat mongo db allocates a size to the db that is created and then the actual data is filled.\n\nWhy does mongo does this preallocation and is there a way I can avoid this preallocation ?  \nIf Yes, will it be a good idea to do it or it is going to affect from performance point of view ?  \nThanks!!!\n\nDetails can be found here: https://www.mongodb.org/display/DOCS/Excessive+Disk+Space  \nNow to your case. I think that the current size of your datafiles cannot be caused by preallocation alone. The maximum size of allocated data files is 2G. Maybe you once stored much more data than today? The link above also shows how to reclaim this data if you’re sure you will not need it soon anyway. This will require a lot of disk space during the process, so you must give this some planning.\n\nDisabling preallocation will indeed have perfomance impact. When you do many inserts, the process would regularly have to wait to create a new file."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Get n-th element of an array in mongo",
      "explanation": "As part of my document in mongo I’m storing an array of objects. How can I query it for only the 4th element of the array for example? So I don’t want the get the entire array out, just the 4th element.  \nUse $slice.\n\n    db.foo.find({ bar : “xyz” } , { my_array : { $slice : [n , 1] } } )\n\nwill retrieve the nth element of the array “my\\_array” of all documents in the foo collection where bar = “xyz”.  \nSome other examples from the MongoDB documentation:\n\n    db.posts.find({}, {comments:{$slice: 5}}) // first 5 comments\n    db.posts.find({}, {comments:{$slice: -5}}) // last 5 comments\n    db.posts.find({}, {comments:{$slice: [20, 10]}}) // skip 20, limit 10\n    db.posts.find({}, {comments:{$slice: [-20, 10]}}) // 20 from end, limit 10\n\nWhich you can read here: [https://www.mongodb.org/display/DOCS/Retrieving+a+Subset+of+Fields](https://www.mongodb.org/display/DOCS/Retrieving+a+Subset+of+Fields)\n\nAnother way to do this is to use the update array syntax. Here, contribs.1 sets the second element in the contribs array to have value ALGOL 58 (Taken from the manual page on update syntax)\n\n    db.bios.update(\n    { _id: 1 },\n    { $set: { ‘contribs.1’: ‘ALGOL 58’ } }\n    )"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to delete n-th element of array in mongodb",
      "explanation": "For example i have a document\n\n    db.test.save({_id: 1, list:[\n    {key: “a”},\n    {key: “b”},\n    {key: “c”},\n    {key: “d”},\n    {key: “e”}\n    ]})\n\n  \nand i need remove second element from the list. For now I do that in a two steps. First of all I unset second list element but unset operator do not remove element, it is going to be null, after that I pull any nullable value from the list field\n\n    db.test.update({_id: 1}, {$unset: {“list.2”: 1}})\n    db.test.update({_id: 1}, {$pull: {list: null}})\n\nI want to ask whether there is solution do that in a one operation?  \nNo, unfortunately what you are doing is currently the best option. Have a look at this question: In mongoDb, how do you remove an array element by it’s index which links to a Jira for this very issue."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Slow pagination over tons of records in mongo",
      "explanation": "I have over 300k records in one collection in Mongo.  \nWhen I run this very simple query:  \ndb.myCollection.find().limit(5);  \nIt took only few miliseconds. When I use skip in the query:  \ndb.myCollection.find().skip(200000).limit(5)  \nIt won’t return anything… it is running minutes and return nothing.  \nHow to make it better?  \nFrom MongoDB documentation:\n\n**Paging Costs**\n\nUnfortunately skip can be (very) costly and requires the server to walk from the beginning of the collection, or index, to get to the offset/skip position before it can start returning the page of data (limit). As the page number increases skip will become slower and more cpu intensive, and possibly IO bound, with larger collections.  \nRange based paging provides better use of indexes but does not allow you to easily jump to a specific page.\n\nYou have to ask yourself a question: how often do you need 40000th page? Also see this article;  \nOne approach to this problem, if you have large quantities of documents and you are displaying them in sorted order (I’m not sure how useful skip is if you’re not) would be to use the key you’re sorting on to select the next page of results.\n\nSo if you start with\n\n    db.myCollection.find().limit(100).sort(created_date:true);\n\nand then extract the created date of the last document returned by the cursor into a variablemax\\_created\\_date\\_from\\_last\\_result, you can get the next page with the far more efficient (presuming you have an index on created\\_date) query\n\n    db.myCollection.find({created_date : { $gt : max_created_date_from_last_result} }).limit(100).sort(created_date:true);"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to use variables in MongoDB Map-reduce map function",
      "explanation": "Given a document\n\n    {_id:110000, groupings:{A:’AV’,B:’BV’,C:’CV’,D:’DV’},coin:{old:10,new:12}}\n\nMy specs call for the specification of attributes for mapping and aggregation at run time, as the groupings the interested in are not known up front, but specified by the user.  \nFor example, one user would specify \\[A,B\\] which will cause mapping emissions of\n\n    emit( {A:this.groupings.A,B:this.groupings.B},this.coin )\n\nwhile another would want to specify \\[A,C\\] which will cause mapping emissions of\n\n    emit( {A:this.groupings.A,C:this.groupings.C},this.coin )\n\nB/c the mapper and reducer functions execute server side, and don’t have access to client variables, I haven’t been able to come up with a way to use a variable map key in the mapper function.\n\nIf I could reference a list of things to group by from the scope of the execution of the map function, this is all very strait forward. However, b/c the mapping function end up getting from from a different scope, I don’t know how to do this, or if it’s even possible.\n\nBefore I start trying to dynamically build java script to execute through the driver, does anyone have a better suggestion? Maybe a ‘group’ function will handle this scenario better?  \nYou can pass global, read-only data into map-reduce functions using the “scope” parameter on the map-reduce command. It’s not very well documented, I’m afraid. As pointed out by @Dave Griffith, you can use the scope parameter of the mapReduce function.\n\nI struggled a bit to figure out how to properly pass it to the function because, as pointed out by others, the documentation is not very detailed. Finally, I realised that mapReduce is expecting 3 params:  \n1\\. map function  \n2\\. reduce function  \n3\\. object with one or more of the params definedin the doc\n\nEventually, I arrived at the following code in Javascript:\n\n    // I define a variable external to my map and to my reduce functions\n    var KEYS = {STATS: “stats”};\n    function m() {\n    // I use my global variable inside the map function\n    emit(KEYS.STATS, 1);\n    }\n    function r(key, values) {\n    // I use a helper function\n    return sumValues(values);\n    }\n    // Helper function in the global scope\n    function sumValues(values) {\n    var result = 0;\n    values.forEach(function(value) {\n    result += value;\n    });\n    return value;\n    }\n    db.something.mapReduce(\n    m,\n    r,\n    {\n    out: {inline: 1},\n    // I use the scope param to pass in my variables and functions\n    scope: {\n    KEYS: KEYS,\n    sumValues: sumValues // of course, you can pass function objects too\n    }\n    }\n    );"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "mongo dbname –eval ‘db.collection.find()’ does not work",
      "explanation": "Why does this work:\n\n    # mongo dbname\n    MongoDB shell version: 1.8.3\n    connecting to: nextmuni_staging\n    > db.collection.find()\n    { “foo” : “bar” }\n    > bye\n\nWhile this does not work:\n\n    # mongo localhost/dbname –eval ‘db.collection.find()’\n    MongoDB shell version: 1.8.3\n    connecting to: localhost/dbname\n    DBQuery: dbname.collection -> undefined\n\nIt should be exactly the same, no?  \nThanks!  \nThe return val of db.collection.find() is a cursor type. Executing this command from within the shell will create a cursor and show you the first page of data. You can start going through the rest by repeating the ‘it’ command.\n\nI think the scope of variables used during the execution of an eval’d script is only for the lifetime of the script (data can be persisted into collections of course) so once the script terminates those cursor variables no longer exist and so you would be able to send another eval script to page the data. So the behaviour you get during a shell session wouldn’t really work from an eval script.\n\nTo get close to the behaviour you could run something like this:  \nmongo dbname –eval “db.collection.find().forEach(function(x){printjson(x);})”  \nThat shows you that the command does execute and produce a cursor which you can then iterate over sending the output to stdout.  \nEdit: I think the point I was trying to make was that the command you are issuing is working its just the output is not what you expect."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Compact command not freeing up space in MongoDB 2.0",
      "explanation": "I just installed MongoDB 2.0 and tried to run the compact command instead of the repair command in earlier versions. My database is empty at the moment, meaning there is only one collection with 0 entries and the two system collections (indices, users). Currently the db takes about 4 GB of space on the harddisk. The db is used as a temp queue with all items being removes after they have been processed.\n\nI tried to run the following in the mongo shell.  \nuse mydb  \ndb.theOnlyCollection.runCommand(“compact”)  \nIt returns with  \nok: 1  \nBut still the same space is taken on the harddisk. I tried to compact the system collections as well, but this did not work.  \nWhen I run the normal repair command  \ndb.repairDatabase()  \nthe database is compacted and only takes 400 MB.  \nAnyone has an idea why the compact command is not working?  \nThanks a lot for your help.  \nBest  \nAlex  \nCollection compaction is not supposed to decrease the size of data files. Main point is to defragment collection and index data – combine unused space gaps into continuous space allowing new data to be stored there. Moreover it may actually increase the size of data files:  \nCompaction may increase the total size of your data files by up to 2GB. Even in this case, total collection storage space will decrease.\n\nhttps://www.mongodb.org/display/DOCS/compact+Command#compactCommand-Effectsofacompaction"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB: order by two fields sum",
      "explanation": "SELECT (a+b) as c FROM my\\_table ORDER BY c ASC;  \nHow can I order by two columns sum in Mongo?  \nYou can’t do it easy without an extra action.  \nTo sort on any computed value you need to store it in a document first or in other worlds you need to create extra field ‘c’, and store a + b in it with each update/insert and only then sort on ‘c’ as usual."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Is it practical to use Mongo’s capped collection a memory cache?",
      "explanation": "Is it practical to use Mongo’s capped collections as poor man’s memcache assuming it will be kept in memory most of the time?  \nYou can definitely use capped collections for this purpose. But it’s important to know the basic limitations.\n\n1\\. Data will expire based on insert order not time-to-expire  \n2\\. Data that is frequently accessed may still be pushed out of memory  \n3\\. \\_idcolumns are not defined on Capped Collections by default, you will need to ensure those indexes.  \n4\\. The objects cannot be modified in a way that changes the size of the object. For example, youcouldincrement an existing integer, but you cannot add a field or modify a string value on the entry.  \n5\\. The capped collection scannot be sharded.\n\nBecause of #1 & #4 & #5, you are definitely losing some the core Memcache functionality.  \nThere is a long-outstanding JIRA ticket for TTL-based capped collections which is probably exactly what you want.  \nOf course, the big question in this whole debate is “where is the extra RAM”. Many people who use MongoDB as their primary store simply drop Memcache. If you have a bunch of extra RAM sitting around, why not just use it store the actual data instead of copies of that data?"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB Path Change?",
      "explanation": "My server went down from an electrical failure and for a few horrifying seconds, I thought I’d lost all MongoDB data. I then realized that when the server restarted, mongo automatically restarted without the –dbpath option.\n\nWhat I can’t figure out is why, even though my mongodb.conf has the dbpath set to /var/lib/mongodb, mongo looked for the db files in /data/db on startup. Can anyone shed some light on this?  \nThanks!\n\n/data/db is the default path that mongod will look for data files in if it is started with no options. Does your startup script instruct mongod to load the correct config file? If not, that would explain this behavior.\n\nWhen was the last time you updated Mongod on your system? and how did you update it?\n\nDepending how you installed / updated Mongod it could happen that either the /etc/init.d/mongo script or the /etc/mongo.conf files could be overwritten.  \nOr somebody edited those files a long time ago, and this is the first time Mongo got restarted, and now it looks like there’s suddenly a change.  \nI haven’t seen either of those two things happen in a long time (but if you’re using Gentoo, you would probably see this happen)  \nWhat OS do you use?  \nIt’s a good idea to keep backups of those files…"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "In MongoDB, how do I find documents where array size is greater than 1?",
      "explanation": "I have a MongoDB collection (accommodations) that has documents in the following format:\n\n    Skip code block\n    {\n    “_id” : ObjectId(“4e8ae86d08101908e1000001”),\n    “name” : [“Some Name”],\n    “zipcode” : [“2223”]\n    }\n    {\n    “_id” : ObjectId(“4e8ae86d08101908e1000002”),\n    “name” : [“Another “, “Name”],\n    “zipcode” : [“2224”]\n    }\n    {\n    “_id” : ObjectId(“4e8ae86d08101908e1000003”),\n    “name” : [“Yet”, ” Another “, “Name”],\n    “zipcode” : [“2225”]\n    }\n\nMy question is: How can I select all documents that have a name array size of greater than 1?  \nI can get documents that match an array size:\n\n    db.accommodations.find({“name”:{$size:2 }})\n\nThis will correctly return the documents where there are 2 elements in the name array. However I can’t do a $gt command to return all documents where the name field has an array size of greater than 2:\n\n    db.accommodations.find({“name”:{$size: {$gt:1} }})\n\nAny ideas how I can do this on existing data… preferably without modifying the data?  \nMany thanks\n\n**Update:**\n\nFor mongodb versions 2.2+ more efficient way to do this described by @JohnnyHK in anotheranswer.  \n1.Using $where  \ndb.accommodations.find( { $where: “this.name.length > 1” } );  \nBut…  \nJavascript executes more slowly than the native operators listed on this page, but is very flexible. See the server-side processing page for more information.\n\n2.Create **extra** field NamesArrayLength, update it with names array length and then use in queries:  \ndb.accommodations.find({“NamesArrayLength”: {$gt: 1} });  \nIt will be better solution, and will work much faster (you can create index on it).  \nThere’s a more efficient way to do this in MongoDB 2.2+ now that you can use numeric array indexes in query object keys.  \n// Find all docs that have at least a second name array element.  \ndb.accommodations.find({‘name.1’: {$exists: true}})  \nI believe this is the fastest query that answers your question, because it doesn’t use an interpreted$where clause:\n\n    {$nor: [\n    {name: {$exists: false}},\n    {name: {$size: 0}},\n    {name: {$size: 1}}\n    ]}\n\nIt means “all documents except those without a name (either non existant or empty array) or with just one name.”  \nTest:  \nSkip code block\n\n    > db.test.save({})\n    > db.test.save({name: []})\n    > db.test.save({name: [‘George’]})\n    > db.test.save({name: [‘George’, ‘Raymond’]})\n    > db.test.save({name: [‘George’, ‘Raymond’, ‘Richard’]})\n    > db.test.save({name: [‘George’, ‘Raymond’, ‘Richard’, ‘Martin’]})\n    > db.test.find({$nor: [{name: {$exists: false}}, {name: {$size: 0}}, {name: {$size: 1}}]})\n    { “_id” : ObjectId(“511907e3fb13145a3d2e225b”), “name” : [ “George”, “Raymond” ] }\n    { “_id” : ObjectId(“511907e3fb13145a3d2e225c”), “name” : [ “George”, “Raymond”, “Richard” ] }\n    { “_id” : ObjectId(“511907e3fb13145a3d2e225d”), “name” : [ “George”, “Raymond”, “Richard”, “Martin” ] }\n    >"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "heterogeneous bulk update in mongodb",
      "explanation": "I know that we can bulk update documents in mongodb with  \ndb.collection.update( criteria, objNew, upsert, multi )  \nin one db call, but it’s homogeneous, i.e. all those documents impacted are following one kind of criteria. But what I’d like to do is something like  \ndb.collection.update(\\[{criteria1, objNew1}, {criteria2, objNew2}, …\\]  \n, to send multiple update request which would update maybe absolutely different documents or class of documents in single db call.  \nWhat I want to do in my app is to insert/update a bunch of objects with compound primary key, if the key is already existing, update it; insert it otherwise.  \nCan I do all these in one combine in mongodb?  \nThat’s two seperate questions. To the first one; there is no MongoDB native mechanism to bulk send criteria/update pairs although technically doing that in a loop yourself is bound to be about as efficient as any native bulk support.  \nChecking for the existence of a document based on an embedded document (what you refer to as compound key, but in the interest of correct terminology to avoid confusion it’s better to use the mongo name in this case) and insert/update depending on that existence check can be done with upsert :\n\n    document A :\n    {\n    _id: ObjectId(…),\n    key: {\n    name: “Will”,\n    age: 20\n    }\n    }\n\ndb.users.update({name:”Will”, age:20}, {$set:{age: 21}}), true, false)  \nThis upsert (update with insert if no document matches the criteria) will do one of two things depending on the existence of document A :  \n1\\. Exists : Performs update “$set:{age:21}” on the existing document  \n2\\. Doesn’t exist : Create a new document with fields “name” and field “age” with values “Will” and “20” respectively (basically the criteria are copied into the new doc) and then the update is applied ($set:{age:21}). End result is a document with “name”=”Will” and “age”=21.  \nHope that helps"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Is it possible to mongodump the last “x” records from a collection?",
      "explanation": "Can you use mongodump to dump the latest “x” documents from a collection? For example, in the mongo shell you can execute:  \ndb.stats.find().sort({$natural:-1}).limit(10);  \nIs this same capability available to mongodump?  \nI guess the workaround would be to dump the above documents into a new temporary collection and mongodump the entire temp collection, but would be great to just be able to do this via mongodump.  \nThanks in advance,\n\nMichael  \nmongodump does not fully expose the cursor interfaces. But you can work around it, using the –query parameter. First get the total number of documents of the collection  \ndb.collection.count()  \nLet’s say there are 10000 documents and you want the last 1000. To do so get the id of first document you want to dump.\n\n    db.collection.find().sort({_id:1}).skip(10000 – 1000).limit(1)\n\nIn this example the id was “50ad7bce1a3e927d690385ec”. Now you can feed mongodump with this information, to dump all documents a with higher or equal id.  \n$ mongodump -d ‘your\\_database’ -c ‘your\\_collection’ -q ‘{\\_id: {$lte: ObjectId(“50ad7bce1a3e927d690385ec”)}}’\n\n**UPDATE** the new parameters ‘–limit’ and ‘–skip’ were added to mongoexport will be probably available in the next version of the tool: https://github.com/mongodb/mongo/pull/307  \nmongodump supports the –query operator. If you can specify your query as a json query, you should be able to do just that.\n\nIf not, then your trick of running a query to dump the records into a temporary collection and then dumping that will work just fine. In this case, you could automate the dump using a shell script that calls a mongo with a javascript command to do what you want and then calling mongodump."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How do you get the size of an individual index in MongoDB?",
      "explanation": "I know I can use db.collection.totalIndexSize() to get the total index size, but I’m interested in seeing the size of an individual index.  \nIs this supported?\n\nCertainly can. db.collection.stats().indexSizes is an embedded document where each index name is a key and the value is the total index size in bytes :\n\n    > db.test.stats()\n    {\n    “ns” : “test.test”,\n    \n    “indexSizes” : {\n    “_id_” : 137904592,\n    “a_1” : 106925728\n    },\n    “ok” : 1\n    }"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "mongodb Mongod complains that there is no /data/db folder",
      "explanation": "I am using my new mac for the first time today. I am following the get started guide on the mongodb.org up until the step where one creates the /data/db directory. btw, I used the homebrew route.\n\nSo I open a terminal, and I think I am at what you called the Home Directory, for when I do “ls”, I see folders of Desktop Application Movies Music Pictures Documents and Library.  \nSo I did a  \nmkdir -p /data/db  \nfirst, it says permission denied. I kept trying different things for half and hour and finally :  \nmkdir -p data/db  \nworked. and when I “ls”, a directory of data and nested in it a db folder do exist.  \nthen I fire up mongod and it complains about not finding data/db  \nHave I done something wrong?  \nNow I have done the  \nsudo mkdir -p /data/db  \nand when I do a “ls” I do see the data dir and the db dir. inside the db dir though, there is absolutely nothing in it and when I now run mongod\n\nSkip code block\n\n    Sun Oct 30 19:35:19 [initandlisten] exception in initAndListen: 10309 Unable to create/openlock file: /data/db/mongod.lock errno:13 Permission denied Is a mongod instance already running?, terminating\n    Sun Oct 30 19:35:19 dbexit:\n    Sun Oct 30 19:35:19 [initandlisten] shutdown: going to close listening sockets…\n    Sun Oct 30 19:35:19 [initandlisten] shutdown: going to flush diaglog…\n    Sun Oct 30 19:35:19 [initandlisten] shutdown: going to close sockets…\n    Sun Oct 30 19:35:19 [initandlisten] shutdown: waiting for fs preallocator…\n    Sun Oct 30 19:35:19 [initandlisten] shutdown: lock for final commit…\n    Sun Oct 30 19:35:19 [initandlisten] shutdown: final commit…\n    Sun Oct 30 19:35:19 [initandlisten] shutdown: closing all files…\n    Sun Oct 30 19:35:19 [initandlisten] closeAllFiles() finished\n    Sun Oct 30 19:35:19 [initandlisten] shutdown: removing fs lock…\n    Sun Oct 30 19:35:19 [initandlisten] couldn’t remove fs lock errno:9 Bad file descriptor\n    Sun Oct 30 19:35:19 dbexit: really exiting now\n    EDIT Getting error message for\n    sudo chown mongod:mongod /data/db\n    chown: mongod: Invalid argument\n\nThanks, everyone!  \n**You created the directory in the wrong place**\n\n/data/db means that it’s directly under the ‘/’ root directory, whereas you created ‘data/db’ (without the leading /) probably just inside another directory, such as the ‘/root’ homedirectory.\n\n**You need to create this directory as root**\n\nEither you need to use sudo , e.g. sudo mkdir -p /data/db  \nOr you need to do su – to become superuser, and then create the directory with mkdir -p /data/db\n\nNote:  \nMongoDB also has an option where you can create the data directory in another location, but that’s generally not a good idea, because it just slightly complicates things such as DB recovery, because you always have to specify the db-path manually. I wouldn’t recommend doing that.\n\nEdit:  \nthe error message you’re getting is “U**nable to create/open lock file: /data/db/mongod.lock errno:13 Permission denied**”. The directory you created doesn’t seem to have the correct permissions and ownership — it needs to be writable by the user who runs the MongoDB process.  \nTo see the permissions and ownership of the ‘/data/db/’ directory, do this: (this is what the permissions and ownership should look like)  \n$ ls -ld /data/db/  \ndrwxr-xr-x 4 mongod mongod 4096 Oct 26 10:31 /data/db/  \nThe left side ‘drwxr-xr-x’ shows the permissions for the User, Group, and Others. ‘mongod mongod’ shows who owns the directory, and which group that directory belongs to. Both are called ‘mongod’ in this case.\n\n**If your ‘/data/db’ directory doesn’t have the permissions and ownership above, do this:  \nsudo chmod 0755 /data/db**  \nsudo chown mongod:mongod /data/db  \nthat should make it work..\n\nCheck here to better understand the meaning of the directory permissions:\n\n[https://www.dartmouth.edu/~rc/help/faq/permissions.html](https://www.dartmouth.edu/~rc/help/faq/permissions.html)\n\nMaybe also check out one of the tutorials you can find via Google: “UNIX for beginners”  \nAfter getting the same error as Nik  \nchown: id -u: Invalid argument  \nI found out this apparently occurred from using the wrong type of quotation marks (should have been **backquotes**) Ubuntu Forums  \nInstead I just used  \nsudo chown $USER /data/db  \nas an alternative and now mongod has the permissions it needs."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "mongodb replicaset host name change error",
      "explanation": "I have a mongodb replicaset on ubuntu.. In replica set, hosts are defined as localhost. You can see ;\n\n    Skip code block\n    {\n    “_id” : “myrep”,\n    “version” : 4,\n    “members” : [\n    {\n    “_id” : 0,\n    “host” : “localhost:27017”\n    },\n    {\n    “_id” : 2,\n    “host” : “localhost:27018”\n    },\n    {\n    “_id” : 1,\n    “host” : “localhost:27019”,\n    “priority” : 0\n    }\n    ]\n    }\n\nI want to change host adresses with real ip of server. But when i run rs.reconfig, I get error :\n\n    {\n    “assertion” : “hosts cannot switch between localhost and hostname”,\n    “assertionCode” : 13645,\n    “errmsg” : “db assertion failure”,\n    “ok” : 0\n    }\n\nHow can i solve it ? Thank you.  \nThere is a cleaner way to do this:  \nuse local\n\n    cfg = db.system.replset.findOne({_id:”replicaSetName”})\n    cfg.members[0].host=”newHost:27017?\n    db.system.replset.update({_id:”replicaSetName”},cfg)\n\nthen restart mongo"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongo compound indexes, using less-than-all in a query",
      "explanation": "I understand that with MongoDB, for a query to make use of a compound index it must use ALL of the keys in the index, or at least some of the keys starting from the left. For example  \ndb.products.find({ “a”:”foo”, “b”:”bar” })  \nWill happily make use of an index made up of {a, b, c}.  \nHowever, if I want to query:  \ndb.products.find( {“a”:”foo”, “c”:”thing” })  \nI believe this can’t use the index. Could this be solved by adding a trivial condition on “b”, e.g.  \ndb.products.find( {“a”:”foo”, “b”:{ $ne : “” }, “c”:”thing” })  \nEven when I don’t actually care about the value of b. The reason for this is that we currently have 45m objects, and it’s going to continue growing so we’re looking to consolidate our indexes to save on resources.  \nMany thanks.\n\nIn general, a query on a multi-column index that does not sufficiently limit matches for one of the columns will restrict the usefulness of the multi-column index. In your example, using query criteria of {“a”:”foo”, “b”:{$ne:””}, “c”:”thing”} will limit the usefulness of an {a,b,c} index to matching only on a. If your query criteria will be executed often, create an {a,c,b} index (or {a,c} ifb will not be used in query criteria).\n\nUse the explain function on your queries to see if an index is being used to its full potential. Ifexplain tells you indexOnly is true, then your query is only using the index to find matching documents; otherwise, MongoDB needs to look at each document to find matches.  \nFor further information, see:  \n1\\. Optimization  \n2\\. Indexes  \n3\\. Query Optimizer"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "New to MongoDB Can not run command mongo",
      "explanation": "I was trying to run MongoDB:\n\n    Skip code block\n    E:mongobin>mongod\n    mongod –help for help and startup options\n    Sun Nov 06 18:48:37\n    Sun Nov 06 18:48:37 warning: 32-bit servers don’t have journaling enabled by default. Please use –journal if you want durability.\n    Sun Nov 06 18:48:37\n    Sun Nov 06 18:48:37 [initandlisten] MongoDB starting : pid=7108 port=27017 dbpath=/data/db 32-bit host=pykhmer-PC\n    Sun Nov 06 18:48:37 [initandlisten]\n    Sun Nov 06 18:48:37 [initandlisten] ** NOTE: when using MongoDB 32 bit, you are limited to about 2 gigabytes of data\n    Sun Nov 06 18:48:37 [initandlisten] **       see https://blog.mongodb.org/post/137788967/32-bit-limitations\n    Sun Nov 06 18:48:37 [initandlisten] **       with –journal, the limit is lower\n    Sun Nov 06 18:48:37 [initandlisten]\n    Sun Nov 06 18:48:37 [initandlisten] db version v2.0.1, pdfile version 4.5\n    Sun Nov 06 18:48:37 [initandlisten] git version: 3a5cf0e2134a830d38d2d1aae7e88cac31bdd684\n    Sun Nov 06 18:48:37 [initandlisten] build info: windows (5, 1, 2600, 2, ‘Service Pack 3’) BOOST_LIB_VERSION=1_42\n    Sun Nov 06 18:48:37 [initandlisten] options: {}\n    Sun Nov 06 18:48:37 [initandlisten] exception in initAndListen: 10296 dbpath (/data/db) does not exist, terminating\n    Sun Nov 06 18:48:37 dbexit:\n    Sun Nov 06 18:48:37 [initandlisten] shutdown: going to close listening sockets…\n    Sun Nov 06 18:48:37 [initandlisten] shutdown: going to flush diaglog…\n    Sun Nov 06 18:48:37 [initandlisten] shutdown: going to close sockets…\n    Sun Nov 06 18:48:37 [initandlisten] shutdown: waiting for fs preallocator…\n    Sun Nov 06 18:48:37 [initandlisten] shutdown: closing all files…\n    Sun Nov 06 18:48:37 [initandlisten] closeAllFiles() finished\n    Sun Nov 06 18:48:37 dbexit: really exiting now\n    E:mongobin>mongo\n    MongoDB shell version: 2.0.1\n    connecting to: test\n    Sun Nov 06 18:48:42 Error: couldn’t connect to server 127.0.0.1 shell/mongo.js:84\n    exception: connect failed\n    E:mongo>ls\n    GNU-AGPL-3.0  README  THIRD-PARTY-NOTICES  bin  data\n\nI was looking at\n\n[https://www.mongodb.org/display/DOCS/Quickstart+Windows](https://www.mongodb.org/display/DOCS/Quickstart+Windows)\n\nand following the instructions. Could anyone tell me what is the problem with running MongoDB (I am using Windows 7)?  \nI think your log output states it clearly;\n\nexception in initAndListen: 10296 dbpath (/data/db) does not exist, terminating  \nYou may simply create this directory or better to define it as a configuration value within your configuration file then use it as mongod -f C:pathtoyourmongodb.conf.  \nAfter you’ve installed MongoDB you should firstly create a data directory for your db.  \nBy default MongoDB will store data in /data/db,  \nbut it won’t automatically create that directory. To create it, do:\n\n    $ sudo mkdir -p /data/db/\n    $ sudo chown `id -u` /data/db\n\nYou can also tell MongoDB to use a different data directory,  \nwith the –dbpath option.  \nFor more detailed information go to MongoDB wiki page.  \nSpecify the database path explicitly like so, and see if that resolves the issue.\n\n    mongod –dbpath data/db\n    mongod –dbpath “c://data/db”\n\nrun the above code, this will start the server.  \nCheck that path to database data files exists :  \nSun Nov 06 18:48:37 \\[initandlisten\\] **exception** in initAndListen: 10296 **dbpath (/data/db) does not exist,** terminating"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB: Mapreduce: reduce->multiple not supported yet",
      "explanation": "I have a MongoDB collection (named “catalog”) containing about 5 astronomical catalogs. Several of these catalogs reference each other, so one of the the documents might look something like this:\n\n    { “_id” : ObjectId(“4ec574a68e4e7a519166015f”), “bii” : 20.9519, “class” : 2480, “cpdname” : “CPD -21 6109”, “decdeg” : -21.8417, “decpm” : 0.004, “dmname” : “-21 4299”, “hdname” : “HD 145612”, “lii” : 352.8556, “name” : “PPM 265262”, “ppmname” : “PPM 265262”, “radeg” : 243.2005, “rapm” : 0.0012, “vmag” : 9.6, “xref” : [ ] }\n\nWhat I want to do is use mapreduce to move the fields such as “hdname”,”ppmname”, etc into the xref array (then unset them).  \nSo I try to do this one at a time, starting with the hdname field. Here are the map and reduce functions:\n\n    Skip code block\n    map = function() {\n    for (var hdname in this.hdname) {\n    emit(this._id,this.hdname);\n    }\n    }\n    reduce = function(key, values) {\n    var result = [];\n    for (var hdname in values) {\n    result.push(hdname);\n    }\n    return result;\n    }\n\nI try running the following command in the mongo shell:  \ndb.catalog.mapReduce(map, reduce,”catalog2?);  \nUnfortunately, I get the following error:  \nThu Nov 17 15:52:17 uncaught exception: map reduce failed:\n\n    {\n    “assertion” : “reduce -> multiple not supported yet”,\n    “assertionCode” : 10075,\n    “errmsg” : “db assertion failure”,\n    “ok” : 0\n    }\n\nObviously I’m a newbie… can anyone help?  \nJason  \nThe documentation says “Currently, the return value from a reduce function cannot be an array (it’s typically an object or a number).”  \nSo create an object instead and wrap your array in that. Make sure also that the output of reduce is the same as the input type, so you’ll need to emit a similar value in the map operation.  \nBUT … why use Map-Reduce to do this? If you emit the \\_id value there’s nothing to reduce as each key will be unique. Why not just iterate over the collection copying the values and updating each record one by one?"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB sorting",
      "explanation": "I want implement a “bump” feature for topics. Once a topic is bumped, it will have a new “bump\\_date” field. I want to sort it so that when there is a “bump\\_date” field, it will be sorted as if it was the “created” field. Here’s an example of my db.topics:\n\n    Skip code block\n    {\n    “text” : “test 1”,\n    “created” : “Sun Nov 20 2011 02:03:28 GMT-0800 (PST)”\n    },\n    {\n    “text” : “test 2”,\n    “created” : “Sun Nov 18 2011 02:03:28 GMT-0800 (PST)”\n    },\n    {\n    “text” : “test 3”,\n    “created” : “Sun Nov 17 2011 02:03:28 GMT-0800 (PST)”,\n    “bump_date: : “Sun Nov 19 2011 02:03:28 GMT-0800 (PST)”\n    }\n\nI want the sort to return in the order of “test 1”, “test 3”, “test 2”  \nSorting in MongoDB is done like so:  \ndb.collection.find({ … spec … }).sort({ key: 1 })  \nwhere 1 is ascending and -1 is descending.  \nIn your specific example: db.topics.find().sort({ bump\\_date: 1 }), although it might be better to call it something like “updated\\_at”.  \nYou’ll also definitely want to put an index on your “bump\\_date” field.\n\n    sorting: https://www.mongodb.org/display/DOCS/Sorting+and+Natural+Order\n    indexes: https://www.mongodb.org/display/DOCS/Indexes\n\nAs Brian Hicks suggested, creating an additional updated\\_at field is the way to go. This way, when a document is created you can have created\\_at and updated\\_at initially be the same.\n\n    {\n    “created_at”: xxx,\n    “updated_at”: xxx\n    }\n\nIf you then “bump” the updated\\_at field by setting it to the current time when there is a a bump event you can sort on the updated\\_at field to achieve the ordering you desire."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB inserts float when trying to insert integer",
      "explanation": "> db.data.update({‘name’: ‘zero’}, {‘$set’: {‘value’: 0}})\n    > db.data.findOne({‘name’: ‘zero})\n    {‘name’: ‘zero’, ‘value’: 0.0}\n\nHow do I get Mongo to insert an integer?  \nThank you\n\n    db.data.update({‘name’: ‘zero’}, {‘$set’: {‘value’: NumberInt(0)}})\n\nYou can also use NumberLong."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongodb : $in operator vs lot of single queries",
      "explanation": "I know mongo is able to handle a lot of requests/sec, but let’s say I have to query a lot of documents of a collection given their \\_id; what sounds better, making a $in on the \\_id attribute with all the ids I want to get, or loop over findOne queries?  \nThanks by advance :)\n\nI would definitely go with using the $in query and providing a array of \\_ids instead of looping unless the documents are large and returning them all in a single query would go over the size limit of 4mb (16mb on more recent versions of MongoDB)  \nExample:\n\n    db.collection.find({\n    “key”: { “$in”: [\n    ObjectId(“xxx”),\n    ObjectId(“yyy”),\n    ObjectId(“zzz”)\n    ]}}\n    )\n\nWhy?  \n1\\. If you loop, there is a certain amount of setup and teardown for each query creating and exhausting cursors which would create overhead.  \n2\\. If you are not doing this on a local machine it also creates tcp/ip overhead for every request. Locally you could use domain sockets.  \n3\\. There is a index on “\\_id” created by default and collecting a group of documents to return in a batch request should be extremely fast so there is no need to break this up into smaller queries.  \nThere’s some additional documentation here if you want to check it out."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongodb – How to find string in multiple fields?",
      "explanation": "Using Pymongo for this scenario.  \nI have User that has email, first\\_name, last\\_name.  \nI am using this Pymongo snippet:\n\n    user_found = users.find({‘$or’:[\n    {’email’:{‘$regex’:searchString, ‘$options’:’i’}},\n    {‘first_name’:{‘$regex’:searchString, ‘$options’:’i’}},\n    {‘last_name’:{‘$regex’:searchString, ‘$options’:’i’}}]})\n\nthis example works, if I want to find searchString in:  \n1\\. email, or  \n2\\. first\\_name, or  \n3\\. last\\_name  \nnow I need to also find searchString in first\\_name + last\\_name combined.  \nhow can I do that? Is there a way in mongo, through the query, to combine the two into a “fullname” then search the fullname?  \nEasiest way is to add an array field and populate it with all of the variants that you want to search on. Index that array field.\n\nThat way you only need one index and your search across all fields is simple and doesn’t change when you want to search on some new search variant. You can also normalize the text you put into the search array, for example, lower casing it, removing punctuation etc.\n\nSee https://stackoverflow.com/q/8206188/224370\n\nEdit: MongoDB’s documentation now covers keyword search and the new full-text search feature."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Get BinData UUID from Mongo as string",
      "explanation": "I currently have some ids stored in Mongo as UUIDs (necessary for processing). They get returned like this:  \n“\\_id” : new BinData(3, “JliB6gIMRuSphAD2KmhzgQ==”)  \nWhat would be an easy way to turn this value into a string for debugging?  \nJust to be clear – the application can handle the data fine. I just need a way to get the actual UUID from Mongo quickly.\n\nThe answer to your question is more complicated that you would expect! The main reason it’s complicated is that for historical reasons (unfortunately) different drivers have written UUIDs to the database using different byte orders. You don’t mention which driver you are using, but I’ll use the C# driver as an example.  \nSuppose I use the following code to insert a document:\n\n    var guid = new Guid(“00112233-4455-6677-8899-aabbccddeeff”);\n    collection.Insert(new BsonDocument {\n    { “_id”, guid },\n    { “x”, 1 }\n    });\n    If I then examine the document using the Mongo shell, it looks like this:\n    > db.test.findOne()\n    { “_id” : BinData(3,”MyIRAFVEd2aImaq7zN3u/w==”), “x” : 1 }\n    >\n    The Mongo shell has a built-in function called hex that you can use to display the binary value as a hex string:\n    > var doc = db.test.findOne()\n    > doc._id.hex()\n    33221100554477668899aabbccddeeff\n    >\n\nLook carefully: the byte order of the hex string doesn’t match the original UUID value used in the C# program. That’s because the C# driver uses the byte order returned by Microsoft’s ToByteArray method of the Guid class (which sadly returns the bytes in a bizarre order, which fact was not discovered for many months). Other drivers have their own idiosyncracies.  \nTo help out with this we have some helper functions written in Javascript that can be loaded into the Mongo shell. They are defined in this file:\n\nhttps://github.com/mongodb/mongo-csharp-driver/blob/master/uuidhelpers.js\n\nThe Mongo shell can be told to process a file as it starts up by providing the name of the file on the command line (along with the –shell argument). Having loaded this file we have access to a number of helper functions to create and display BinData values that are UUIDs. For example:\n\nC:mongodbmongodb-win32-x86\\_64-2.0.1bin>mongo –shell uuidhelpers.js\n\nMongoDB shell version: 2.0.1  \nconnecting to: test  \ntype “help” for help\n\n    > var doc = db.test.findOne()\n    > doc._id.toCSUUID()\n    CSUUID(“00112233-4455-6677-8899-aabbccddeeff”)\n    > db.test.find({_id : CSUUID(“00112233-4455-6677-8899-aabbccddeeff”)})\n    { “_id” : BinData(3,”MyIRAFVEd2aImaq7zN3u/w==”), “x” : 1 }\n    >\n\nIn this example the toCSUUID function is used to display a BinData value as a CSUUID and the CSUUID function is used to create a BinData value for a UUID using the C# driver’s byte ordering conventions so that we can query on a UUID. There are similar functions for the other drivers (toJUUID, toPYUUID, JUUID, PYUUID).  \nSome day in the future all drivers will standardize on a new binary subtype 4 with a standard byte order. In the meantime you have to use the appropriate helper function that matches whatever driver you are using."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "All columns in MongoDB",
      "explanation": "I was wondering how you would find all of the column names in a table in MongoDB, like how you use SHOW COLUMNS FROM foo; in mysql.  \nMongoDB is schemaless and does not have tables. In MongoDB, each collection can have different types of items. You could store two very different items in the same collection:  \ndb.test.insert( { “SomeString” : “How much wood would the woodchop chop …” } );  \ndb.test.insert( { “Amount”: 2040.20, “Due”: new ISODate(“2012-11-10”), “UserId” : new ObjectId(“…”)} );  \nusually the objects are somehow related or have a common base type, but it’s not required.  \nYou can, however, take a look at invidual records using  \ndb.collectionName.findOne()  \nor  \ndb.collectionName.find().pretty()  \nHowever, there’s no guarantee from MongoDB that any two records look alike or have the same fields: **there’s no schema.**"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Can’t map file memory in MongoDB",
      "explanation": "The windows server 2003 box hosting our MongoDB instance ran out of disk space, and Mongo started generating the error:  \n**Can’t map file memory.**  \nAfter adding additional disk space to the server and restarting the MongoDB windows service, any operation against the database still generates the “Can’t map file memory” error. I tried doing a repair even and it gives the same error:\n\n**\\> db.repairDatabase(); { “assertion” : “can’t map file memory”, “assertionCode” : 10085, “errmsg” : “db assertion failure”, “ok” : 0 }**\n\nAny idea what I can do to get my database operational again??  \nStopping the service, deleting the lock file, and then doing a mongod –repair worked, even though db.repairDatabase did not.\n\nThe answer from Justin worked for me.  \nHere are some more detailed instructions for Ubuntu:  \nStop Mongo Service: sudo service mongodb stop  \nDelete Lock File: sudo rm /var/lib/mongodb/mongod.lock  \nRepair the DB: sudo mongod –repair –dbpath=/var/lib/mongodb  \nRestart the Mongo Service: sudo service mongodb start  \nHope that helps someone.  \n(edited – note that mongodb is the service name, but mongod is the correct command for repairing)"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Why does MongoDB have collections",
      "explanation": "MongoDB being document-oriented, the structure of collections seems to be a special case of documents. By that I mean one can define a document to contain other documents. So a collection is just a document containing other documents.  \nSo why do we need collections after all?  \nLogically yes, you could design a database system like that, but practically speaking no.  \n1\\. A collection has indexes on the documents in it.  \n2\\. A collection requires the documents in it to have unique ids.  \n3\\. A document is limited in size."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to update a field in MongoDB using existing value",
      "explanation": "I’ve been looking for a way to create an update statement that will take an existing numeric field and modify it using an expression. For example, if I have a field called Price, is it possible to do an update that sets Price to 50% off the existing value ?  \nSo, given { Price : 19.99 }  \nI’d like to do db.collection.update({tag : “refurb”}, {$set {Price : Price \\* 0.50 }}, false, true);  \nCan this be done or do I have to read the value back to the client, modify, then update ? I guess the question then is can expressions be used in update, and can they reference the document being updated. Thanks.\n\n    You can run server-side code with db.eval().\n    db.eval(function() {\n    db.collection.find({tag : “refurb”}).forEach(function(e) {\n    e.Price = e.Price * 0.5;\n    db.collection.save(e);\n    });\n    });\n\nNote this will block the DB, so it’s better to do find-update operation pair.\n\nSee [https://www.mongodb.org/display/DOCS/Server-side+Code+Execution](https://www.mongodb.org/display/DOCS/Server-side+Code+Execution)\n\nAs of the release notes from upcoming Mongo 2.6 you would be able to use the new $mul operator. It would multiply the value of the field by the number with the following syntax.\n\n    {\n    field: { $mul: }\n    }\n    So in your case you will need to do the following:\n    db.collection.update(\n    { tag : “refurb”},\n    { $mul: { Price : 0.5 } }\n    );"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB $where queries and tailable cursors — WAS: date math best practices",
      "explanation": "My problem: give me a list of documents older than X amount of time.  \nIf I have a document created with:  \ndb.dates.insert({date: new Date()});  \nAnd now I want to find it only when the “date” has become 30 minutes old:  \ndb.dates.find({ $where: “this.date.getTime() + 30 \\* 60000 <= new Date()”});  \nThis works, but in the Mongo documentation states quite clearly that there is a significant performance penalty to $where queries.  \nThus the question, is there a better way?\n\n\\==========UPDATE 1==========  \nI should have added that I am hoping to have this query function “dynamically” be creating the query one time and using it to obtain a tailable cursor on a capped collection… and I am not sure any longer that it is actually possible.  \nI will test and repost.\n\n\\==========UPDATE 2==========  \nSo, looks like my “delayed” queue is going to have to be handled in code, either with polling or some “check, then sleep” algorithm, because that appears to be what mongo’s delayed replication is doing (from db.cpp):\n\n    Skip code block\n    if ( replSettings.slavedelay && ( unsigned( time( 0 ) ) < nextOpTime.getSecs() + replSettings.slavedelay) ) {\n    assert( justOne );\n    oplogReader.putBack( op );\n    _sleepAdviceTime = nextOpTime.getSecs() + replSettings.slavedelay + 1;\n    dblock lk;\n    if ( n > 0 ) {\n    syncedTo = last;\n    save();\n    }\n    log() << “repl:   applied ” << n << ” operations” << endl;\n    log() << “repl:   syncedTo: ” << syncedTo.toStringLong() << endl;\n    log() << “waiting until: ” << _sleepAdviceTime << ” to continue” << endl;\n    return okResultCode;\n    }\n\nThe $lte operator (and other range queries) will work and utilize indexes, but it cannot evaluate expressions. You have to query against a query-time constant (which would be ‘now – 30 min’):  \nvar threshold = new Date();  \nthreshold.setMinutes(-30);  \n// now, query against a constant:  \ndb.dates.find({“date” : {$lte : threshold}});  \nOf course, you can do the same with any driver for any programming language, e.g. C#\n\n    var c = db.Collection.Find(Query.LTE(“date”, DateTime.UtcNow.AddMinutes(-30));"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to implement post tags in Mongo?",
      "explanation": "I’m toying with Mongo to make a SO-like pet project, and I want to implement post tags. Each tag has a name and a slug (string to be used as an id in the URL), and a post has multiple tags. I’d like to be able to create queries like “find posts, which have tag A, don’t have tag B”, and I’m wondering what’s the mongo-way to do this.  \nOne way is to store an array of tag ids with each post – this will make the said query easy, but will require an extra one for each post to get tag name and a slug. Another way is to store an array of \\[tag name, tag slug\\] with each post, but I’m not sure I’ll be able to use that info in a find.  \nIs there some other method, which will work better for mongo? I’m new to NoSQL, so I’d appreciate any advise on how this can be accomplished. Also, I’m using PHP binding, but that shouldn’t matter probably.  \nIf the tags you use and their respective slugs are unlikely to change, I think your second approach is the better one. However I would suggest a small change – rather than storing an array of \\[name, slug\\], make the fields explicit by creating a tag subdocument as in this example post document:\n\n    Skip code block\n    {\n    “_id” : ObjectId(“4ee33229d8854784468cda7e”),\n    “title” : “My Post”,\n    “content” : “This is a post with some tags”,\n    “tags” : [\n    {\n    “name” : “meta”,\n    “slug” : “34589734”\n    },\n    {\n    “name” : “post”,\n    “slug” : “34asd97x”\n    },\n    ]\n    }\n\n  \nYou can then query for posts with a particular tag using dot notation like this:\n\n    db.test.find({ “tags.name” : “meta”})\n\n  \nBecause tags is an array, mongo is clever enough to match the query against any element of the array rather than the array as a whole, and dot-notation allows you to match against a particular field.  \nTo query for posts not containing a specific tag, use $ne:\n\n    db.test.find({ “tags.name” : { $ne : “fish” }})\n    And to query for posts containing one tag but not the other, use $and:\n    db.test.find({ $and : [{ “tags.name” : { $ne : “fish”}}, {“tags.name” : “meta”}]})\n\n  \nHope this helps!"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What operations are cheap/expensive in mongodb?",
      "explanation": "I’m reading up on MongoDB, and trying to get a sense of where it’s best used. One question that I don’t see a clear answer to is which operations are cheap or expensive, and under what conditions.  \nCan you help clarify?  \nThanks.  \nIt is often claimed that mongodb has insanely fast writes. While they are not slow indeed, this is quite an overstatement. Write throughput in mongodb is limited by global write lock. Yes, you heard me right, there can be only **ONE** write operation happening on the server at any given moment.  \nAlso I suggest you take advantage of schemaless nature of mongodb and store your data denormalized. Often it is possible to do just one disk seek to fetch all required data (because it is all in the same document). Less disk seeks – faster queries.  \nIf data sits in RAM – no disk seeks are required at all, data is served right from memory. So, make sure you have enough RAM.  \nMap/Reduce, group, $where queries are slow.  \nIt is not fast to keep writing to one big document (using $push, for example). The document will outgrow its disk boundaries and will have to be copied to another place, which involves more disk operations.  \nAnd I agree with @AurelienB, some basic principles are universal across all databases."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB: Find a document by non-existence of a field?",
      "explanation": "Is there a way to specify a condition of “where document doesn’t contain field” ?  \nFor example, I want to only find the first of these 2 because it doesn’t have the “price” field.\n\n    {“fruit”:”apple”, “color”:”red”}\n    {“fruit”:”banana”, “color”:”yellow”, “price”:”2.00?}\n    db.mycollection.find( { “price” : { “$exists” : false } } )\n\n  \nnostalgic URL : https://www.mongodb.org/display/DOCS/Advanced+Queries#AdvancedQueries-%24exists  \nupdated URL: https://docs.mongodb.org/manual/reference/operator/query/exists/#op.\\_S\\_exists"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to remove deprecated fields in Mongo?",
      "explanation": "I have removed some fields from document definition. I want to remove this field across all documents of collection. How can I do it?  \nTry:\n\n    db.collection.update(\n    { ‘’: { ‘$exists’: true } },  // Query\n    { ‘$unset’: { ‘’: true  } },  // Update\n    false, true                          // Upsert, Multi\n    )\n\n  \nwhere field is your deprecated field and collection is the collection it was removed from.  \nThe general update command is of the form db.collection.update( criteria, objNew, upsert, multi ). The false and true trailing arguments disable upsert mode and enable multi update so that the query updates all of the documents in the collection (not just the first match).\n\n  \n**Update for MongoDB 2.2+:**\n\nYou can now provide a JSON object instead of positional arguments for upsert and multi.\n\n    db.collection.update(\n    { ‘’: { ‘$exists’: true } },  // Query\n    { ‘$unset’: { ‘’: true  } },  // Update\n    { ‘multi’: true }                    // Options\n    )\n    just do something like this\n    db.people.find().forEach(function(x) {\n    delete x.badField;\n    db.people.save(x);\n    })\n\n  \noooh the $unset answer someone gave using update() here is pretty awesome too."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Combine two $or statements",
      "explanation": "I am trying to perform a query which is composed of two $or’s:\n\n    |——————–\n    | Date1  |  Date2   |\n    |——————–\n    | NULL   |  NULL    | *\n    | NULL   |  TODAY   | *\n    | NULL   |  TOMRW   |\n    | TODAY  |  TODAY   | *\n    | TODAY  |  NULL    | *\n    | TOMRW  |  NULL    |\n    |——————–\n    (I’ve marked the rows that would match with an asterisk)\n    (Date1 == null || Date1 <= today) && (Date2 == null || Date2 <= today)\n\n  \nI am not sure how to express this query in MongoDB.  \nIt can be broken down into two individual queries that do exactly what they should:  \nSkip code block\n\n    {\n    “$or”: [{\n    “Date1”: {\n    “$exists”: false\n    }\n    },\n    {\n    “Date1”: {\n    “$exists”: true,\n    “$lte”: new Date(“2012-01-07T04:45:52.057Z”)\n    }\n    }]\n    }\n    and\n    Skip code block\n    {\n    “$or”: [{\n    “Date2”: {\n    “$exists”: false\n    }\n    },\n    {\n    “Date2”: {\n    “$exists”: true,\n    “$lte”: new Date(“2012-01-07T04:45:52.057Z”)\n    }\n    }]\n    }\n\n  \nBoth of these select the correct set of documents – I just dont know how to execute them as a single query.  \nMy initial thought was to do a query like this:\n\n    {\n    $and: [orQuery1, orQuery2]\n    }\n\n  \nUsing an $and query returns 0 results. It was explained why here in this thread: $and query returns no result  \nAlso in that thread, a suggestion was made to do a query like this:\n\n    {\n    Key: {valToMatch1: 1, valToMatch2: 2}\n    }\n\n  \nBut I dont think an $or can be executed this way.  \nSo, the question is: How do I construct my query such that I can combine the two $or’s into a single query?  \n(Its getting very late so I hope this question makes sense.)  \nuse test\n\n    db.test.insert({a:1})\n    db.test.insert({a:2, Date2:new Date(“01/07/2012”)})\n    db.test.insert({a:3, Date2:new Date(“01/08/2012”)})\n    db.test.insert({a:4, Date1:new Date(“01/07/2012”), Date2:new Date(“01/07/2012”)})\n    db.test.insert({a:5, Date1:new Date(“01/07/2012”)})\n    db.test.insert({a:6, Date1:new Date(“01/08/2012”)})\n    first subquery db.test.distinct(‘a’, {…});\n    [1, 2, 3]\n    second subquery db.test.distinct(‘a’, {…});\n    [ 1, 5, 6 ]\n    (Date1 == null || Date1 <= today) && (Date2 == null || Date2 <= today)\n    unwind\n    Date1 == null && Date2 == null ||\n    Date1 == null && Date2 <= today ||\n    Date1 <= today && Date2 == null ||\n    Date1 <= today && Date2 <= today ||\n    query\n    Skip code block\n    db.test.find(\n    {\n    $or :\n    [\n    {$and: [\n    {“Date1”: {“$exists”: false}},\n    {“Date2”: {“$exists”: false}}\n    ]},\n    {$and: [\n    {“Date1”: {“$exists”: false}},\n    {“Date2”: {\n    “$exists”: true,\n    “$lte”: new Date(“2012-01-07T04:45:52.057Z”)}\n    }\n    ]},\n    {$and: [\n    {“Date2”: {“$exists”: false}},\n    {“Date1”: {\n    “$exists”: true,\n    “$lte”: new Date(“2012-01-07T04:45:52.057Z”)}\n    }\n    ]},\n    {$and: [\n    {“Date2”: {\n    “$exists”: true,\n    “$lte”: new Date(“2012-01-07T04:45:52.057Z”)}\n    },\n    {“Date1”: {\n    “$exists”: true,\n    “$lte”: new Date(“2012-01-07T04:45:52.057Z”)}\n    }\n    ]}\n    ]\n    })\n    >[ 1 ]\n\n  \nthis should work too (assume that ‘not exist’ and ‘null’ is the same)\n\n    Skip code block\n    db.test.find(\n    {\n    $and :\n    [\n    {$or: [\n    {“Date1”: null},\n    {“Date1”: { “$lte”: new Date(“2012-01-07T04:45:52.057Z”)} }\n    ]},\n    {$or: [\n    {“Date2”: null},\n    {“Date2”: { “$lte”: new Date(“2012-01-07T04:45:52.057Z”)} }\n    ]}\n    ]\n    }\n    )"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "mongodb: how to do backup of mongodb",
      "explanation": "I think someone has already suggested:  \n1\\. stop the mongod  \n2\\. backup the data directory  \nIs it reliable, I mean, ensure 100% success to restore? And I can’t find which directory stores the data… any command can help me to find it?  \nIf mongod process exits cleanly (that is, no crashes or kill -9 stuff), then it is safe to copy data files somewhere.  \nIf your current installation breaks (for example, data corruption due to unclean shutdown), you can delete its files, copy that backup over and start mongod again.  \nDefault data directory is /data/db, but it may be set to another value in your config file. For example, I set it to /var/lib/mongodb.  \nYou can also use mongodump to do a backup from a live server (this may impact performance). Usemongorestore to restore backups made by mongodump.  \nAt IGN we do hot backups via mongodump running as an hourly cron job, and take filer snapshots (NetApp storage) on a half-hourly basis. If your system is not too write heavy and you can afford write blocks, try using fsync and lock to flush the writes to the disk and prevent further writes. This can be followed by mongodump and upon completion you can unlock the db. Please note that you’ve to be admin to do so.  \ndb.runCommand({fsync:1,lock:1})"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to update date field in mongo console?",
      "explanation": "For example I want to update all records to\n\n‘2012-01-01’ ( “time” : ISODate(“2011-12-31T13:52:40Z”) ).\n\n    db.test.update( { time : ‘2012-01-01’ }, false, true  )\n    return error:\n    Assert failed : need an object\n    Error(“Printing Stack Trace”)@:0\n    ()@shell/utils.js:35\n    (“assert failed : need an object”)@shell/utils.js:46\n    (false,”need an object”)@shell/utils.js:54\n    ([object Object],false,true)@shell/collection.js:189\n    @(shell):1\n    Wed Jan 11 17:52:35 uncaught exception: assert failed : need an object\n\n  \nYou need to create a new ISODate object like this:\n\ndb.test.insert({“Time” : new ISODate(“2012-01-10”) });\n\nThis is true both for updates and for queries. Note that your query syntax is incorrect, it should be\n\n    db.test.update({ criteria }, { newObj }, upsert, multi);\n    For example, to update all objects, consider\n    db.test.update( {}, { $set : { “time” : new ISODate(“2012-01-11T03:34:54Z”) } }, true, true);\n    Also note that this is very different from\n    db.test.update( {}, { “time” : new ISODate(“2012-01-11T03:34:54Z”) }, true, false);\n\n  \nbecause the latter will **replace** the object, rather than add a new field to the existing document or updating the existing field. In this example, I changed the last parameter to false, because multi updates only work with $ operators."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "return query based on date",
      "explanation": "I have a data like this in mongodb\n\n{ “latitude” : “”, “longitude” : “”, “course” : “”, “battery” : “0”, “imei” : “0”, “altitude” : “F:3.82V”, “mcc” : “07”, “mnc” : “007B”, “lac” : “2A83”, “\\_id” : ObjectId(“4f0eb2c406ab6a9d4d000003”), “createdAt” : ISODate(“2012-01-12T20:15:31Z”) }\n\nHow to query db.gpsdatas.find({‘createdAt’: ??what here??}) so that it return me the above result from db?  \nYou probably want to make a range query, for example, all items created after a given date:\n\n    db.gpsdatas.find({“createdAt” : { $gte : new ISODate(“2012-01-12T20:15:31Z”) }});\n\n  \nI’m using $gte (greater than or equals), because this is often used for date-only queries, where the time component is 00:00:00.  \nIf you really want to find a date that equals another date, the syntax would be\n\n    db.gpsdatas.find({“createdAt” : new ISODate(“2012-01-12T20:15:31Z”) });"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB query comparing 2 fields in same collection without $where",
      "explanation": "Does MongoDB supports comparing two fields in same collection by using native operators (not $where and JavaScript)? I already looked at similar questions and all answers used $where / JavaScript.  \nMongoDB documentation clearly states that:  \nJavaScript executes more slowly than the native operators listed on this page, but is very flexible.  \nMy primary concern is speed and I would like to use indexes if possible. So is comparing two fields in MongoDB possible without using JavaScript?  \nThis is not currently possible, but it will be possible through the new aggregation framework currently under development (2.1+). This aggregation framework is native and does not rely on relatively slow JavaScript execution paths.  \nFor more details check\n\nhttps://www.mongodb.org/display/DOCS/Aggregation+Framework and the progress at https://jira.mongodb.org/browse/SERVER-447"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Upserts in mongodb when using custom \\_id values",
      "explanation": "I need to insert a document if it doesn’t exist. I know that the “upsert” option can do that, but I have some particular needs.  \nFirst I need to create the document with its \\_id field only, but only if it doesn’t exist already. My \\_id field is a number generated by me (not an ObjectId). If I use the “upsert” option then I get “Mod on \\_id not allowed”\n\n    db.mycollection.update({ _id: id }, { _id: id }, { upsert: true });\n\n  \nI know that we can’t use the \\_id in a $set.  \nSo, my question is: If there any way to a “create if doesn’t exists” atomically in mongodb?  \nEDIT: As proposed by @Barrie this works (using nodejs and mongoose):\n\n    var newUser = new User({ _id: id });\n    newUser.save(function (err) {\n    if (err && err.code === 11000) {\n    console.log(‘If duplicate key the user already exists’, newTwitterUser);\n    return;\n    }\n    console.log(‘New user or err’, newTwitterUser);\n    });\n\n  \nBut I still wonder if it is the best way to do it.  \nYou can just use insert(). If the document with the \\_id you specify already exists, the insert() will fail, nothing will be modified – so “create if it doesn’t exist” is what it’s already doing by default when you use insert() with a user-created \\_id.  \nI had the same problem, but found a better solution for my needs. You can use that same query style if you simply remove the \\_id attribute from the update object. So if at first you get an error with this:\n\n    db.mycollection.update({ _id: id }, {$set: { _id: id, name: ‘name’ }}, { upsert: true });\n\n  \ninstead use this:\n\n    db.mycollection.update({ _id: id }, {$set: { name: ‘name’ }}, { upsert: true });\n\n  \nThis is better because it works for both insert and update."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to list all collections in the mongo shell?",
      "explanation": "In the MongoDB shell, how do I list all collections for the current database that I’m using? I can’t seem to find it anywhere in the docs.  \nYou can do:\n\n    db.getCollectionNames()\n    > show collections\n\n  \nwill list all the collections in the currently selected DB, as stated in the command line help (help).  \n\\> show tables  \nIt gives the same result as Cameron’s answer.  \nApart from the options suggested by other people:  \nshow collections  //output every collection  \nshow tables  \ndb.getCollectionNames() //shows all collections as a list  \nThere is also another way which can be really handy if you want to know how each of the collections was created (for example it is a capped collection with a particular size)  \ndb.system.namespaces.find()  \nTo list all databases  \nshow dbs  \nenters or uses given database  \nuse databasename  \nTo list all collections:  \nshow collections  \nOutput:\n\n    collection1\n    collection2\n    system.indexes\n\n  \n(or)  \ns\n\n    how tables\n    //output\n    collection1\n    collection2\n    system.indexes\n\n  \n(or)\n\n    db.getCollectionNames()\n    //output\n    [ “collection1”, “collection2”, “system.indexes” ]\n\n  \nTo enter or use given collection  \nuse collectionname"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to remove one document by Id using the official CSharp driver for Mongo",
      "explanation": "can someone please show me if there is a better way to remove one document from a MongoDB using the standard 10gen driver for C# than what I have below:  \nvar query = Query.EQ(“\\_id”, a.Id);  \ndatabase.GetCollection(“Animal”).Remove(query);  \nThe above works, but seems dirty and like too much work to me. The “Save” command for instance takes an instance and updates it. Perhaps I’m too used to things like EF/LINQToSQL/NHib where I’d say something like Remove(item).  \nAm I missing something?  \n(also, I’m trying to use the official driver rather than NoRM which seems to be dead or Samus which seems equally out of date.)  \nThat’s the way you do it. I’m sure you know this, but if you want to put it on one line you could combine it so you don’t need to define a query variable:  \ncollection.Remove(Query.EQ(“\\_id”, a.Id));  \nIf the \\[id\\] is string, you must use ObjectId instance explicitly.  \nvar query = Query.EQ(“\\_id”, ObjectId.Parse(id));"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Clone a collection in MongoDB",
      "explanation": "I want to clone a MongoDB collection and save it on the same server with a different name. So for example right now I have the following collections: demo1.categories, demo1.users and demo2.users.  \nI want to have a “demo2.categories” which is identical to “demo1.categories”. (It just has a different name.)  \nYet again the MongoDB documentation comes to the rescue  \nassuming that the collection actually is named “demo1.categories”:\n\n    db.demo1.categories.find().forEach( function(x){db.demo2.categories.insert(x)} );\n\n  \nThe most simple & efficient way is by using copyTo(), so you can use:  \ndb.source.copyTo(“target”);  \n& if “target” doesn’t exist, it will be created  \n**— Update —**  \nAccording to CopyTo Documentation, Because copyTo() uses eval internally, the copy operations will block all other operations on the mongod instance. So it shouldn’t be used on production environment."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "File write operations in mongo script?",
      "explanation": "Is this possible to write the query result to the file from mongo js script. I have searched a lot, but I didn’t find any solution.  \nex:-\n\n    cursor = db.users.find();\n    while(cursor.hasNext()) {\n    cursor.next();\n    // writing the cursor output to file ????\n    }\n    You could use print and then redirect output:\n    script.js:\n    cursor = db.users.find();\n    while(cursor.hasNext()){\n    printjson(cursor.next());\n    }\n\n  \nthen run the script and redirect output to a file:\n\nmongo –quiet script.js > result.txt\n\nhttps://www.mongodb.org/display/DOCS/Scripting+the+shell\n\nparagraph “Differences between scripted and interactive/ Printing”.\n\n    ./mongo server.com/mydb –quiet –eval “db.users.find().forEach(printjson);” > 1.txt\n\n  \nyou can skip the while loop using forEach :\n\n    db.users.find().forEach( function(user){ printjson(user) });\n\n  \nP.S: I would haved added this answer as a comment to @milan’s answer but don’t have enough reputation yet to comment :p"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Do arrays stored in MongoDB keep their order?",
      "explanation": "Simple question, do arrays keep their order when stored in MongoDB?  \nyep MongoDB keeps the order of the array.. just like Javascript engines..  \nYes, in fact from a quick google search on the subject, it seems that it’s rather difficult to re-order them:\n\nhttps://groups.google.com/group/mongodb-user/browse\\_thread/thread/1df1654889e664c1"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB: update every document on one field",
      "explanation": "I have a collected named foo hypothetically.  \nEach instance of foo has a field called lastLookedAt which is a UNIX timestamp since epoch. I’d like to be able to go through the MongoDB client and set that timestamp for all existing documents (about 20,000 of them) to the current timestamp.  \nWhat’s the best way of handling this?  \nIn the Mongo shell, or with any Mongodb client, you can do something like that:  \n**For Mongodb < 2.2:**\n\n    db.foo.update({}, {$set: {lastLookedAt: Date.now() / 1000}}, false, true)\n\n  \n**For Mongodb >= 2.2:**\n\n    db.foo.update({}, {$set: {lastLookedAt: Date.now() / 1000}}, { multi: true })\n\n  \nSee https://www.mongodb.org/display/DOCS/Updating\n\n  \n1\\. {}is the condition (the empty condition matches any document)  \n2\\. {$set: {lastLookedAt: Date.now()}}is what you want to do  \n3\\. falseis for the “upsert” parameter (insert if not present, or else update – not what you want)  \n4\\. true is for the “multi” parameter (update multiple records)"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to check if Mongodb is properly installed",
      "explanation": "I installed MongoDb yesterday on a Mac Snow Leopard and got the following error message  \nMongo::ConnectionFailure: Failed to connect to a master node at localhost:27017  \nwhen trying to run some tests in Rails that used a mongodb.  \nAnother SO question mongo – ruby connection problem about the same error message had an answer that recommended removing the lock file  \nsudo rm /var/lib/mongodb/mongod.lock  \nbut when I run that command i’m getting  \nNo such file or directory  \nAny ideas how I can figure out how to get it working or see if it’s properly installed?  \nThe easiest way to run mongodb on Mac OS is:  \nDownload binary package from\n\nhttps://www.mongodb.org/downloads,\n\nfor me, I am using lastest 64 bit version\n\n(https://fastdl.mongodb.org/osx/mongodb-osx-x86\\_64-2.0.2.tgz  \n  \n1\\. mkdir -p $HOME/opt  \n2\\. cd $HOME/opt  \n3\\. wget https://fastdl.mongodb.org/osx/mongodb-osx-x86\\_64-2.0.2.tgzto download the latest (2.0.2 for now) 64 bit binary package for Mac OS  \n4\\. tar xf mongodb-osx-x86\\_64-2.0.2.tgz -C $HOME/optto unpack the package, and it will be unpacked to $HOME/opt/mongodb-osx-x86\\_64-2.0.2  \n5\\. mkdir -p $HOME/opt/mongodatato create the data directory for mongodb  \n6\\. $HOME/opt/mongodb-osx-x86\\_64-2.0.2/bin/mongod –dbpath=$HOME/opt/mongodata –7. logpath=$HOME/opt/mongod.logto start the mongodb daemon\n\n7\\. Then you can run $HOME/opt/mongodb-osx-x86\\_64-2.0.2/bin/mongo to connect to your local mongodb service\n\nYou can also have https://www.mongodb.org/display/DOCS/Quickstart+OS+X as additional reference"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "In MongoDB, how to filter by location, sort by another field and paginate the results correctly?",
      "explanation": "When i don’t use pagination, everything works fine (i have only 3 records in this collection, so all of them are listed here):\n\n    db.suppliers.find({location: {$near: [-23.5968323, -46.6782386]}},{name:1,badge:1}).sort({badge:-1})\n    { “_id” : ObjectId(“4f33ff549112b9b84f000070”), “badge” : 3, “name” : “Dedetizadora Alvorada” }\n    { “_id” : ObjectId(“4f33ff019112b9b84f00005b”), “badge” : 2, “name” : “Sampex Desentupidora e Dedetizadora” }\n    { “_id” : ObjectId(“4f33feae9112b9b84f000046”), “badge” : 1, “name” : “Higitec Desentupimento e Dedetizao” }\n    But when i try to paginate from the first to the second page, one record doesn’t show up and one is repeated:\n    db.suppliers.find({location: {$near: [-23.5968323, -46.6782386]}},{name:1,badge:1}).sort({badge:-1}).skip(0).limit(2)\n    { “_id” : ObjectId(“4f33ff549112b9b84f000070”), “badge” : 3, “name” : “Dedetizadora Alvorada” }\n    { “_id” : ObjectId(“4f33feae9112b9b84f000046”), “badge” : 1, “name” : “Higitec Desentupimento e Dedetizao” }\n    db.suppliers.find({location: {$near: [-23.5968323, -46.6782386]}},{name:1,badge:1}).sort({badge:-1}).skip(2).limit(2)\n    { “_id” : ObjectId(“4f33feae9112b9b84f000046”), “badge” : 1, “name” : “Higitec Desentupimento e Dedetizao” }\n\n  \nAm i doing something wrong or is this some kind of bug?  \n**edit**:  \nHere is a workaround for this. Basically you shouldn’t mix $near queries with sorting; use $within instead.  \nThere is an open issue regarding the same problem. Please have a look & vote Geospatial result paging fails when sorting with additional keys"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Add a new field to a collection with value of an existing field",
      "explanation": "I’d like to add a new field to a collection, with the value of the new field set to the value of an existing field.  \nSpecifically, I’d like to go from this:\n\n    # db.foo.findOne()\n    {\n    “_id”     : ObjectId(“4f25c828eb60261eab000000”),\n    “created” : ISODate(“2012-01-29T16:28:56.232Z”),\n    “…”     : …\n    }\n    to this:\n    # db.foo.findOne()\n    {\n    “_id”      : ObjectId(“4f25c828eb60261eab000000”),\n    “created”  : ISODate(“2012-01-29T16:28:56.232Z”),\n    “event_ts” : ISODate(“2012-01-29T16:28:56.232Z”),  #same as created\n    “…”      : …\n    }\n    (New documents in this collection won’t all have this peculiar redundancy, but I want to do this for my existing documents)\n    function addEventTsField(){\n    db.foo.find().forEach(function(doc){\n    db.foo.update({_id:doc._id}, {$set:{“event_ts”:doc.created}});\n    });\n    }\n    Run from console:\n    addEventTsField();"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongodb update the specific element from subarray",
      "explanation": "I have a collection with a following schema:\n\n    Skip code block\n    {\n    “_id” : 28,\n    “n” : [{\n    “a” : ObjectId(“4ef8466e46b3b8140e000000”),\n    “c” : 28,\n    “p” : [ObjectId(“4f00640646b3b88005000003”), ObjectId(“4f00640146b3b88005000002”), ObjectId(“4f00637d46b3b8cc0e000001”), ObjectId(“4f00638046b3b8cc0e000002”), ObjectId(“4f00638246b3b8cc0e000003”), ObjectId(“4f00631646b3b85002000001”), ObjectId(“4f00631846b3b85002000002”)],\n    “u” : 26\n    }, {\n    “a” : ObjectId(“4ef8466e46b3b8140e000000”),\n    “c” : 10,\n    “p” : [ObjectId(“4f00640146b3b88005000002”), ObjectId(“4f0063fd46b3b88005000001”)],\n    “u” : 26\n    }, {\n    “a” : ObjectId(“4ef8467846b3b8780d000001”),\n    “u” : 26,\n    “p” : [ObjectId(“4f00637b46b3b8cc0e000000”)],\n    “c” : 28\n    }, {\n    “a” : ObjectId(“4ef85a3e46b3b84408000000”),\n    “u” : 26,\n    “p” : [ObjectId(“4f00631046b3b85002000000”)],\n    “c” : 28\n    }]\n    }\n\n  \nI need to update one of the elements in the array in the document with \\_id = 28 but only if the a = to some value and c = some value\n\n    Skip code block\n    db.coll.update({\n    ‘_id’ : 28,\n    ‘n.a’ : new ObjectId(‘4ef85a3e46b3b84408000000’),\n    ‘n.c’ : 28\n    },\n    {\n    $push : {\n    ‘n.$.p’ : ObjectId(“4b97e62bf1d8c7152c9ccb74”)\n    },\n    $set : {\n    ‘n.$.t’ : ISODate(“2013-05-13T14:22:46.777Z”)\n    }\n    })\n\n  \nSo basically I want to update specific element from array: and as far as one can see, this is the fourth element. The problem is that when the query is executing, it most likely updates the first element.  \nHow can I fix it?  \nThe problem in your code is dot-notation because When you specify the dot notation you assume that the filter criterias specified must match the single array element that satisfies all the criteria. But it doesnt. Dot notation on arrays may pickup any array element if any single criteria matches. Thats why you are getting the unexpected update.  \nYou have to use $elemMatch to match all the filters in the array element.\n\n    Skip code block\n    db.coll.update({\n    ‘_id’ : 28,\n    n: {\n    $elemMatch:{\n    a : new ObjectId(‘4ef85a3e46b3b84408000000’),\n    c : 28 }\n    }\n    },\n    {\n    $push : {\n    ‘n.$.p’ : ObjectId(“4b97e62bf1d8c7152c9ccb74”)\n    },\n    $set : {\n    ‘n.$.t’ : ISODate(“2013-05-13T14:22:46.777Z”)\n    }\n    })\n    and the output is\n    {\n    “a” : ObjectId(“4ef85a3e46b3b84408000000”),\n    “c” : 28,\n    “p” : [\n    ObjectId(“4f00631046b3b85002000000”),\n    ObjectId(“4b97e62bf1d8c7152c9ccb74”)\n    ],\n    “t” : ISODate(“2013-05-13T14:22:46.777Z”),\n    “u” : 26\n    }"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How can I rename a field for all documents in MongoDB?",
      "explanation": "Assuming I have a collection in MongoDB with 5000 records, each containing something similar to:\n\n    {\n    “occupation”:”Doctor”,\n    “name”: {\n    “first”:”Jimmy”,\n    “additional”:”Smith”\n    }\n\n  \nIs there an easy way to rename the field “additional” to “last” in all documents? I saw the $rename operator in the documentation but I’m not really clear on how to specify a subfield.  \nNot tested, but you can try:\n\n    db.foo.update({}, {$rename:{“name.additional”:”name.last”}}, false, true);\n    If this does not work, maybe you have to use the former way:\n    remap = function (x) {\n    if (x.additional){\n    db.foo.update({_id:x._id}, {$set:{“name.last”:x.name.additional}, $unset:{“name.additional”:1}});\n    }\n    }\n    db.foo.find().forEach(remap);\n    please try db.collectionName.update({}, { $rename : { ‘name.additional’ : ‘name.last’ } }, { multi: true } )\n\n  \nand read this :)\n\nhttps://docs.mongodb.org/manual/reference/operator/rename/#\\_S\\_rename"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongodb field not null delete",
      "explanation": "How do I remove all documents in a collection where a field’s value isn’t null? Basically the MySql query version would be like this:  \n// MySql query  \nDELETE FROM companies WHERE createdBy != NULL  \n// What I tried but did not work.  \n$this->mongo->companies->remove(array(‘createdBy’ => true));  \nI don’t even know if it is possible, if anyone could help me with this I would appreciate it  \nThanks :)  \nYou can do it easy via not equal operator:  \ndb.companies.find( { createdBy : { $ne : null } } );"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How can I see what ports mongo is listening on from mongo shell?",
      "explanation": "If I have a mongo instance running, how can I check what port numbers it is listening on from the shell? I thought that db.serverStatus() would do it but I don’t see it. I see this  \n“connections” : {  \n“current” : 3,  \n“available” : 816  \nWhich is close… but no. Suggestions? I’ve read the docs and can’t seem to find any command that will do this.  \nFrom the system shell you can use lsof (see Derick’s answer below) or netstat -an to view what a process is actually doing. However, assuming you only have access to the mongo shell (which your question title implies), then you can run the serverCmdLineOpts() command. That output will give you all the arguments passed on the command line (argv) and the ones from the config file (parsed) and you can infer the ports mongod is listening based on that information. Here’s an example:  \nSkip code block\n\n    db.serverCmdLineOpts()\n    {\n    “argv” : [\n    “./mongod”,\n    “-replSet”,\n    “test”,\n    “–rest”,\n    “–dbpath”,\n    “/data/test/r1”,\n    “–port”,\n    “30001”\n    ],\n    “parsed” : {\n    “dbpath” : “/data/test/r1”,\n    “port” : 30001,\n    “replSet” : “test”,\n    “rest” : true\n    },\n    “ok” : 1\n    }\n\n  \nIf you have not passed specific port options like the ones above, then the mongod will be listening on 27017 and 28017 (http console) by default. Note: there are a couple of other arguments that can alter ports without being explicit, see here:\n\nhttps://docs.mongodb.org/manual/administration/security/#interfaces-and-port-numbers\n\nYou can do this from the Operating System shell by running:  \nsudo lsof -iTCP -sTCP:LISTEN | grep mongo  \nMongoDB only listens on one port by default (27017). If the –rest interface is active, port 28017 (27017+1000) will also be open handling web requests for details.  \nMongoDB supports a getParameter command, but that only works if you’re already connected to the Database (at which point you already know the port)."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "mongodb indexing embedded fields (dot notation)",
      "explanation": "Let’s assume this is a document representing a customer.\n\n    {\n    company_name: ‘corporate ltd.’,\n    pocs: [\n    {name: ‘Paul’, email: ‘paul@corporate.com’},\n    {name: ‘Jessica’, email: ‘jessica@corporate.com’}\n    ]\n    }\n\n  \nI wanted to define a unique index for pocs.email So I issued the following command:\n\n    db.things.ensureIndex({“pocs.email”: 1}, {unique: true})\n\nThe strange thing is that when trying to add another company with a poc having an email already exists in another company, mongo rejects that, respecting the unique index constraint.  \nthat is, the following cannot exists:\n\n    Skip code block\n    {\n    company_name: ‘corporate ltd.’,\n    pocs: [\n    {name: ‘Paul’, email: ‘paul@corporate.com’},\n    {name: ‘Jessica’, email: ‘jessica@corporate.com’}\n    ]\n    },\n    {\n    company_name: ‘contoso llc’,\n    pocs: [\n    {name: ‘Paul’, email: ‘paul@corporate.com’},\n    ]\n    }\n    Which is fine. However, having duplicate poc within the same doc is possible, e.g.\n    {\n    company_name: ‘corporate ltd.’,\n    pocs: [\n    {name: ‘Paul’, email: ‘paul@corporate.com’},\n    {name: ‘Paul’, email: ‘paul@corporate.com’},\n    {name: ‘Jessica’, email: ‘jessica@corporate.com’}\n    ]\n    },\n\n  \nsee my cli commands sequence below:  \nSkip code block\n\n    > version()\n    version: 2.0.2\n    >\n    > use test\n    switched to db test\n    > db.test.ensureIndex({“poc.email”: 1}, {unique: true})\n    >\n    > db.test.insert({company: “contoso”, poc: [{email: ‘me@comapny.com’}]})\n    > db.test.insert({company: “contoso”, poc: [{email: ‘me@comapny.com’}]})\n    E11000 duplicate key error index: test.test.$poc.email_1  dup key: { : “me@comapny.com” }\n    > ({company: “contoso”, poc: [{email: ‘me.too@comapny.com’}, {email: ‘me.too@company.com’}]})\n    >\n    >\n    > db.test.find()\n    { “_id” : ObjectId(“4f44949685926af0ecf9295d”), “company” : “contoso”, “poc” : [ { “email” : “me@comapny.com” } ] }\n    { “_id” : ObjectId(“4f4494b885926af0ecf9295f”), “company” : “contoso”, “poc” : [ { “email” : “me.too@comapny.com” }, { “email” : “me.too@company.com” } ] }\n    Moreover, this happens either at insert or at update.\n    > db.test.update({“_id” : ObjectId(“4f44949685926af0ecf9295d”)}, {$push: { poc: {email: ‘me@company.com’}}})\n    > db.test.find()\n    { “_id” : ObjectId(“4f4494b885926af0ecf9295f”), “company” : “contoso”, “poc” : [ { “email” : “me.too@comapny.com” }, { “email” : “me.too@company.com” } ] }\n    { “_id” : ObjectId(“4f44949685926af0ecf9295d”), “company” : “contoso”, “poc” : [        {      “email” : “me@comapny.com” },   {       “email” : “me@company.com” },   {      “email” : “me@company.com” } ] }\n    >\n    \n\n  \nIs this a **bug** or a **by-design-feature** I missed spotting in the documentation?  \nThere is an open issue regarding the same problem unique indexes not enforced within array of single document . You can vote for it.  \nAlso there is a nice workaround suggested by Kyle Banker in this similar post Unique indexes on embedded documents  \n**Update**  \nThis is not only related to the embedded fields, we can reproduce the same for array fields too.\n\n    >db.uniqqueTest.insert({a:[1],x:1})\n    >db.uniqqueTest.createIndex({a:1}, {unique: true})\n    > db.uniqqueTest.find()\n    { “_id” : ObjectId(“4f44c6252434860b44986b02”), “a” : [ 1 ],”x”:1 }\n    and it throws an error if we try to create a new document with the same value (correct behavior )\n    > db.uniqqueTest.insert({a:[1],x:3})\n    E11000 duplicate key error index: stack.uniqqueTest.$a_1  dup key: { : 1.0 }\n    But this works fine if we put the same value inside the array (no errors, silently accepts the duplicate value inside the array)\n    > db.uniqqueTest.insert({a:[2],x:2})\n    > db.uniqqueTest.update({x:2},{$push:{a:2}})\n    { “_id” : ObjectId(“4f44c65f2434860b44986b05”), “a” : [ 2, 2 ], “x” : 2 }\n    But not for this\n    > db.uniqqueTest.update({x:2},{$push:{a:1}])\n    E11000 duplicate key error index: stack.uniqqueTest.$a_1  dup key: { : 1.0 }"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Updating a sub-document in mongodb?",
      "explanation": "How do I target an subdocument in the authors array as shown below, in order to update it?\n\n    collection.update({‘_id’: “4f44af6a024342300e000001”}, {$set: { ‘authors.?’ }} )\n\nThe document:\n\n    {\n    _id:     “4f44af6a024342300e000001”,\n    title:   “A book”,\n    created: “2012-02-22T14:12:51.305Z”\n    authors: [{“_id”:”4f44af6a024342300e000002?}]\n    }\n    By specifying actual position of embedded document like this:\n    // update _id field of first author\n    collection.update({‘_id’: “4f44af6a024342300e000001”},\n    {$set: { ‘authors.0._id’: “1” }} )\n    Or via positional operator:\n    // update _id field of first matched by _id author\n    collection.update({‘_id’: “4f44af6a024342300e000001”,\n    //you should specify query for embedded document\n    ‘authors._id’ : “4f44af6a024342300e000002” },\n    // you can update only one nested document matched by query\n    {$set: { ‘authors.$._id’: “1” }} )"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Update MongoDB collection using $toLower",
      "explanation": "I have an existing MongoDB collection containing user names. The user names contain both lower case and upper case letters. I want to update all the user names so they only contain lower case letters. I have tried this script, but it didn’t work\n\n    db.myCollection.find().forEach(\n    function(e) {\n    e.UserName = $toLower(e.UserName);\n    db.myCollection.save(e);\n    }\n    )\n    Any information on getting this to work will be really appreciated, Scott\n    MongoDB does not have a concept of $toLower as a command. The solution is to run a big forloop over the data and issue the updates individually.\n    You can this in any driver or from the shell:\n    db.myCollection.find().forEach(\n    function(e) {\n    e.UserName = e.UserName.toLowerCase();\n    db.myCollection.save(e);\n    }\n    )\n\n  \nYou can also replace the save with an atomic update:  \ndb.myCollection.update({\\_id: e.\\_id}, {$set: {UserName: e.UserName.toLowerCase() } })  \nAgain, you could also do this from any of the drivers, the code will be very similar.  \nEDIT: Remon brings up a good point. The $toLower command does exist as part of the aggregation framework, but this has nothing to do with updating. The documentation for updating is here."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongo ObjectIDs: Safe to use in the wild?",
      "explanation": "I’m designing an API that interacts with MongoDB.  \nNow the question is, if it is **safe** to use the raw ObjectID to query for objects etc. Could any security issues arise when using the OIDs directly (e.g. in queries), or should I encrypt/decrypt them before they leave my server environment?  \nLook at BSON Object ID specification here and you will know **if it is safe for you to use.**  \nIf you try to protect from users sending different URLs from scripts (fuskators) then it seems for me it has weak security. There won’t be too many ‘machine’, ‘pid’ part combinations. ‘time’ part can calculated if attacker can have an idea how data was inserted (especially if using batch). ‘inc’ – very weak.  \nI won’t trust ObjectIDs as the only security.  \nPlease note there can’t be a right answer to the question “is it safe” in general. You must decide yourself.  \n1\\. But keep in mind that such URL-based security will fall to dust when users will share URLs they visited. Even best your encryption won’t help."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "In MongoDB, How to toggle a boolean field in one document with atomic operation?",
      "explanation": "Is there any way to toggle the boolean field of ONE document in MongoDB with atomic operation? Say, (In python)\n\n    cl.update({“_id”: …}, {“$toggle”: {“field”: 1}})\n\nRight now, I don’t think it’s possible to do this with one operation. The bitwise operators (https://www.mongodb.org/display/DOCS/Updating#Updating-%24bit) don’t have a ‘$xor’ yet although I’ve a patch for it.  \nRight now the workaround I think think of is by always using ‘$inc’:\n\n    cl.update( { “_id”: …}, { ‘$inc’ : { ‘field’ : 1 } } );\n\nThen instead of checking for true or false, you can do check whether an item is “true”:\n\n    cl.find( { “_id”: …, ‘field’ : { ‘$mod’ : [ 2, 1 ] } );\n\nIE, you using the modulo operator to see whether it’s even or uneven with even being “unset”, and uneven being “set”. If you want to have the oppposite behaviour (ie, find all items that don’t have the flag set), then use  \n\\[ 2, 0 \\];"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Working with special characters in a Mongo collection",
      "explanation": "I have a collection I’m unable to drop, I’m assuming that the “-” in its name is a special character. In MongoDB, what is the best way to escape special characters?\n\n    > db.tweets.drop();\n    true\n    BUT\n    > db.tweets-old.drop();\n    ReferenceError: old is not defined (shell):1\n\n  \nI’ve tried to escape with quotes (both single and double) and a slash, but nothing works.  \nThe following works:\n\n    db[“tweets-old”].drop();\n\n  \nIt’s called the square bracket notation, which allows you to use special characters in property names."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Add new field to all documents in a nested array",
      "explanation": "I have a database of person documents. Each has a field named photos, which is an array of photo documents. I would like to add a new ‘reviewed’ flag to each of the photo documents and initialize it to false.\n\nThis is the query I am trying to use:\n\n    db.person.update({ “_id” : { $exists : true } }, {$set : {photos.reviewed : false} }, false, true)\n    \n\nHowever I get the following error:  \nSyntaxError: missing : after property id (shell):1  \nIs this possible, and if so, what am I doing wrong in my update?  \nHere is a full example of the ‘person’ document:  \nSkip code block\n\n    {\n    “_class” : “com.foo.Person”,\n    “_id” : “2894”,\n    “name” : “Pixel Spacebag”,\n    “photos” : [\n    {\n    “_id” : null,\n    “thumbUrl” : “https://site.com/a_s.jpg”,\n    “fullUrl” : “https://site.com/a.jpg”\n    },\n    {\n    “_id” : null,\n    “thumbUrl” : “https://site.com/b_s.jpg”,\n    “fullUrl” : “https://site.com/b.jpg”\n    }]\n    }\n\n  \nBonus karma for anyone who can tell me a cleaner why to update “all documents” without using the query { “\\_id” : { $exists : true } }  \nIs this possible, and if so, what am I doing wrong in my update?  \nNo. In general MongoDB is only good at doing updates on top-level objects.  \nThe exception here is the $ positional operator. From the docs: Use this to find an array member and then manipulate it.  \nHowever, in your case you want to modify all members in an array. So that is not what you need.  \nBonus karma for anyone who can tell me a cleaner why to update “all documents”  \nTry db.coll.update(query, update, false, true), this will issue a “multi” update. That lasttrue is what makes it a multi.  \nIs this possible,  \nYou have two options here:  \n1\\. Write afor loop to perform the update. It will basically be a nested for loop, one to loop through the data, the other to loop through the sub-array. If you have a lot of data, you will want to write this is your driver of choice (and possibly multi-thread it).  \n2\\. Write your code to handlereviewed as nullable. Write the data such that if it comes across a photo with reviewed undefined then it must be false. Then you can set the field appropriately and commit it back to the DB.  \nMethod #2 is something you should get used to. As your data grows and you add fields, it becomes difficult to “back-port” all of the old data. This is similar to the problem of issuing a schema change in SQL when you have 1B items in the DB.  \nInstead just make your code resistant against the null and learn to treat it as a default.  \nAgain though, this is still not the solution you seek."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to listen for changes to a MongoDB collection?",
      "explanation": "I’m creating a sort of background job queue system with MongoDB as the data store. How can I “listen” for inserts to a MongoDB collection before spawning workers to process the job? Do I need to poll every few seconds to see if there are any changes from last time, or is there a way my script can wait for inserts to occur? This is a PHP project that I am working on, but feel free to answer in Ruby or language agnostic.  \nWhat you are thinking of sounds a lot like triggers. MongoDB does not have any support for triggers, however some people have “rolled their own” using some tricks. The key here is the oplog.  \nWhen you run MongoDB in a Replica Set, all of the MongoDB actions are logged to an operations log (known as the oplog). The oplog is basically just a running list of the modifications made to the data. Replicas Sets function by listening to changes on this oplog and then applying the changes locally.  \n**Does this sound familiar?**  \nI cannot detail the whole process here, it is several pages of documentation, but the tools you need are available.  \nFirst some write-ups on the oplog – Brief description – Layout of the local collection (which contains the oplog)  \nYou will also want to leverage tailable cursors. These will provide you with a way to listen for changes instead of polling for them. Note that replication uses tailable cursors, so this is a supported feature.  \nMongoDB has what is called capped collections and tailable cursors that allows MongoDB to push data to the listeners.  \nA capped collection is essentially a collection that is a fixed size and only allows insertions. Here’s what it would look like to create one:\n\n    db.createCollection(“messages”, { capped: true, size: 100000000 })\n\n**Ruby example of using tailable cursors:**\n\n    coll = db.collection(‘my_collection’)\n    cursor = Mongo::Cursor.new(coll, :tailable => true)\n    loop do\n    if doc = cursor.next_document\n    puts doc\n    else\n    sleep 1\n    end\n    end\n\n  \n**Additional Resources:**  \nRuby/Node.js Tutorial which walks you through creating an application that listens to inserts in a MongoDB capped collection.  \nAn article talking about tailable cursors in more detail.  \nPHP, Ruby, Python, and Perl examples of using tailable cursors."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB – Query on nested field with index",
      "explanation": "I am trying to figure out how I must structure queries such that they will hit my index. I have documents structured like so:\n\n    { “attributes” : { “make” : “Subaru”, “color” : “Red” } }\n\n  \nWith an index of: db.stuff.ensureIndex({“attributes.make”:1})  \nWhat I’ve found is that querying using dot notation hits the index while querying with a document does not.  \nExample:  \nSkip code block\n\n    db.stuff.find({“attributes.make”:”Subaru”}).explain()\n    {\n    “cursor” : “BtreeCursor attributes.make_1”,\n    “nscanned” : 2,\n    “nscannedObjects” : 2,\n    “n” : 2,\n    “millis” : 0,\n    “nYields” : 0,\n    “nChunkSkips” : 0,\n    “isMultiKey” : false,\n    “indexOnly” : false,\n    “indexBounds” : {\n    “attributes.make” : [\n    [\n    “Subaru”,\n    “Subaru”\n    ]\n    ]\n    }\n    }\n\nvs  \nSkip code block\n\n    db.stuff.find({attributes:{make:”Subaru”}}).explain()\n    {\n    “cursor” : “BasicCursor”,\n    “nscanned” : 2,\n    “nscannedObjects” : 2,\n    “n” : 0,\n    “millis” : 1,\n    “nYields” : 0,\n    “nChunkSkips” : 0,\n    “isMultiKey” : false,\n    “indexOnly” : false,\n    “indexBounds” : {\n    }\n    }\n\n  \nIs there a way to get the document style query to hit the index? The reason is that when constructing queries from my persistent objects it’s much easier to serialize them out as documents as opposed to something using dot notation.  \nI’ll also add that we’re using a home grown data mapper layer built w/ Jackson. Would using something like Morphia help with properly constructing these queries?  \nDid some more digging and this thread explains what’s going with the sub-document query. My problem above was that to make the sub-document based query act like the dot-notation I needed to use elemMatch.\n\n    Skip code block\n    db.stuff.find({“attributes”:{“$elemMatch” : {“make”:”Subaru”}}}).explain()\n    {\n    “cursor” : “BtreeCursor attributes.make_1”,\n    “nscanned” : 2,\n    “nscannedObjects” : 2,\n    “n” : 0,\n    “millis” : 2,\n    “nYields” : 0,\n    “nChunkSkips” : 0,\n    “isMultiKey” : false,\n    “indexOnly” : false,\n    “indexBounds” : {\n    “attributes.make” : [\n    [\n    “Subaru”,\n    “Subaru”\n    ]\n    ]\n    }\n    }"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Save Subset of MongoDB Collection to Another Collection",
      "explanation": "I have a set like so\n\n    {date: 20120101}\n    {date: 20120103}\n    {date: 20120104}\n    {date: 20120005}\n    {date: 20120105}\n\n  \nHow do I save a subset of those documents with the date ‘20120105’ to another collection?  \n**i.e** db.subset.save(db.full\\_set.find({date: “20120105”}));  \nHere’s the shell version:\n\n    db.full_set.find({date:”20120105?}).forEach(function(doc){\n    db.subset.insert(doc);\n    });\n\n  \nThere’s no direct equivalent of SQL’s insert into … select from ….  \nYou have to take care of it yourself. Fetch documents of interest and save them to another collection.  \nYou can do it in the shell, but I’d use a small external script in Ruby. Something like this:  \nrequire ‘mongo’\n\n    db = Mongo::Connection.new.db(‘mydb’)\n    source = db.collection(‘source_collection’)\n    target = db.collection(‘target_collection’)\n    source.find(date: “20120105”).each do |doc|\n    target.insert doc\n    end\n\n  \nActually, there is an equivalent of SQL’s insert into … select from in MongoDB. First, you convert multiple documents into an array of documents; then you insert the array into the target collection\n\ndb.subset.insert(db.full\\_set.find({date:”20120105?}).toArray())"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What characters are NOT allowed in MongoDB field names?",
      "explanation": "I figured out that of course . and SPACE aren’t allowed. Are there other forbidden characters ?  \nYou can use any (UTF8) character in the field name which aren’t special (contains “.”, or starts with “$”).\n\nhttps://jira.mongodb.org/browse/SERVER-3229  \nhttps://stackoverflow.com/a/7976235/311220\n\nIt’s generally best to stick with lowercase alphanumeric with underscores though."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Is MongoDB somehow limited to a single core?",
      "explanation": "Perhaps I have misconfigured MongoDB somehow, but even under heavy load I don’t see it using more than one core. For example, top is currently showing:  \nSkip code block  \n  \n\n    Tasks: 145 total,   1 running, 144 sleeping,   0 stopped,   0 zombie\n    Cpu0  :  0.0%us,  0.0%sy,  0.0%ni,100.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st\n    Cpu1  :  0.0%us,  0.0%sy,  0.0%ni,100.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st\n    Cpu2  :  0.0%us,  0.0%sy,  0.0%ni,100.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st\n    Cpu3  :100.0%us,  0.0%sy,  0.0%ni,  0.0%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st\n    Mem:  41182768k total, 40987476k used,   195292k free,   109956k buffers\n    Swap:  2097144k total,  1740288k used,   356856k free, 28437928k cached\n    PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND\n    16297 mongod    15   0  521g  18g  18g S 99.8 47.2   2929:32 mongod\n    1 root      15   0 10372  256  224 S  0.0  0.0   0:03.39 init\n\n  \nIs there something I can do to get Mongo to use the other cores more effectively? If it’s relevant, I currently have a big M/R running which seems to have put a lot of read queries in “waiting” mode.  \nMongoDB can saturate all cores on a multi-core machine for read operations, but for write operations and map-reduce MongoDB can only utilize a single core per mongod process.  \nThe limitation for single-core MapReduce is due to the Javascript interpreter that MongoDB utilizes. This is something that is supposed to be fixed in the future but in the interim you can use Hadoop to execute the MapReduce and store the result set in your MongoDB database.  \nAnother option which has seen mixed results is to run a single mongod process for every core on the instance this will not increase performance for a single database unless they are configured to run in a sharded setup."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongo count really slow when there are millions of records",
      "explanation": "    //FAST\n    db.datasources.find().count()\n    12036788\n    //SLOW\n    db.datasources.find({nid:19882}).count()\n    10161684\n\n  \nIndex on nid  \nAny way to make the second query faster? (It is taking about 8 seconds)  \nCount queries, indexed or otherwise, are slow due to the fact that MongoDB still has to do a full b-tree walk to find the appropriate number of documents that match your criteria. The reason for this is that the MongoDB b-tree structure is not “counted” meaning each node does not store information about the amount of elements in the node/subtree.  \nThe issue is reported here https://jira.mongodb.org/browse/SERVER-1752 and there is currently no workaround to improve performance other than manually maintaining a counter for that collection which obviously comes with a few downsides.  \nAlso note that the db.col.count() version (so no criteria) can take a big shortcut and doesn’t actually perform a query hence it’s speed. That said it does not always report the same value as a count query would that should return all elements (it won’t be in sharded environments with high write throughput for example). Up for debate whether or not that’s a bug. I think it is.  \nNote that in 2.3+ a significant optimization was introduced that should (and does) improve performance of counts on indexed fields.\n\nSee: https://jira.mongodb.org/browse/SERVER-7745"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB not equal to",
      "explanation": "I’m trying to display a query in MongoDB where a text field is not ” (blank)\n\n    { ‘name’ : { $not : ” }}\n\n  \nHowever I get the error invalid use of $not  \nI’ve looked over the documentation but the examples they use are for complicated cases (with regexp and $not negating another operator).  \nHow would I do the simple thing I’m trying to do?  \nUse $ne — $not should be followed by the standard operator:  \nAn examples:  \nuse test  \nswitched to db test\n\n    db.test.insert({author : ‘me’, post: “”})\n    db.test.insert({author : ‘you’, post: “how to query”})\n    db.test.find({‘post’: {$ne : “”}})\n    { “_id” : ObjectId(“4f68b1a7768972d396fe2268”), “author” : “you”, “post” : “how to query” }\n    And now $not:\n    db.test.find({‘post’: {$not: {$ne : “”}}})\n    { “_id” : ObjectId(“4f68b19c768972d396fe2267”), “author” : “me”, “post” : “” }\n\n  \nUse $ne instead of $not\n\n**https://docs.mongodb.org/manual/reference/operator/ne/#op.\\_S\\_ne**  \ndb.collections.find({“name”: {$ne: “”}});\n\nIf you want to do multiple $ne then do\n\n    db.users.find({name : {$nin : [“mary”, “dick”, “jane”]}})"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Multiple $elemMatch expressions for matching array values using $all in MongoDB?",
      "explanation": "In the answer to a question I found a interesting solution for searching array values using $elemMatch.  \nIf we have the following documents in our collection:\n\n    {\n    foo : [ { bar : “xy”, baz : 1 },\n    { bar : “a”, baz : 10 } ]\n    },\n    {\n    foo : [ { bar : “xy”, baz : 5 },\n    { bar : “b”, baz : 50 } ]\n    }\n\n  \nThe following query will match only the first document:\n\n    db.test.find({\n    foo : { “$all” : [ { “$elemMatch” : { bar : “xy”, baz : 1} }, { “$elemMatch” : { bar : “a”, baz: 10 } } ] }\n    });\n\n  \nI tried it with several other examples and it really works. But the official documentation for $alloperator doesn’t say anything about combining these two queries.  \nIs this the intended behavior or a bug? Or is this just a problem that the documentation does not cover this use case?  \nThis is the intended behavior. The documentation doesn’t cover this use case and we are working on it to make it better. Its difficult, however, to document every possible query combination."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to remove duplicate entries from an array?",
      "explanation": "How to remove duplicate entries from an array?  \nIn below example “Algorithms in C++” is added twice.  \n$unset modifier removes a particular field but how to remove an entry from a field?\n\n    > db.users.find()\n    { “_id” : ObjectId(“4f6cd3c47156522f4f45b26f”),\n    “favorites” : { “books” : [ “Algorithms in C++”,\n    “The Art of Computer Programmning”,\n    “Graph Theory”,\n    “Algorithms in C++” ] },\n    “name” : “robert” }\n\n  \nWhat you have to do is use map reduce to detect and count duplicate tags .. then use $set to replace the entire books based on { “\\_id” : ObjectId(“4f6cd3c47156522f4f45b26f”),  \nThis has been discussed sevel times here .. please seee  \nRemoving duplicate records using MapReduce  \nFast way to find duplicates on indexed column in mongodb\n\nhttps://csanz.posterous.com/look-for-duplicates-using-mongodb-mapreduce  \nhttps://www.mongodb.org/display/DOCS/MapReduce\n\nHow to remove duplicate record in MongoDB by MapReduce?  \nThanks :)  \nAs of MongoDB 2.2 you can use the aggregation framework with an $unwind, $group and$project stage to achieve this:\n\n    db.users.aggregate([{$unwind: ‘$favorites.books’},\n    {$group: {_id: ‘$_id’,\n    books: {$addToSet: ‘$favorites.books’},\n    name: {$first: ‘$name’}}},\n    {$project: {‘favorites.books’: ‘$books’, name: ‘$name’}}\n    ])\n\n  \nNote the need for the $project to rename the favorites field, since $group aggregate fields cannot be nested."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Order of $lt and $gt in MongoDB range query",
      "explanation": "Today I have noticed that the order in which the $lt and $gt operators are given seem to matter in MongoDB 2.0.2.  \nI have a database of games. “player” is an array of two strings representing both players, “endedAtMS” is a timestamp when the game has ended. I have created this index:  \ndb.games.ensureIndex({player:1,endedAtMS:-1})  \nTo get 30 of my games which were finished in a certain time range, ordered by the time the games where finished, I do:\n\n    Skip code block\n    db.games.find({ “player” : “Stefan” ,\n    “endedAtMS” : { “$lt” : 1321284969946 ,\n    “$gt” : 1301284969946}}).\n    sort({endedAtMS:-1}).\n    limit(30).\n    explain()\n    {\n    “cursor” : “BtreeCursor player_1_endedAtMS_-1”,\n    “nscanned” : 30,\n    “nscannedObjects” : 30,\n    “n” : 30,\n    “millis” : 0,\n    “nYields” : 0,\n    “nChunkSkips” : 0,\n    “isMultiKey” : true,\n    “indexOnly” : false,\n    “indexBounds” : {\n    “player” : [\n    [\n    “Stefan”,\n    “Stefan”\n    ]\n    ],\n    “endedAtMS” : [\n    [\n    1321284969946,\n    -1.7976931348623157e+308\n    ]\n    ]\n    }\n    }\n\n  \nAll seems to work fine. However when I change the order of $lt and $gt in the query above I get this:\n\n    Skip code block\n    db.games.find({ “player” : “Stefan” ,\n    “endedAtMS” : { “$gt”:1301284969946,\n    “$lt” : 1321284969946}}).\n    sort({endedAtMS:-1}).\n    limit(30).\n    explain()\n    {\n    “cursor” : “BtreeCursor player_1_endedAtMS_-1”,\n    “nscanned” : 126,\n    “nscannedObjects” : 126,\n    “n” : 30,\n    “millis” : 1,\n    “nYields” : 0,\n    “nChunkSkips” : 0,\n    “isMultiKey” : true,\n    “indexOnly” : false,\n    “indexBounds” : {\n    “player” : [\n    [\n    “Stefan”,\n    “Stefan”\n    ]\n    ],\n    “endedAtMS” : [\n    [\n    1.7976931348623157e+308,\n    1301284969946\n    ]\n    ]\n    }\n    }\n\n  \nAs you can see 126 docs need to be scanned to get the 30 docs for the result. If you take a look at the indexBounds in the explain output it seems that only the first operator is used to limit the search space in the index.  \nWhat do I miss? Why is Mongo only using one operator to limit the search space?  \nThis is a known issue. The short answer is that it has to do with the fact that a multikey index is used (“player” is an array), and the index cannot be constrained on both upper and lower bounds.  \nThis is explained in more detail in the Jira case: https://jira.mongodb.org/browse/SERVER-4155 – “Index bound incorrect?”  \nThere is an open Jira ticket to improve this behavior: https://jira.mongodb.org/browse/SERVER-4180 – “Wrong indexbounds picked for a date range query (regression)” which is slated to be released in version 2.1.2 (this version is subject to change). Please vote for it!"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Foreign key like relationship in Mongo DB",
      "explanation": "How can I implement a foreign key like relationship in Mongo DB?  \nhiya see this: MongoDB normalization, foreign key and joining &&\n\nfurther ===>https://books.google.com/books/about/Document\\_Design\\_for\\_MongoDB.html?id=TbIHkgEACAAJ&redir\\_esc=y\n\nhope it helps! cheerios!  \nMongoDB doesn’t support server side foreign key relationships, normalization is also discouraged. You should embed your child object within parent objects if possible, this will increase performance and make foreign keys totally unnecessary. That said it is not always possible, so there is a special construct called DBRef which allows to reference objects in a different collection. This may be then not so speedy because DB has to make additional queries to read objects but allows for kind of foreign key reference.  \nStill you will have to handle your references manually. Only while looking up your DBRef you will see if it exists, the DB will not go through all the documents to look for the references and remove them if the target of the reference doesn’t exist any more. But I think removing all the references after deleting the book would require a single query per collection, no more, so not that difficult really.\n\n  \n**Edit update**\n\n  \nhttps://levycarneiro.com/tag/mongodb/\n\nlevycarneiro.com/tag/mongodb \\[quote\\] So you create 4 collections: Clients, Suppliers, Employees and Contacts. You connect them all together via a db reference. This acts like a foreign key. But, this is not the mongoDB way to do things. Performance will penalized. \\[unquote\\]"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to get a specific embedded document inside a MongoDB collection?",
      "explanation": "I have a collection Notebook which has embedded array document called Notes. The sample  \ndocument looks like as shown below.\n\n    Skip code block\n    {\n    “_id” : ObjectId(“4f7ee46e08403d063ab0b4f9”),\n    “name” : “MongoDB”,\n    “notes” : [\n    {\n    “title” : “Hello MongoDB”,\n    “content” : “Hello MongoDB”\n    },\n    {\n    “title” : “ReplicaSet MongoDB”,\n    “content” : “ReplicaSet MongoDB”\n    }\n    ]\n    }\n\n  \nI want to find out only note which has title “Hello MongoDB”. I am not getting what should  \nbe the query. Can anyone help me.  \nI don’t believe what you are asking is possible, at least without some map-reduce maybe?  \nSee here: Filtering embedded documents in MongoDB  \nThat answer suggests you change your schema, to better suit how you’d like to work with the data.  \nYou can use a either “dot notation” or $elemMatch to get back the correct, document that has the matching “note title” …  \n\\>\n\n     db.collection.find({ “notes.title” : “Hello MongoDB”}, { “notes.title” : 1?});\n    or …\n    > db.collection.find({ “notes” : { “$elemMatch” : { “title” : “Hello MongoDB”} }});\n\n  \nBut you will get back the whole array, not just the array element that caused the match.  \nAlso, something to think about … with your current setup it woud be hard to do any operations on the items in the array.  \nIf you don’t change your schema (as the answer linked to suggests) … I would consider adding “ids” to each element in the array so you can do things like delete it easily if needed.  \nYou can do this with mongo version higher 2.2  \nthe query like this:\n\n    db.coll.find({ ‘notes.title’: ‘Hello MongoDB’ }, {‘notes.$’: 1});\n\n  \nyou can try with $elemMatch like Justin Jenkins"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "can you have mongo $push prepend instead of append?",
      "explanation": "I’d like to have push add at the beginning of my set rather than appended to the back when I do a mongo $push.  \nIs it possible to do an atomic push update that adds it as the first element rather than the last?  \nA similar question was asked a few days ago. Unfortunately, the short answer is, “no”, but there is an open request for this feature.  \nhttps://jira.mongodb.org/browse/SERVER-2191 – “$push() to front of array”  \nThere is some more information as well as a possible work-around on the other thread: “Use MongoDB array as stack” – Use MongoDB array as stack  \nHopefully the above will be useful and help you to find an acceptable work-around.  \nUse negative index with $set for prepend, tested in mongo v2.2:\n\n    Skip code block\n    > db.test.insert({‘array’: [4, 5, 6]})\n    > db.test.find()\n    { “_id” : ObjectId(“513ad0f8afdfe1e6736e49eb”),\n    “array” : [ 4, 5, 6 ] }\n    //prepend 3\n    > db.test.update({“_id” : ObjectId(“513ad0f8afdfe1e6736e49eb”)},\n    {‘$set’: {‘array.-1’:     3}})\n    > db.test.find()\n    { “_id” : ObjectId(“513ad0f8afdfe1e6736e49eb”),\n    “array” : [ 3, 4, 5, 6 ] }\n    //prepend 2\n    > db.test.update({“_id” : ObjectId(“513ad0f8afdfe1e6736e49eb”)},\n    {‘$set’: {‘array.-1’:     2}})\n    > db.test.find()\n    { “_id” : ObjectId(“513ad0f8afdfe1e6736e49eb”),\n    “array” : [ 2, 3, 4, 5, 6 ] }\n    //prepend 1\n    > db.test.update({“_id” : ObjectId(“513ad0f8afdfe1e6736e49eb”)},\n    {‘$set’: {‘array.-1’:     1}})\n    > db.test.find()\n    { “_id” : ObjectId(“513ad0f8afdfe1e6736e49eb”),\n    “array” : [ 1, 2, 3, 4, 5, 6 ] }\n\n  \nAs of MongoDB v2.5.3, there is a new $position operator that you can include along with the$each operator as part of your $push query to specify the location in the array at which you would like to insert a value.  \nHere’s an example from the docs page to add the elements 20 and 30 at the array index of 2::\n\n    db.students.update( { _id: 1 },\n    { $push: { scores: {\n    $each: [ 20, 30 ],\n    $position: 2\n    }\n    }\n    }\n    )\n\n  \nReference: https://docs.mongodb.org/master/reference/operator/update/position/#up.\\_S\\_position"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How can I create unique IDs for embedded documents in MongoDB?",
      "explanation": "So I need to reference particular subdocuments uniquely from items in my collection. For instance:\n\n    User = {\n    ‘name’: ‘jim’,\n    ‘documents: [\n    {‘id’: 0001, ‘title’: “My document”},\n    {‘id’: 0002, ‘title’: “My second document!”},\n    ]\n    }\n\n  \nSo I need to be able to auto-create IDs for new documents, preferably not at the application level (since there will be race conditions in the actual development scenario).  \nIs there a way to use mongo’s autogenerated ObjectId (used in the \\_id field at the collection level), or something similar?  \nYes, using mongo’s ObjectId is the way to go. The only thing is: you have to generate them yourself, in the application code. They are meant to be globally unique, different workers won’t generate two identical ObjectIds, so there’s no race condition in that sense.  \nAll official drivers should provide a way to generate ObjectId. Here’s how it is in Ruby:\n\n    oid = BSON::ObjectId.new\n    All drivers have functionality for generating ObjectIds.\n    In the shell you just do new ObjectId():\n    > db.test.insert({x:new ObjectId()});\n    > db.test.find();\n    { “_id” : ObjectId(“4f88592a06c05e4de90d0bc1”), “x” : ObjectId(“4f88592a06c05e4de90d0bc0”) }\n\n  \nIn Java it’s new ObjectId() as well. See the API docs for your driver to see the specific syntax."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Deleting a key/value from existing MongoDB entry",
      "explanation": "I need to delete a certain key and value from every entry in a particular collection. I’ve looked into remove and that seems to be for only entire entries. Looking at update, I don’t believe updating a particular key with null or an empty string would achieve what I’m trying to do. I’m very much a beginner with mongodb, so please excuse my ignorance.  \nLong story short, how can I turn\n\n    {\n    “_id” : 1234,\n    “name” : “Chris”,\n    “description” : “Awesome”\n    }\n    into\n    {\n    “_id” : 1234,\n    “name” : “Chris”\n    }\n\n  \nwithout deleting the entry and creating a new one, or using any non-mongodb commands? Thanks!  \nTry $unset in a call to update().  \nIn other words,\n\n    db.collection_name.update({ _id: 1234 }, { $unset : { description : 1} });\n    \n\nshould work. Link."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Problems with Mongo value-in-array query: $not operator doesn’t work, and ObjectIds cannot be selected",
      "explanation": "I am trying to select a document which does NOT contain a value in a document’s array.  \nI’m having two problems, which I’ll present separately:"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Why does this not work?",
      "explanation": "(2) I cannot get the value in array query to work if the array values are ObjectIds:\n\n    { _id: Object(“000000000000000000000000”), mylist: [ ObjectId(“111111111111111111111111”) ] }\n    The following does NOT retrieve this document:\n    myCol.find({mylist:ObjectId(“111111111111111111111111”)})\n    Can someone suggest what I might be doing wrong?\n    There is no $not that works like, you want $ne for simple cases like yours:\n    db.myCol.find({ mylist: { $ne: ‘orange’ } })\n\n  \nYour second should (and does) work fine for me, perhaps you’re using the wrong number of ones."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Updating a document field in mongo based on another field’s value",
      "explanation": "Suppose I have two fields F1 and F2. I want to update F1 to become F1 + “, ” + F2. Can I do so with a single update command in mongo?  \nNo, you can’t do that. You can’t use expressions in mongodb updates. The only way is to fetch this document to the client, compose new field value there and issue a prepared update statement."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "mongodb map reduce to get uniques by date/day",
      "explanation": "trying to group by date and number\n\n    my document\n    { “_id” : ObjectId(“4f956dee76ddb26752026e8f”), “request” : “default”, “options” : “1”, “keyword” : “somekey”, “number” : “5b234b79-4d70-437e-8eef-32a2941af40a”, “date” : “20120423200446”, “time” : 1335193066 }\n    my query\n    map = “function() { var date = new Date(this.time * 1000); var key = date.getFullYear() + date.getMonth() + date.getDate(); emit({day : key, number: this.number}, {count: 1}); }”\n    reduce = “function(key, values) { var count = 0; values.forEach(function(v) { count += v[‘count’]; }); return {count: count}; }”\n    db.txtweb.mapReduce(map, reduce, {out: “pageview_results”});\n    my error\n\n  \nuncaught exception: map reduce failed:{ “errmsg” : “ns doesn’t exist”, “ok” : 0 }  \nI cannot figure out whats wrong, but I think it is do something with the date functionality.  \nAny ideas.  \nns doesn’t exist means that you’re accessing a non-existent database or collection. Check their names  \nScript below group this.amount by day (without time)\n\n    Skip code block\n    db.payments.mapReduce(\n    function() {\n    var k = clone(this.payment_time);\n    k.setMinutes(0);\n    k.setSeconds(0);\n    k.setMilliseconds(0);\n    emit(k, this.amount);\n    },\n    function (key, vals) {\n    var total = 0;\n    for (var i = 0; i < vals.length; i++) {\n    total += vals[i];\n    }\n    return total;\n    },\n    {\n    out : {merge : “resultName” }\n    }\n    );\n    }\n    };"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "mongo – how to query a nested json",
      "explanation": "I am a complete mongo newbie. I am using mongo hub for mac. I need to query the for following json –\n\n     {“_id” : ObjectId( “abcd” ),\n    “className” : “com.myUser”,\n    “reg” : 12345,\n    “test” : [\n    { “className” : “com.abc”,\n    “testid” : “pqrs” } ] }\n\n  \nand find records where testid is pqrs. How would I go about doing that?  \nYou can type {‘test.testid’: ‘pqrs’} in the query field of Mongo Hub."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongodb find a document with all subdocuments satisfying a condition",
      "explanation": "I have Game collection in my DB:\n\n    var game = {\n    players: [{username:”user1?, status:”played”},\n    {username:”user2?, status:”accepted”}]\n    }\n\n  \nAs far as I understand query like this: db.games.find({“players.status”:”played”}) will give me all games where at least one player has status “played”. How can I find games with ALL players having status “played”?  \nIf you only have one other status than “played” use the query:\n\n    db.games.find({“players.status”:{$ne:”accepted”}})\n\n  \nYou can adjust the query to handle more status values, as long as they are all known at the time of the query."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB – Update an object in nested Array",
      "explanation": "Assume we have the following collection, which I have few questions about:\n\n    Skip code block\n    {\n    “_id” : ObjectId(“4faaba123412d654fe83hg876”),\n    “user_id” : 123456,\n    “total” : 100,\n    “items” : [\n    {\n    “item_name” : “my_item_one”,\n    “price” : 20\n    },\n    {\n    “item_name” : “my_item_two”,\n    “price” : 50\n    },\n    {\n    “item_name” : “my_item_three”,\n    “price” : 30\n    }\n    ]\n    }\n\n  \n1 – I want to increase the price for “item\\_name”:”my\\_item\\_two” **and if it doesn’t exists**, it should be appended to the “items” array.  \n2 – How can I update two fields at the same time. For example, increase the price for “my\\_item\\_three” and at the same time increase the “total” (with the same value).  \nI prefer to do this on the MongoDB side, otherwise I have to load the document in client-side (Python) and construct the updated document and replace it with the existing one in MongoDB.  \n**UPDATE** This is what I have tried and works fine **IF THE Object Exists :**  \ndb.test\\_invoice.update({user\\_id : 123456 , “items.item\\_name”:”my\\_item\\_one”} , {$inc: {“items.$.price”: 10}})  \nBut if the key doesn’t exist it does nothing. Also it only updates the nested object. There is no way with this command to update the “total” field as well.  \nFor question #1, let’s break it into two parts. First, increment any document that has “items.item\\_name” equal to “my\\_item\\_two”. For this you’ll have to use the positional “$” operator. Something like:\n\n    db.bar.update( {user_id : 123456 , “items.item_name” : “my_item_two” } ,\n    {$inc : {“items.$.price” : 1} } ,\n    false ,\n    true);\n\n  \nNote that this will only increment the first matched subdocument in any array (so if you have another document in the array with “item\\_name” equal to “my\\_item\\_two”, it won’t get incremented). But this might be what you want.  \nThe second part is trickier. We can push a new item to an array without a “my\\_item\\_two” as follows:\n\n    db.bar.update( {user_id : 123456, “items.item_name” : {$ne : “my_item_two” }} ,\n    {$addToSet : {“items” : {‘item_name’ : “my_item_two” , ‘price’ : 1 }} } ,\n    false ,\n    true);\n    For your question #2, the answer is easier. To increment the total and the price of item_three in any document that contains “my_item_three,” you can use the $inc operator on multiple fields at the same time. Something like:\n    db.bar.update( {“items.item_name” : {$ne : “my_item_three” }} ,\n    {$inc : {total : 1 , “items.$.price” : 1} ,\n    false ,\n    true);"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "To what extent are ‘lost data’ criticisms still valid of MongoDB?",
      "explanation": "I’m referring to the following\n\nhttps://pastebin.com/raw.php?i=FD3xe6Jt\n\nWhich, if still valid would be worrying to some extent. The article primarily references v1.6 and v1.8 and since then v2 has been released. Are the shortcomings discussed in the article still outstanding as of the current release?  \nThat particular post was debunked, point by point by the MongoDB CTO and co-founder, Eliot Horowitz, here:\n\nhttps://news.ycombinator.com/item?id=3202959\n\nThere is also a good summary here:\n\nhttps://www.betabeat.com/2011/11/10/the-trolls-come-out-for-10gen/\n\nThe short version is, it looks like this was basically someone trolling for attention (successfully), with no solid evidence or corroboration. There have been genuine incidents in the past, which have been dealt with as the product evolved (see the introduction of journaling in 1.8 for example) or as more specific bugs were found and fixed.  \n**Disclaimer**: I do work for MongoDB (formerly 10gen), and love the fact that philnate got here and refuted this independently first – that probably says more about the product than anything else :)  \n**Update: August 19th 2013**  \nI’ve seen quite a bit of activity on this answer recently, which I assume is related to the announcement of the bug in SERVER-10478 – it is most certainly an edge case, but I would still recommend anyone using sharding with large documents to upgrade ASAP to v2.2.6 and v2.4.6 which include the fix for this issue.  \nNever heard of those severe problems in recent versions. What you need to consider is that MongoDB has no decade of development as relational Systems in the back. Further it may be true that MongoDB doesn’t offer that much functionality to avoid data loss at all. But even with relational Systems you won’t be ever sure that you’ll never loose any data. It highly depends on your system configuration (so with Replication and manual data backups you should be quite safe).  \nAs a general guideline to avoid Beta Bugs or bugs from early versions, avoid to use fresh versions in productions (there’s a reason why debian is so popular for servers). If MongoDB would suffer such severe problems (all the time) the list of users would be smaller:https://www.mongodb.org/display/DOCS/Production+Deployments Additionally I don’t really trust this pastebin message, why is this published anonymously? Is this person company shamed to tell that they used mongodb, do they fear 10gen? Where a links to those Bug reports (or did 10gen delete them from JIRA?)  \nSo lets talk shortly about those points:  \n1\\. Yep MongoDB operates normally in fire and forget mode. But you can modify this bevavior with several options: https://www.mongodb.org/display/DOCS/getLastError+Command. So only because MongoDB defaults to it, it doesn’t mean you can’t change it to your needs. But you need to live less performance if you don’t fire and forget within your app, as you’re adding a roundtrip.  \n2\\. Never heard of such problems, except those caused to own failure…but that can happen with relational systems as well. I guess this point only talks about Master-Slave Replication. Replica-Sets are much never and stable. Some links from the web where other dbms caused data loss due to malfunction as well: https://blog.lastinfirstout.net/2010/04/bit-by-bug-data-loss-running-oracle-on.html https://dbaspot.com/oracle-server/430465-parallel-cause-data-lost-another-oracle-bug.html https://bugs.mysql.com/bug.php?id=18014 (Those posted links aren’t in any favor of a given system or should imply anything else than showing that there are bugs in other systems as well, who can cause data loss.)  \n3\\. Yes actually there’s Locking at instance level, I don’t think that in sharded environment this is a global one, I think this will be at instance level for each shard separate, as there’s no need to lock other instances as there are no consistency checks needed. The upcoming Version 2.2 will lock at DB Level, tickets for Collection Level and maybe extend or document exists as well (https://jira.mongodb.org/browse/SERVER-4328). But locking at deeper levels may affect the actual performance of MongoDB, as a lock management is expensive.  \n4\\. Moving chunks shouldn’t cause much problems as rebalancing should take a few chunks from each node and move them to the new one. It never should cause ping/pong of chunks nor does rebalancing start just because of a difference of one or two chunks. What can be problematic is when your shard key is choosen wrong. So you may end up with all new entries inserted to one node rather than all. So you would see more often rebalancing which can cause problems, but that would be not due to mongo rather than your poorly choosen shardkey.  \n5\\. Can’t comment on this one  \n6\\. Not 100% sure, but I think Replicasets where introduced in 1.6, so as told earlier never use the latest version for production, except you can live with loss of data. As with every new feature there’s the possibility of bugs. Even extensive testing may not reveal all problems. Again always run some manual backup for gods sake, except you can live with data loss.  \n7\\. Can’t comment on this. But in reality software may contain severe bugs. Many games suffer those problems as well and there are other areas as well where banana software was quite well known or is. Can’t Comment about something concrete as this was before my MongoDB time.  \n8\\. Replication can cause such problems. Depending on the replication strategy this may be a problem and another system may fit better. But without a really really write intensive workload you may not encounter such problems. Indeed it may be problematic to have 3 replicas polling changes from one master. You could cure the problem by adding more shards.  \nAs a general conclusion: Yeah it may be that those problems were existent, but MongoDB did much in this direction and further I doubt that other DBMS never had the one or other problem itself. Just take traditional relational dbms, would those scale well to web-scale there would be no need for Systems like MongoDB, HBase and what else. You can’t get a system which fits all needs. So you have to live with the downsides of one or try to build a combined system of multiple to get what you need.  \nDisclaimer: I’m not affiliated with MongoDB or 10gen, I’m just working with MongoDB and telling my opinion about it.  \nI can’t speak for every case, only my own. However I don’t work for 10gen or its competitors, and I have lost data when using MongoDB, so here goes.  \nThe main criticisms of Mongo around the last time I used it (2010) were:  \n1\\. Supposedly stable versions of Mongo had major data-losing bugs that weren’t made explicit to users. Eg, prior to 1.8 non-clustered configurations were likely to lose data. This was documented by Mongo, but not to the extent a data losing bug in a stable-versioned DB would normally be.  \nThe main defence of that criticism was:  \n2\\. Users were informed of this danger, albeit not so explicitly. Users should read all the documentation before they begin.  \nIn my own case, I was using 1.7 in a single server configuration but aware of the risk. I shut down the DB to take a back up. The act of shutting down the DB itself lost my data, 10gen assisted (for free) but were unable to recover the data."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongo DB Delete a field and value",
      "explanation": "In mongo shell how would I delete all occurrences of “id” : “1” the value of the field is always different. Would I use the $unset operator? Would that delete the value and the field?  \nYou’re saying remove all occurrences of the field, right? If so, then it should be like this:\n\n    db.collection.update(\n    { id: { $exists: true } },  // criteria\n    { $unset: { id: 1 } },      // modifier\n    false,                      // no need to upsert\n    true                        // multi-update\n    );"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Checking if a field contains a string",
      "explanation": "I’m looking for an operator, which allows me to check, if the value of a field contains a certain string.  \nSomething like:\n\n    db.users.findOne({$contains:{“username”:”son”}})\n\n  \nIs that possible?  \nYou can do it with the following code.\n\n    db.users.findOne({“username” : {$regex : “.*son.*”}});\n    \n\nAs Mongo shell support regex, that’s completely possible.\n\n    db.users.findOne({“username” : /.*son.*/});\n\n  \nIf we want the query to be case-insensitive, we can use “i” option, like shown below:\n\n    db.users.findOne({“username” : /.*son.*/i});\n\nSee: https://www.mongodb.org/display/DOCS/Advanced+Queries#AdvancedQueries-RegularExpressions  \nhttps://www.mongodb.org/display/DOCS/SQL+to+Mongo+Mapping+Chart  \nhttps://php.net/manual/en/mongo.sqltomongo.php\n\n    MySQL\n    SELECT * FROM users WHERE username LIKE “%Son%”\n    MongoDB\n    db.users.find({username:/Son/})"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What’s the fastest way to copy a collection within the same database?",
      "explanation": "I want to copy a collection within the **same** database and give it a different name – basically take a snapshot.  \nWhat’s the best way to do this? Is there a command, or do I have to copy each record in turn?  \nI’m aware of the cloneCollection command, but it seems to be for copying to another server only.  \nI’m also aware of mongoimport and mongoexport, but as I’m doing this via PHP I’d prefer not to make calls out to the shell.  \nYou have a few options, but the fastest is:  \nmongoexport -d db -c sourcecollection | mongoimport -d db -c targetcollection –drop  \nor in php:\n\n    `mongoexport -d db -c sourcecollection | mongoimport -d db -c targetcollection –drop`;\n\n  \nafter that you have\n\n    mongo db < script.js\n\n  \nwhere, as shown in the mongo docs, script.js contains something like:\n\n    db.myoriginal.find().forEach( function(x){db.mycopy.insert(x)} );\n\n  \nThe slowest (by an order of magnitude or more) way to copy a collection will be to use the native php driver – simply because of moving information around. But you could issue the above mongo query if you absolutely want to avoid cli calls using the db execute function."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "mongodb: upserting: only set value if document is being inserted",
      "explanation": "Considering a simple mongo document structure:  \n{ \\_id, firstTime, lastTime }  \nThe client needs to insert a document with a known ID, or update an existing document. The ‘lastTime’ should always be set to some latest time. For the ‘firstTime’, if a document is being inserted, then the ‘firstTime’ should be set to current time. However, if the document is already created, then ‘firstTime’ remain unchanged. I would like to do it purely with upserts (to avoid look ups).\n\n  \nI’ve crawled the https://www.mongodb.org/display/DOCS/Updating,\n\nbut I just don’t see how that particular operation can be done.  \nI don’t believe this is something unreasonable, there are $push and $addToSet operations that effectively do that on array fields, just nothing that would do the same on simple fields. It’s like there should be something like $setIf operation.  \nI ran into the exact same problem and there was no simple solution for <2.4 however since 2.4 the **$setOnInsert** operator let’s you do exactly that.\n\n    db.collection.update( ,\n    { $setOnInsert: { “firstTime”: } },\n    { upsert: true }\n    )\n\n  \nSee the 2.4 release notes of setOnInsert for more info."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Change collection name in mongodb",
      "explanation": "Changing the name of a collection in mongodb can be achieved by copy the documents in the collection to a new one and delete the original.  \nBut is there a simpler way to change the name of a collection in mongodb?  \ndb.oldname.renameCollection(“newname”)  \nReally? No google search?\n\nhttps://www.mongodb.org/display/DOCS/renameCollection+Command  \n  \n\\> db.oldname.renameCollection(“newname”)"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "what’s the diff between findAndModify and update in MongoDB?",
      "explanation": "I’m a little bit confused by the findAndModify method in MongoDB. What’s the advantage of it over update method? For me, it seems that it just returns the item first and then update it.But Why do I need to return the item first? I read the **MongoDB: the definitive guide** and it says that It is handy for manipulating queues and performing other operations that need get-and-set style atomicity. But I didn’t understand how it achieves this. Can somebody explain this to me?  \nIf you fetch an item and then update it, there may be an update by another thread between those two steps. If you update an item first and then fetch it, there may be another update in-between and you will get back a different item than what you updated.  \nDoing it “atomically” means you are guaranteed that you are getting back the exact same item you are updating – i.e. no other operation can happen in between.  \nfindAndModify returns the document, update does not.  \nIf I understood Dwight Merriman (one of the original authors of mongoDB) correctly, using update to modify a single document i.e.(“multi”:false} is also atomic. Currently, it should also be faster than doing the equivalent update using findAndModify.  \nOne useful class of use cases is counters and similar cases. For example, take a look at this code (one of the MongoDB tests): find\\_and\\_modify4.js.  \nThus, with findAndModify you increment the counter and get its incremented value in one step. Compare: if you (A) perform this operation in two steps and somebody else (B) does the same operation between your steps then A and B may get the same last counter value instead of two different (just one example of possible issues)."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Getting an error, “Error: couldn’t connect to server 127.0.0.1 shell/mongo.js” & when trying to run mongodb on mac osx lion",
      "explanation": "I am using Mac OS X Lion and i just did a fresh install of MongoDB using macports and when i try to run mongodb for the first time i get the following error  \nMongoDB shell version: 2.0.5  \nconnecting to: test  \nFri Jun  1 11:20:33 Error: couldn’t connect to server 127.0.0.1 shell/mongo.js:84  \nexception: connect failed  \ncan anybody please help with this? thanks  \nwhen i run mongod i get:\n\n    Skip code block\n    hisham-agil:~ hisham$ mongod\n    mongod –help for help and startup options\n    Fri Jun  1 11:24:47 [initandlisten] MongoDB starting : pid=53452 port=27017 dbpath=/data/db/ 64-bit host=hisham-agil.local\n    Fri Jun  1 11:24:47 [initandlisten] db version v2.0.5, pdfile version 4.5\n    Fri Jun  1 11:24:47 [initandlisten] git version: nogitversion\n    Fri Jun  1 11:24:47 [initandlisten] build info: Darwin gamma.local 11.3.0 Darwin Kernel Version 11.3.0: Thu Jan 12 18:48:32 PST 2012; root:xnu-1699.24.23~1/RELEASE_I386 i386 BOOST_LIB_VERSION=1_49\n    Fri Jun  1 11:24:47 [initandlisten] options: {}\n    Fri Jun  1 11:24:47 [initandlisten] exception in initAndListen: 10296 dbpath (/data/db/) doesnot exist, terminating\n    Fri Jun  1 11:24:47 dbexit:\n    Fri Jun  1 11:24:47 [initandlisten] shutdown: going to close listening sockets…\n    Fri Jun  1 11:24:47 [initandlisten] shutdown: going to flush diaglog…\n    Fri Jun  1 11:24:47 [initandlisten] shutdown: going to close sockets…\n    Fri Jun  1 11:24:47 [initandlisten] shutdown: waiting for fs preallocator…\n    Fri Jun  1 11:24:47 [initandlisten] shutdown: lock for final commit…\n    Fri Jun  1 11:24:47 [initandlisten] shutdown: final commit…\n    Fri Jun  1 11:24:47 [initandlisten] shutdown: closing all files…\n    Fri Jun  1 11:24:47 [initandlisten] closeAllFiles() finished\n    Fri Jun  1 11:24:47 dbexit: really exiting now\n\n  \nYou’re running the mongo client without starting the server first. Try running mongod first.  \nYou’ll have to specify where the mongo “workspace” is using the –dbpath switch, such as mongod –dbpath /mongo/db. The specified folder should exist.  \nBy default mongod will try to use the directory /data/db for the database files.  \nIn your case that directory does not exist.  \nBefore starting mongod you should create those directories and make sure they are writable by the same user that is running mongod process."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB – how to query for a nested item inside a collection?",
      "explanation": "I have some data that looks like this:\n\n    Skip code block\n    [\n    {\n    “_id” : ObjectId(“4e2f2af16f1e7e4c2000000a”),\n    “advertisers” : [\n    {\n    “created_at” : ISODate(“2011-07-26T21:02:19Z”),\n    “category” : “Infinity Pro Spin Air Brush”,\n    “updated_at” : ISODate(“2011-07-26T21:02:19Z”),\n    “lowered_name” : “conair”,\n    “twitter_name” : “”,\n    “facebook_page_url” : “”,\n    “website_url” : “”,\n    “user_ids” : [ ],\n    “blog_url” : “”,\n    },\n\n  \nand I was thinking that a query like this would give the id of the advertiser:\n\n    var start  = new Date(2011, 1, 1);\n    > var end  = new Date(2011, 12, 12);\n    > db.agencies.find( { “created_at” : {$gte : start , $lt : end} } , { _id : 1 , program_ids: 1 , advertisers { name : 1 }  } ).limit(1).toArray();\n\n  \nBut my query didn’t work. Any idea how I can add the fields inside the nested elements to my list of fields I want to get?  \nThanks!  \nUse dot notation (e.g. advertisers.name) to query and retrieve fields from nested objects:\n\n    db.agencies.find( { “advertisers.created_at” : {$gte : start , $lt : end} } , { _id : 1 , program_ids: 1 , “advertisers.name”: 1 }  } ).limit(1).toArray();\n\n  \nReference: Retrieving a Subset of Fields and Dot Notation"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How do I abort a query in mongo javascript shell",
      "explanation": "Executing queries in the Javascript shell of Mongo feels a lot like executing commands right in the bash shell. Because of this, my fingers keep trying to Ctrl+C my way out of a command that I want to “abort” and land back at the command prompt. Unfortunately, in the mongo shell at 2.0.4, when I Ctrl+C, it drops me out of the javascript shell, rather than just aborting the command and giving me the command prompt.  \nIs there a way in the javascript shell to abandon the current command without executing it and land back at the command prompt?  \nWhat I’ve always naturally done is type a ( and then immediately hit enter three times. The shell will assume you are just hitting enter to break up your command, but if you hit enter 2 more times it will exit back to the shell. It’s wonky, but it works."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Where does mongodb stand in the CAP theorem?",
      "explanation": "Everywhere I look, I see that MongoDB is CP. But when I dig in I see it is eventually consistent. Is it CP when you use safe=true? If so, does that mean that when I write with safe=true, all replicas will be updated before getting the result?  \nMongoDB is strongly consistent by default – if you do a write and then do a read, assuming the write was successful you will always be able to read the result of the write you just read. This is because MongoDB is a single-master system and all reads go to the primary by default. If you optionally enable reading from the secondaries then MongoDB becomes eventually consistent where it’s possible to read out-of-date results.  \nMongoDB also gets high-availability through automatic failover in replica sets: https://www.mongodb.org/display/DOCS/Replica+Sets  \nThis should help answer the question, along with other NoSQL and other persistent storage systems."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB: BIllions of documents in a collection",
      "explanation": "I need to load 6.6 billion bigrams into a collection but I can’t find any information on the best way to do this.  \nLoading that many documents onto a single primary key index would take forever but as far as I’m aware mongo doesn’t support the equivalent of partitioning?  \nWould sharding help? Should I try and split the data set over many collections and build that logic into my application?  \nIt’s hard to say what the optimal bulk insert is — this party depends on the size of the objects you’re inserting and other immeasurable factors. You could try a few ranges and see what gives you the best performance. As an alternative, some people like using mongoimport, which is pretty fast, but your import data needs to be json or csv. There’s obviously mongodrestore, if the data is in BSON format.  \nMongo can easily handle billions of documents and can have billions of documents in the one collection but remember that the maximum document size is 16mb. There are many folk with billions of documents in MongoDB and there’s lots of discussions about it on the MongoDB Google User Group. Here’s a document on using a large number of collections that you may like to read, if you change your mind and want to have multiple collections instead. The more collections you have, the more indexes you will have also, which probably isn’t what you want.  \nHere’s a presentation from Craigslist on inserting billions of documents into MongoDB and the guy’s blogpost.  \nIt does look like sharding would be a good solution for you but typically sharding is used for scaling across multiple servers and a lot of folk do it because they want to scale their writes or they are unable to keep their working set (data and indexes) in RAM. It is perfectly reasonable to start off with a single server and then move to a shard or replica-set as your data grows or you need extra redundancy and resilience.  \nHowever, there are other users use multiple mongods to get around locking limits of a single mongod with lots of writes. It’s obvious but still worth saying but a multi-mongod setup is more complex to manage than a single server. If your IO or cpu isn’t maxed out here, your working set is smaller than RAM and your data is easy to keep balanced (pretty randomly distributed), you should see improvement (with sharding on a single server). As a FYI, there is potential for memory and IO contention. With 2.2 having improved concurrency with db locking, I suspect that there will be much less of a reason for such a deployment.  \nYou need to plan your move to sharding properly, i.e. think carefully about choosing your shard key. If you go this way then it’s best to pre-split and turn off the balancer. It will be counter-productive to be moving data around to keep things balanced which means you will need to decide up front how to split it. Additionally, it is sometimes important to design your documents with the idea that some field will be useful for sharding on, or as a primary key.  \nHere’s some good links –  \n1\\. Choosing a Shard Key  \n2\\. Blog post on shard keys  \n3\\. Overview presentation on sharding  \n4\\. Presentation on Sharding Best Practices  \nYou can absolutely shard data in MongoDB (which partitions across N servers on the shard key). In fact, that’s one of it’s core strengths. There is no need to do that in your application.  \nFor most use cases, I would strongly recommend doing that for 6.6 billion documents. In my experience, MongoDB performs better with a number of mid-range servers rather than one large one."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Why are key names stored in the document in MongodDB",
      "explanation": "I’m curious about this quote from Kyle Banker’s MongoDB In Action:  \nIts important to consider the length of the key names you choose, since key names are stored in the documents themselves. This contrasts with an RDBMS, where column names are always kept separate from the rows they refer to. So when using BSON, if you can live with dob in place of date\\_of\\_birth as a key name, youll save 10 bytes per document. That may not sound like much, but once you have a billion such documents, youll have saved nearly 10 GB of storage space just by using a shorter key name. This doesnt mean you should go to unreasonable lengths to ensure small key names; be sensible. But if you expect massive amounts of data, economizing on key names will save space.  \nI am interested in the reason why this is not optimized on the database server side. Would a in-memory lookup table with all key names in the collection be too much of a performance penalty that is not worth the potential space savings?  \nWhat you are referring to is often called “key compression”\\*. There are several reasons why it hasn’t been implemented:  \n1\\. If you want it done, you can currently do it at the Application/ORM/ODM level quite easily.  \n2\\. It’s not necessarily a performance\\*\\* advantage in all cases think collections with lots of key names, and/or key names that vary wildly between documents.  \n3\\. It might not provide a measurable performance\\*\\* advantage at all until you have millions of documents.  \n4\\. If the server does it, the full key names still have to be transmitted over the network.  \n5\\. If compressed key names are transmitted over the network, then readabilityreally suffers using the javascript console.  \n6\\. Compressing the entire JSON document might offer an even better performance advantage.  \nLike all features, there’s a cost benefit analysis for implementing it, and (at least so far) other features have offered more “bang for the buck”.  \nFull document compression is being considered for a future MongoDB version.  \n\\* An in-memory lookup table for key names is basically a special case of LZW style compression that’s more or less what most compression algorithms do.  \n\\*\\* Compression provides both a space advantage and a performance advantage. Smaller documents means that more documents can be read per IO, which means that in a system with fixed IO, more documents per second can be read.  \nI believe one of the original reasons behind storing the key names with the documents is to allow a more easily scalable schema-less database. Each document is self-contained to a greater extent, in that if you move the document to another server (for example, via replication or sharding) you can index the contents of the document without having to reference separate or centralized metadata such as a mapping of key names to more compact key IDs.  \nSince there is no enforced schema for a MongoDB collection, the field names can potentially be different for every document in the same collection. In a sharded environment, inserts to each shard are (intentionally) independent so at a document level the raw data could end up differing unless the key mapping was able to be consistent per shard.  \nDepending on your use case, the key names may or may not consume a significant amount of space relative to the accompanying data. You could always workaround the storage concern from the application / ODM implementation by mapping YourFriendlyKeyNames to shorter DB key equivalents.  \nThere is an open MongoDB Jira issue and some further discussion to have the server tokenize field names, which you can vote on to help prioritize including this feature in a future release.  \nMongoDB’s current design goals include performance with dynamic schemas, replication & high availability, auto-sharding, and in-place updates .. with one potential tradeoff being some extra disk usage."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How can I copy collection to another database in MongoDB?",
      "explanation": "Is there some simple way to fulfill this?  \nAt the moment there is no command in MongoDB that would do this. Please note the JIRA ticket with related feature request: https://jira.mongodb.org/browse/SERVER-732.  \nYou could do something like:  \ndb..find().forEach(function(d){ db.getSiblingDB(‘’)\\[‘’\\].insert(d); });  \nPlease note that with this, the two databases would need to share the same mongod for this to work.  \nBesides this, you can do a mongodump of a collection from one database and then mongorestore the collection to the other database.  \nThe best way is to do a mongodump then mongorestore.  \nYou can select the collection via:  \nmongodump -d some\\_database -c some\\_collection  \n\\[Optionally, zip the dump (zip some\\_database.zip some\\_database/\\* -r) and scp it elsewhere\\]  \nThen restore it:  \nmongorestore -d some\\_other\\_db -c some\\_or\\_other\\_collection dump/some\\_collection.bson  \nExisting data in some\\_or\\_other\\_collection will be preserved. That way you can “append” a collection from one database to another.  \nPrior to version 2.4.3, you will also need to add back your indexes after you copy over your data. Starting with 2.4.3, this process is automatic, and you can disable it with –noIndexRestore.  \nI would abuse the connect function in mongo cli mongo doc. so that means you can start one or more connection. if you want to copy customer collection from test to test2 in same server. first you start mongo shell  \nuse test  \nvar db2 = connect(‘localhost:27017/test2’)  \ndo a normal find and copy the first 20 record to test2.  \ndb.customer.find().limit(20).forEach(function(p) { db2.customer.insert(p); });  \nor filter by some criteria  \ndb.customer.find({“active”: 1}).forEach(function(p) { db2.customer.insert(p); });  \njust change the localhost to IP or hostname to connect to remote server. I use this to copy test data to a test database for testing.  \nActually, there is a command to move a collection from one database to another. It’s just not called “move” or “copy”.  \nTo copy a collection, you can clone it on the same db, then move the clone.  \nTo clone:\n\n    > use db1\n    > db.source_collection.find().forEach( function(x){db.collection_copy.insert(x)} );\n    To move:\n    > use admin\n    switched to db admin\n    > db.runCommand({renameCollection: ‘db1.source_collection, to: ‘db2.target_collection’}) // who’d think rename could move?\n    The other answers are better for copying the collection, but this is especially useful if you’re looking to move it.\n    I’d usually do:\n\n    use sourcedatabase;\n    var docs=db.sourcetable.find();\n    use targetdatabase;\n    docs.forEach(function(doc) { db.targettable.insert(doc); });"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "In Mongo what is the difference between sharding and replication?",
      "explanation": "Replication seems to be a lot simpler than sharding, unless I am missing the benefits of what sharding is actually trying to achieve. Don’t they both provide horizontal scaling?  \nIn the context of scaling MongoDB:  \nreplicationcreates additional copies of the data and allows for automatic failover to another node. 1. Replication may help with horizontal scaling of reads if you are OK to read data that potentially isn’t the latest.  \n2\\. sharding allows for horizontal scaling of data writes by partitioning data across multiple servers using a shard key. It’s important to choose a good shard key. For example, a poor choice of shard key could lead to “hot spots” of data only being written on a single shard.  \nA sharded environment does add more complexity because MongoDB now has to manage distributing data and requests between shards — additional configuration and routing processes are added to manage those aspects.  \nReplication and sharding are typically combined to created a sharded cluster where each shard is supported by a replica set.  \nFrom a client application point of view you also have some control in relation to the replication/sharding interaction, in particular:  \n1\\. Read preferences  \n2\\. Write concerns  \nReplication is a mostly traditional master/slave setup, data is synced to backup members and if the primary fails one of them can take its place. It is a reasonably simple tool. It’s primarily meant for redundancy, although you can scale reads by adding replica set members. That’s a little complicated, but works very well for some apps.  \nSharding sits on top of replication, usually. “Shards” in MongoDB are just replica sets with something called a “router” in front of them. Your application will connect to the router, issue queries, and it will decide which replica set (shard) to forward things on to. It’s significantly more complex than a single replica set because you have the router and config servers to deal with (these keep track of what data is stored where).  \nIf you want to scale Mongo horizontally, you’d shard. 10gen likes to call the router/config server setup auto-sharding. It’s possible to do a more ghetto form of sharding where you have the app decide which DB to write to as well."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "What does Mongo sort on when no sort order is specified?",
      "explanation": "When we run a Mongo find() query without any sort order specified, what does the database internally use to sort the results?  \nAccording to the documentation on the mongo website:  \nWhen executing a find() with no parameters, the database returns objects in forward natural order.  \nFor standard tables, natural order is not particularly useful because, although the order is often close to insertion order, it is not guaranteed to be. However, for Capped Collections, natural order is guaranteed to be the insertion order. This can be very useful.  \nHowever for standard collections (non capped collections), what field is used to sort the results? Is it the **\\_id** field or something else?  \n**Edit:**  \nBasically, I guess what I am trying to get at is that if I execute the following search query:  \ndb.collection.find({“x”:y}).skip(10000).limit(1000);  \nAt two different points in time: **t1** and **t2**, will I get different result sets:  \n1\\. When there have been no additional writes between t1 & t2?  \n2\\. When there have been new writes between t1 & t2?  \n3\\. There are new indexes that have been added between t1 & t2?  \nI have run some tests on a temp database and the results I have gotten are the same (**Yes**) for all the 3 cases – but I wanted to be sure and I am certain that my test cases weren’t very thorough.  \nBy definition the sort **defaults to undefined**, and so is the return order of documents. If there is no query then it will use the natural order. The results are returned in the **order they are found,** which may coincide with insertion order (but isn’t guaranteed to be) or the order of the index(es) used.  \nSome examples that will affect storage (natural) order:  \n1\\. if documents are updated and don’t fit in their currently allocated space, they will be moved  \nnew documents may be inserted in available gaps created by deleted or moved documents  \n2\\. If an index is used, docs will be returned in the order they are found. If more than one index is used then the order depends internally on which index first identified the document during the de-duplication process.  \nIf you **want a specific order** then you **must** include a sort with your query.  \nThe exception noted for capped collections’ natural order is because documents can’t move and are stored in insertion order. The ordering is part of the capped collection feature that ensures the oldest documents “age out” first. Additionally, documents cannot be deleted or moved in a capped collection (see Usage and Restrictions for more info)."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to reclaiming deleted space without \\`db.repairDatabase()\\`?",
      "explanation": "I want to shrink data files size by reclaiming deleted space, but I can’t run db.repairDatabase(), because free disk space is not enough.  \nThe original answer to this question is here: Reducing MongoDB database file size  \nThere really is nothing outside of repair that will reclaim space. The compact should allow you to go much longer on the existing space. Otherwise, you will have to migrate to a bigger drive.  \nOne way to do this is to use an off-line secondary from your Replica Set. This should give you a whole maintenance window to migrate, repair, move back and bring back up.  \nIf you are not running a Replica Set, it’s time to look at doing just that.  \nYou could run the compact command on a single collection, or one by one in all the collections you want to shrink.  \nhttps://www.mongodb.org/display/DOCS/Compact+Command  \ndb.runCommand( { compact : ‘mycollectionname’ } )  \nAs noted in comments, I was mistaken, compact does not actually reclaim disk space, it only defragments and rebuilds collection indexes.  \nInstead though, you could use “–repairpath” option if you have another drive available which has available freespace.  \nFor example:  \nmongod –dbpath /data/db –repair –repairpath /data/db0  \nShown here: https://docs.mongodb.org/manual/tutorial/recover-data-following-unexpected-shutdown/"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "creating a different database for each collection in MongoDB 2.2",
      "explanation": "MongoDB 2.2 has a write lock per database as opposed to a global write lock on the server in previous versions. So would it be ok if i store each collection in a separate database to effectively have a write lock per collection.(This will make it look like MyISAM’s table level locking). Is this approach faulty?  \nThere’s a key limitation to the locking and that is the local database. That database includes a theoplog collection which is used for replication.  \nIf you’re running in production, you should be running with Replica Sets. If you’re running with Replica Sets, you need to be aware of the write lock effect on that database.  \nBreaking out your 10 collections into 10 DBs is useless if they all block waiting for the oplog.  \nBefore taking a large step to re-write, please ensure that the oplog will not cause issues.  \nAlso, be aware that MongoDB implements DB-level security. If you’re using any security features, you are now creating more DBs to secure."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to stop mongo DB in one command",
      "explanation": "I need to be able to start/stop MongoDB on the cli. It is quite simple to start:  \n./mongod  \nBut to stop mongo DB, I need to run open mongo shell first and then type two commands:  \n$ ./mongo  \nuse admin  \ndb.shutdownServer()  \nSo I don’t know how to stop mongo DB in one line. Any help?  \nStarting and Stopping MongoDB is covered in the MongoDB manual. This section of the manual explains the various options of stopping MongoDB through the shell, cli, drivers etc. It also details the risks of incorrectly stopping MongoDB (such as data corruption) and talks about the different kill signals.  \nAdditionally, if you have installed MongoDB using a package manager for Ubuntu or Debian then you can stop mongod as follows:\n\n1\\. Upstart:sudo service mongodb stop  \n2\\. Sysvinit:sudo /etc/init.d/mongodb stop\n\nOr on Red Hat based systems:  \n1\\. service mongod stop  \nOr on Windows if you have installed as a service named MongoDB:  \n1\\. net stop MongoDB  \nTo learn more about the problems of an unclean shutdown, how to best avoid such a scenario and what to do in the event of an unclean shutdown, please see: Recover Data after an Unexpected Shutdown.  \nIf you literally want a one line equivalent to the commands in your original question, you could alias:  \nmongo –eval “db.getSiblingDB(‘admin’).shutdownServer()”  \nMark’s answer on starting and stopping MongoDB via services is the more typical (and recommended) administrative approach.\n\n    mongod –dbpath /path/to/your/db –shutdown\n\n  \nMore info at official: https://docs.mongodb.org/manual/tutorial/manage-mongodb-processes/"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB select count(distinct x) on an indexed column – count unique results for large data sets",
      "explanation": "I have gone through several articles and examples, and have yet to find an efficient way to do this SQL query in MongoDB (where there are millions of  documents)  \n**First attempt**\n\n    (e.g. from this almost duplicate question – Mongo equivalent of SQL’s SELECT DISTINCT?)\n    db.myCollection.distinct(“myIndexedNonUniqueField”).length\n    Obviously I got this error as my dataset is huge\n    Thu Aug 02 12:55:24 uncaught exception: distinct failed: {\n    “errmsg” : “exception: distinct too big, 16mb cap”,\n    “code” : 10044,\n    “ok” : 0\n    }\n\n  \n**Second attempt**  \nI decided to try and do a group  \ndb.myCollection.group({key: {myIndexedNonUniqueField: 1},  \ninitial: {count: 0},  \nreduce: function (obj, prev) { prev.count++;} } );  \nBut I got this error message instead:  \nexception: group() can’t handle more than 20000 unique keys  \n**Third attempt**\n\n  \nI haven’t tried yet but there are several suggestions that involve mapReduce  \ne.g.  \n1\\. this onehow to do distinct and group in mongodb? (not accepted, answer author / OP didn’t test it)  \n2\\. this oneMongoDB group by Functionalities (seems similar to Second Attempt)  \n3\\. this onehttps://blog.emmettshear.com/post/2010/02/12/Counting-Uniques-With-MongoDB  \n4\\. this one https://groups.google.com/forum/?fromgroups#!topic/mongodb-user/trDn3jJjqtE  \n5\\. this onehttps://cookbook.mongodb.org/patterns/unique\\_items\\_map\\_reduce/\n\n**Also**  \nIt seems there is a pull request on GitHub fixing the .distinct method to mention it should only return a count, but it’s still open: https://github.com/mongodb/mongo/pull/34  \nBut at this point I thought it’s worth to ask here, what is the latest on the subject? Should I move to SQL or another NoSQL DB for distinct counts? or is there an efficient way?  \n**Update:**  \nThis comment on the MongoDB official docs is not encouraging, is this accurate?\n\nhttps://www.mongodb.org/display/DOCS/Aggregation#comment-430445808\n\n**Update2:**  \nSeems the new Aggregation Framework answers the above comment… (MongoDB 2.1/2.2 and above, development preview available, not for production)  \nhttps://docs.mongodb.org/manual/applications/aggregation/\n\n1) The easiest way to do this is via the aggregation framework. This takes two “$group” commands: the first one groups by distinct values, the second one counts all of the distinct values\n\n    Skip code block\n    pipeline = [\n    { $group: { _id: “$myIndexedNonUniqueField”}  },\n    { $group: { _id: 1, count: { $sum: 1 } } }\n    ];\n    //\n    // Run the aggregation command\n    //\n    R = db.runCommand(\n    {\n    “aggregate”: “myCollection” ,\n    “pipeline”: pipeline\n    }\n    );\n    printjson(R);\n\n  \n2) If you want to do this with Map/Reduce you can. This is also a two-phase process: in the first phase we build a new collection with a list of every distinct value for the key. In the second we do a count() on the new collection.\n\n    Skip code block\n    var SOURCE = db.myCollection;\n    var DEST = db.distinct\n    DEST.drop();\n    map = function() {\n    emit( this.myIndexedNonUniqueField , {count: 1});\n    }\n    reduce = function(key, values) {\n    var count = 0;\n    values.forEach(function(v) {\n    count += v[‘count’];        // count each distinct value for lagniappe\n    });\n    return {count: count};\n    };\n    //\n    // run map/reduce\n    //\n    res = SOURCE.mapReduce( map, reduce,\n    { out: ‘distinct’,\n    verbose: true\n    }\n    );\n    print( “distinct count= ” + res.counts.output );\n    print( “distinct count=”, DEST.count() );\n\n  \nNote that you cannot return the result of the map/reduce inline, because that will potentially overrun the 16MB document size limit. You can save the calculation in a collection and then count() the size of the collection, or you can get the number of results from the return value of mapReduce().\n\n    db.myCollection.aggregate(\n    {$group : {_id : “$myIndexedNonUniqueField”} },\n    {$group: {_id:1, count: {$sum : 1 }}});\n    straight to result:\n    db.myCollection.aggregate(\n    {$group : {_id : “$myIndexedNonUniqueField”} },\n    {$group: {_id:1, count: {$sum : 1 }}})\n    .result[0].count;"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB: Find Subdocument in Array Matching Parameters",
      "explanation": "In MongoDB I would like to find a document based on the values of a subdocument meeting certain parameters. Specifically I have a document structured like this:\n\n    Skip code block\n    {\n    name: “test”,\n    data: [{\n    name: “test1”,\n    start: 0,\n    end: 2\n    },\n    {\n    name: “test2”,\n    start: 15\n    end: 18\n    }]\n    }\n\n  \nHow can I tell MongoDB to only return my document if the start time for a data subdocument is less than 5 and the end time for the same subdocument is greater than 5? Currently, if I do\n\n    db.foo.findOne({\n    ‘data.start’: { $lte: 5 },\n    ‘data.end’: { $gte: 5 }\n    })\n    it will return my document always because 5 is greater than 0 and less than 18. How can I tell MongoDB to only return my document if 5 (or whatever value) is greater than 0 and less than 2 OR greater than 15 and less than 18?\n    You want to use $elemMatch.\n    db.foo.findOne({ data: { $elemMatch : {\n    start: { $lte: 5 },\n    end: { $gte: 5 }\n    }}\n    })"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to efficiently perform “distinct” with multiple keys?",
      "explanation": "For example, there is a collection like this:\n\n    {market: ‘SH’, code: ‘000001’, date: ‘2012-01-01’, price: 1000}\n    {market: ‘SZ’, code: ‘000001’, date: ‘2012-01-01’, price: 1000}\n    {market: ‘SH’, code: ‘000001’, date: ‘2012-01-02’, price: 1000}\n    {market: ‘SZ’, code: ‘000001’, date: ‘2012-01-02’, price: 1000}\n    {market: ‘SH’, code: ‘000002’, date: ‘2012-01-03’,price: 1000}\n\n  \n…  \nThis collection contains tens of millions documents.  \nI want to call distinct with two keys:\n\n    collection.distinct(‘market’, ‘code’);\n\n  \nand get result :\n\n    [{market: ‘SH’, code:’000001'}, {market: ‘SZ’, code:’000001'}, {market: ‘SH’, code:’000002'}]\n    \n\nAs native distinct command accept only one key, I try to implement it by using map-reduce. But map-reduce is far too slow to native distinct. In my one-key distinct test, map-reduce spend about ten times longer than native distinct.  \nIs there a efficient way to implement multikey distinct?  \nIf you are willing to wait for the upcoming 2.2 release of MongoDB, you can run this query efficiently using the aggregation framework:\n\n    collection = db.tb;\n    result = collection.aggregate(\n    [\n    {“$group”: { “_id”: { market: “$market”, code: “$code” } } }\n    ]\n    );\n    printjson(result);\n\n  \nOn a million-record collection on my test machine, this ran in 4 seconds, while the map/reduce version took over a minute."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB: Trade-offs of dropping a collection vs. removing all of its documents",
      "explanation": "What are the trade-offs of dropping a MongoDB collection vs. removing all of its documents (assuming the collection will be re-created immediately)?  \nIf you go through a remove all the documents from a collection, then you’ll be doing a lot more work (freeing the document’s storage, clearing the index entries that point to the document, and so on). If you instead just drop the collection, it’ll just be reclaiming the extents that the collection and its indexes use.  \nOne other difference is that dropping the collection will also remove the collection’s indexes.  \nA benefit of simply dropping a collection is that it is much faster than removing all of a collection’s documents. If your collection will be “re-created immediately” anyway (assuming that includes index re-creation), then this is probably the most-attractive option.  \nThe authors of the book MongoDB: The Definitive Guide (Kristina Chodorow and Michael Dirolf) ran an experiment where they provided a Python script which timed a drop vs. a remove of 1000000 records. The results came in at 46.08 seconds for the remove and .01 seconds for the drop. Now while the exact times may differ based-on hardware and other factors, it nonetheless illustrates the point that the drop is significantly faster.  \nreference: Chodorow K., Dirolf M. (2010). MongoDB: The Definitive Guide. O’Reilly Media, Inc. Sebastapol, CA., pp.25\n\n**Related Page: [MongoDB Vs MySQL - Which Is A Better Database?](../../mongodb-vs-mysql)**"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB : find value in Array with multiple criteria",
      "explanation": "I have the following documents:\n\n    {_id : 1, numbers : [-1000, 1000]}\n    {_id : 2, numbers : [5]}\n\n  \nI’m trying to get a query that will find a document that has a value in the numbers array that is between -10 and 10 (in this case, \\_id : 2). However, when I try the following:\n\n    db.foo.find({numbers : $and : [{$gt : -10},{$lt : 10}]})\n\n  \nit returns all documents. Is this possible to do without map-reduce? Thanks, -JWW  \nYou can use $elemMatch to check if an element in an array matches a specified match expression.  \nIn this case, you can use it to get a document whose numbers array has an element that is between -10 and 10:\n\n    db.foo.find( { numbers : { $elemMatch : { $gt : -10 , $lt : 10 } } } );\n    \n\nThis will just return the \\_id : 2 document."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB 2.1 Aggregate Framework Sum of Array Elements matching a name",
      "explanation": "This is a question about the best way to add up a series of data in an array where I have to match another element. I’m trying to use the 2.2 Aggregation framework and it’s possible I can do this with a simple group.  \nSo for a given set of documents I’m trying to get an output like this;\n\n    Skip code block\n    {\n    “result” : [\n    {\n    “_id” : null,\n    “numberOf”: 2,\n    “Sales” : 468000,\n    “profit” : 246246,\n    }\n    ],\n    “ok” : 1\n    }\n    Now, I originally had a list of documents, containing values assigned to named properties, eg;\n    Skip code block\n    [\n    {\n    _id : 1,\n    finance: {\n    sales: 234000,\n    profit: 123123,\n    }\n    }\n    ,\n    {\n    _id : 2,\n    finance: {\n    sales: 234000,\n    profit: 123123,\n    }\n    }\n    ]\n\n  \nThis was easy enough to add up, but the structure didn’t work for other reasons. For instance, there are may other columns like “finance” and I want to be able to index them without creating thousands of indexes, so I need to convert to a structure like this;\n\n    Skip code block\n    [\n    {\n    _id : 1,\n    finance: [\n    {\n    “k”: “sales”,\n    “v”: {\n    “description”:”sales over the year”,\n    v: 234000,\n    }\n    },\n    {\n    “k”: “profit”,\n    “v”: {\n    “description”:”money made from sales”,\n    v: 123123,\n    }\n    }\n    ]\n    }\n    ,\n    {\n    _id : 2,\n    finance: [\n    {\n    “k”: “sales”,\n    “v”: {\n    “description”:”sales over the year”,\n    v: 234000,\n    }\n    },\n    {\n    “k”: “profit”,\n    “v”: {\n    “description”: “money made from sales”,\n    v: 123123,\n    }\n    }\n    ]\n    }\n    ]\n\n  \nI can index finance.k if I want, but then I’m struggling to build an aggregate query to add up all the numbers matching a particular key. This was the reason I originally went for named properties, but this really needs to work in a situation whereby there are thousands of “k” labels.  \nDoes anyone know how to build an aggregate query for this using the new framework? I’ve tried this;\n\n    Skip code block\n    db.projects.aggregate([\n    {\n    $match: {\n    // QUERY\n    $and: [\n    // main query\n    {},\n    ]\n    }\n    },\n    {\n    $group: {\n    _id: null,\n    “numberOf”: { $sum: 1 },\n    “sales”:    { $sum: “$finance.v.v” },\n    “profit”:   { $sum: “$finance.v.v” },\n    }\n    },\n    ])\n    but I get;\n    {\n    “errmsg” : “exception: can’t convert from BSON type Array to double”,\n    “code” : 16005,\n    “ok” : 0\n    }\n    ** For extra kudos, I’ll need to be able to do this in a MapReduce query as well.\n    You can use the aggregation framework to get sales and profit and any other value you may be storing in your key/value pair representation.\n    For your example data:\n    Skip code block\n    var pipeline = [\n    {\n    “$unwind” : “$finance”\n    },\n    {\n    “$group” : {\n    “_id” : “$finance.k”,\n    “numberOf” : {\n    “$sum” : 1\n    },\n    “total” : {\n    “$sum” : “$finance.v.v”\n    }\n    }\n    }\n    ]\n    R = db.tb.aggregate( pipeline );\n    printjson(R);\n    {\n    “result” : [\n    {\n    “_id” : “profit”,\n    “numberOf” : 2,\n    “total” : 246246\n    },\n    {\n    “_id” : “sales”,\n    “numberOf” : 2,\n    “total” : 468000\n    }\n    ],\n    “ok” : 1\n    }\n\n  \nIf you have additional k/v pairs then you can add a match which only passes through k values in \\[“sales”,”profit”\\].  \nYou will have to use ‘$unwind” to break out the values in the array, which will mean that you can’t get the sum of the sales and the profit in a single aggregation command. Given that, the query itself is easy:\n\n    Skip code block\n    var pipeline = [\n    {“$unwind”: “$finance” } ,\n    {“$match”: {“finance.k”: “sales”} },\n    { $group:\n    { _id: null,\n    numberOf: { “$sum”: 1 },\n    sales: {“$sum”: “$finance.v.v” }\n    }\n    }\n    ];\n    R = db.tb.aggregate( pipeline );\n    printjson(R);\n    {\n    “result” : [\n    {\n    “_id” : null,\n    “numberOf” : 2,\n    “sales” : 236340\n    }\n    ],\n    “ok” : 1\n    }\n    You can run a similar query for profit, just substitute “profit” for “sales” in the “$match” operator.\n    Oh, and here’s the map/reduce example:\n    Skip code block\n    map = function() {\n    var ret = { sales: 0.0 , profit: 0.0, count: 1 };\n    // iterate over ‘finance[]’ array\n    this.finance.forEach( function (i) {\n    if ( i.k == “sales” ) ret.sales =  i.v.v ;\n    if ( i.k == “profit” ) ret.profit =  i.v.v ;\n    } );\n    emit( 1, ret );\n    }\n    reduce = function(key, values) {\n    var ret = { sales: 0.0 , profit: 0.0, count: 0 };\n    values.forEach(function(v) {\n    ret.sales += v.sales;\n    ret.profit += v.profit;\n    ret.count += v.count;\n    });\n    return ret;\n    };\n    //\n    // run map/reduce\n    //\n    res = SOURCE.mapReduce( map, reduce );"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Select \\* group by in mongo aggregation",
      "explanation": "I am trying to do something that I think is quite simple. Suppose I have a series of records in mongo that have a common key, and variable number of attributes. I want to select all attributes and group by name across the records. For example  \n  \nTried\n\n    DB.data.aggregate({ $group : { _id : “$Name” } })\n\n  \nUnfortunately I get back all the names as records but not the union of all the possible attributes.  \nIf you want to combine the attributes, you’ll need to add those to the group. For example, using$addToSet to find the unique values of the x,y,z attributes grouped by each name:\n\n    db.data.aggregate(\n    { $group : {\n\n    { Name: George, x: 5, y: 3 }\n    { Name: George, z: 9 }\n    { Name: Rob, x: 12, y: 2 }\n    I would like to produce a CSV that looks like this:\n    Name     X   Y   Z\n    George   5   3   9\n    Rob      12  2\n\n    _id : “$Name”,\n    x: { $addToSet: “$x” },\n    y: { $addToSet: “$y” },\n    z: { $addToSet: “$z” },\n    }}\n    )\n    Returns:\n    Skip code block\n    {\n    “result” : [\n    {\n    “_id” : “Rob”,\n    “x” : [\n    12\n    ],\n    “y” : [\n    2\n    ],\n    “z” : [ ]\n    },\n    {\n    “_id” : “George”,\n    “x” : [\n    5\n    ],\n    “y” : [\n    3\n    ],\n    “z” : [\n    9\n    ]\n    }\n    ],\n    “ok” : 1\n    }"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Creating custom Object ID in MongoDB",
      "explanation": "I am creating a service for which I will use MongoDB as a storage backend. The service will produce a hash of the user input and then see if that same hash (+ input) already exists in our dataset.\n\nThe hash will be unique yet random ( = non-incremental/sequential), so my question is:  \nIs it -legitimate- to use a random value for an Object ID? Example:  \n$object\\_id = new MongoId(HEX-OF-96BIT-HASH);\n\nOr will MongoDB treat the ObjectID differently from other server-produced ones, since a “real” ObjectID also contains timestamps, machine\\_id, etc?  \nWhat are the pros and cons of using a ‘random’ value? I guess it would be statistically slower for the engine to update the index on inserts when the new \\_id’s are not in any way incremental – am I correct on that?  \nYes it is perfectly fine to use a random value for an object id, if some value is present in \\_id field of a document being stored, it is treated as objectId.  \nSince \\_id field is always indexed, and primary key, you need to make sure that different objectid is generated for each object. There are some guidelines to optimize user defined object ids :\n\nhttps://www.mongodb.org/display/DOCS/Optimizing+Object+IDs#OptimizingObjectIDs-Usethecollections%27naturalprimarykey%27intheidfield.\n\nWhether it is good or bad depends upon it’s uniqueness. Of course the ObjectId provided by MongoDB is quite unique so this is a good thing. So long as you can replicate that uniqueness then you should be fine.  \nThere are no inherent risks/performance loses by using your own ID. I guess using it in string form might use up more index/storage/querying power but there you are using it in MongoID (ObjectId) form which should preserve the strengths of not storing it in a simple string.  \nWhile any values, including hashes, can be used for the \\_id field, I would recommend against using random values for two reasons:  \n1\\. You may need to develop a collision-management strategy in the case you produce identical random values for two different objects. In the question, you imply that you’ll generate IDs using a some type of a hash algorithm. I would not consider these values “random” as they are based on the content you are digesting with the hash. The probability of a collision then is a function of the diversity of content and the hash algorithm. If you are using something like MD5 or SHA-1, I wouldn’t worry about the algorithm, just the content you are hashing. If you need to develop a collision-management strategy then you definitely should not use random or hash-based IDs as collision management in a clustered environment is complicated and requires additional queries.  \n2\\. Random values as well as hash values are purposefully meant to be dispersed on the number line. That (a) will require more of the B-tree index to be kept in memory at all times and (b) may cause variable insert performance due to B-tree rebalancing. MongoDB is optimized to handle ObjectIDs, which come in ascending order (with one second time granularity). You’re likely better off sticking with them."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "How to resolve error :dbpath (/data/db/) does not exist permanently in MongoDB",
      "explanation": "I have installed mongodb in my Ubuntu 10.04.  \nI know that when it comes to start the mongodb server with the command “mongod“,then it expects /data/db folder and it can be easily resolved by creating “/data/db/”. One more way is to provide your own path using mongod –dbpath “path”,when we intend to give our own custom path for db.  \nBut while going through https://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/link i found that there is a configuration file.  \nI made the following changes to it.\n\n    # mongodb.conf\n    dbpath=/EBS/Work/mongodb/data/db/\n    logpath=/EBS/Work/mongodb/mongodb.log\n    logappend=true\n\n  \nBut still when I try to start the server with “**mongod**” it throws the same **error i.e error :dbpath (/data/db/) does not exist** . I wanted to know that how can I permanently redirect my dbpath to my own custom folder cause everytime you don’t want to type the path using “mongod –dbpath path”.Rather we look to make some changes in configuration file.  \nAssuming you have followed the instructions to install a packaged version of MongoDB from 10gen, you should be starting and stopping mongod using service.  \nTo start mongod:  \nsudo service mongodb start  \nTo stop mongod:  \nsudo service mongodb stop  \nIf you use the service command to start and stop, it should be using the configuration file:/etc/mongodb.conf.  \n**Starting mongod from the command line**  \nIf you run mongod directly instead of using the service definition, you will also have to specify a configuration file as a command line parameter if you want one to be used:  \nmongod –config /etc/mongodb.conf  \nhere is how i solve this problem. followed their offcial Doc . HERE. long story short , I created a directory inside /srv and excuted the command mongod –dbpath /srv/mongodb/ RAN like a champ.\n\n    Skip code block\n    ~$ mongod\n    Tue Jun  3 20:27:39.564 [initandlisten] MongoDB starting : pid=5380 port=27017 dbpath=/srv/mongodb/ 64-bit host= -SVE1411EGXB\n    Tue Jun  3 20:27:39.564 [initandlisten] db version v2.4.10\n    Tue Jun  3 20:27:39.564 [initandlisten] git version: e3d78955d181e475345ebd60053a4738a4c5268a\n    Tue Jun  3 20:27:39.564 [initandlisten] build info: Linux ip-10-2-29-40 2.6.21.7-2.ec2.v1.2.fc8xen #1 SMP Fri Nov 20 17:48:28 EST 2009 x86_64 BOOST_LIB_VERSION=1_49\n    Tue Jun  3 20:27:39.564 [initandlisten] allocator: tcmalloc\n    Tue Jun  3 20:27:39.564 [initandlisten] options: { dbpath: “/srv/mongodb/” }\n    Tue Jun  3 20:27:39.565 [initandlisten] exception in initAndListen: 10296\n    *********************************************************************\n    ERROR: dbpath (/srv/mongodb/) does not exist.\n    Create this directory or give existing directory in –dbpath.\n    See https://dochub.mongodb.org/core/startingandstoppingmongo\n    *********************************************************************\n    , terminating\n    Tue Jun  3 20:27:39.565 dbexit:\n    Tue Jun  3 20:27:39.565 [initandlisten] shutdown: going to close listening sockets…\n    Tue Jun  3 20:27:39.565 [initandlisten] shutdown: going to flush diaglog…\n    Tue Jun  3 20:27:39.565 [initandlisten] shutdown: going to close sockets…\n    Tue Jun  3 20:27:39.565 [initandlisten] shutdown: waiting for fs preallocator…\n    Tue Jun  3 20:27:39.565 [initandlisten] shutdown: lock for final commit…\n    Tue Jun  3 20:27:39.565 [initandlisten] shutdown: final commit…\n    Tue Jun  3 20:27:39.565 [initandlisten] shutdown: closing all files…\n    Tue Jun  3 20:27:39.565 [initandlisten] closeAllFiles() finished\n    Tue Jun  3 20:27:39.565 dbexit: really exiting now\n    ~$ mongod –dbpath /srv/mongodb/\n    Tue Jun  3 20:27:55.616 [initandlisten] MongoDB starting : pid=5445 port=27017 dbpath=/srv/mongodb/ 64-bit host= -SVE1411EGXB\n    Tue Jun  3 20:27:55.616 [initandlisten] db version v2.4.10\n    Tue Jun  3 20:27:55.616 [initandlisten] git version: e3d78955d181e475345ebd60053a4738a4c5268a\n    Tue Jun  3 20:27:55.616 [initandlisten] build info: Linux ip-10-2-29-40 2.6.21.7-2.ec2.v1.2.fc8xen #1 SMP Fri Nov 20 17:48:28 EST 2009 x86_64 BOOST_LIB_VERSION=1_49\n    Tue Jun  3 20:27:55.616 [initandlisten] allocator: tcmalloc\n    Tue Jun  3 20:27:55.616 [initandlisten] options: { dbpath: “/srv/mongodb/” }\n    Tue Jun  3 20:27:55.617 [initandlisten] exception in initAndListen: 10296\n    ~$ sudo service mongodb start\n    start: Job is already running: mongodb\n    ~$ sudo service mongodb stop\n    mongodb stop/waiting\n    ~$ cd /srv/\n    ~$~$/srv$ ls\n    ~$ /srv$ mkdir mongodb\n    mkdir: cannot create directory mongodb: Permission denied\n    ~$ cd –  /srv\n    ~$ sudo chgrp   /srv\n    ~$ sudo chmod 775 /srv\n    ~$ cd /srv/\n    /srv$ ls\n    /srv$ mkdir mongodb\n    /srv$ ls mongodb\n    /srv$ cd\n    ~$ mongod –dbpath /srv/mongodb/\n    Tue Jun  3 20:40:57.457 [initandlisten] MongoDB starting : pid=6018 port=27017 dbpath=/srv/mongodb/ 64-bit host= -SVE1411EGXB\n    Tue Jun  3 20:40:57.457 [initandlisten] db version v2.4.10\n    Tue Jun  3 20:40:57.457 [initandlisten] git version: e3d78955d181e475345ebd60053a4738a4c5268a\n    Tue Jun  3 20:40:57.457 [initandlisten] build info: Linux ip-10-2-29-40 2.6.21.7-2.ec2.v1.2.fc8xen #1 SMP Fri Nov 20 17:48:28 EST 2009 x86_64 BOOST_LIB_VERSION=1_49\n    Tue Jun  3 20:40:57.457 [initandlisten] allocator: tcmalloc\n    Tue Jun  3 20:40:57.457 [initandlisten] options: { dbpath: “/srv/mongodb/” }\n    Tue Jun  3 20:40:57.520 [initandlisten] journal dir=/srv/mongodb/journal\n    Tue Jun  3 20:40:57.521 [initandlisten] recover : no journal files present, no recovery needed\n    Tue Jun  3 20:41:00.545 [initandlisten] preallocateIsFaster=true 36.86\n    Tue Jun  3 20:41:03.489 [initandlisten] preallocateIsFaster=true 35.06\n    Tue Jun  3 20:41:07.456 [initandlisten] preallocateIsFaster=true 34.44\n    Tue Jun  3 20:41:07.456 [initandlisten] preallocateIsFaster check took 9.935 secs\n    Tue Jun  3 20:41:07.456 [initandlisten] preallocating a journal file /srv/mongodb/journal/prealloc.0\n    Tue Jun  3 20:41:10.009 [initandlisten]         File Preallocator Progress: 985661440/1073741824    91%\n    Tue Jun  3 20:41:22.273 [initandlisten] preallocating a journal file /srv/mongodb/journal/prealloc.1\n    Tue Jun  3 20:41:25.009 [initandlisten]         File Preallocator Progress: 933232640/1073741824    86%\n    Tue Jun  3 20:41:37.119 [initandlisten] preallocating a journal file /srv/mongodb/journal/prealloc.2\n    Tue Jun  3 20:41:40.093 [initandlisten]         File Preallocator Progress: 1006632960/1073741824   93%\n    Tue Jun  3 20:41:52.450 [FileAllocator] allocating new datafile /srv/mongodb/local.ns, fillingwith zeroes…\n    Tue Jun  3 20:41:52.450 [FileAllocator] creating directory /srv/mongodb/_tmp\n    Tue Jun  3 20:41:52.503 [FileAllocator] done allocating datafile /srv/mongodb/local.ns, size: 16MB,  took0.022 secs\n    Tue Jun  3 20:41:52.517 [FileAllocator] allocating new datafile /srv/mongodb/local.0, filling with zeroes…\n    Tue Jun  3 20:41:52.537 [FileAllocator] done allocating datafile /srv/mongodb/local.0, size: 64MB,  took0.02 secs\n    Tue Jun  3 20:41:52.538 [websvr] admin web console waiting for connections on port 28017\n    Tue Jun  3 20:41:52.538 [initandlisten] waiting for connections on port 27017"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "install mongodb-10gen failed with apt-get",
      "explanation": "Install mongodb-10gen as\n\nhttps://docs.mongodb.org/manual/tutorial/install-mongodb-on-debian/but got error below:\n\ndpkg: error processing /var/cache/apt/archives/mongodb-10gen\\_2.2.0\\_amd64.deb (–unpack):\n\ntrying to overwrite ‘/usr/bin/mongoimport’, which is also in package mongodb-clients 1:1.4.4-3  \nconfigured to not write apport reports  \ndpkg-deb: subprocess paste killed by signal (Broken pipe)  \nErrors were encountered while processing:\n\n    /var/cache/apt/archives/mongodb-10gen_2.2.0_amd64.deb\n\n  \nE: Sub-process /usr/bin/dpkg returned an error code (1)  \nA bug here https://jira.mongodb.org/browse/SERVER-6910  \napt-get remove mongodb-clients first, and then the installation of mongodb-10gen should work."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Mongodb Explain for Aggregation framework",
      "explanation": "Is there an explain function for the Aggregation framework in MongoDB? I can’t see it in the documentation.  \nIf not is there some other way to check, how a query performs within the aggregation framework?  \nI know with find you just do  \ndb.collection.find().explain()  \nBut with the aggregation framework I get an error\n\n    Skip code block\n    db.collection.aggregate(\n    { $project : { “Tags._id” : 1 }},\n    { $unwind : “$Tags” },\n    { $match: {$or: [{“Tags._id”:”tag1?},{“Tags._id”:”tag2?}]}},\n    {\n    $group:\n    {\n    _id : { id: “$_id”},\n    “count”: { $sum:1 }\n    }\n    },\n    { $sort: {“count”:-1}}\n    ).explain()\n\n  \nThere isn’t an explain() feature for the Aggregation Framework as at MongoDB 2.2.0, however you should read up on the documentation regarding Optimizing Performance.  \nThe following pipeline operators take advantage of an index when they occur at the beginning of the pipeline:\n\n    $match $sort $limit $skip\n    \n\nThe above operators can also use an index when placed before the following aggregation operators:  \n$project $unwind $group  \nSo for your specific example, the pipeline will execute faster if you add an initial $match at the beginning to reduce the number of documents that need to be projected/unwound for the subsequent$match.  \nUpdate  \nMongoDB 2.6 (which is currently a Release Candidate) now includes an explain option for the aggregation pipeline.\n\n    Skip code block\n    db.collection.aggregate([\n    { $project : { “Tags._id” : 1 }},\n    { $unwind : “$Tags” },\n    { $match: {$or: [{“Tags._id”:”tag1?},{“Tags._id”:”tag2?}]}},\n    { $group: {\n    _id : “$_id”,\n    count: { $sum:1 }\n    }},\n    {$sort: {“count”:-1}}\n    ],\n    {\n    explain:true\n    }\n    )\n\n  \nStarting with version 2.6.x mongodb allows users to do explain with aggregation framework.  \nAll you need to do is to add explain : true\n\n    db.records.aggregate(\n    [ …your pipeline…],\n    { explain: true }\n    )\n\n  \nThanks to Rafa, I know that it was possible to do even in 2.4, but only through runCommand(). But now you can use aggregate as well.\n\n**Related Page: [MongoDB Aggregate](../../mongodb-aggregate)**"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Run script against replica set in MongoDB",
      "explanation": "I have replica set of 3 nodes and I want to run a cleanup script against it every end of day. What I would do if there was only single node would be a simple bash script:  \n~/mongo/bin/mongo host:port cleanupScript.js  \nBut since I want to run the same script against replica-set I can’t use this approach. I would need to somehow find which node is primary and run the script against that node.  \nSo the question: Is there a way how to run the script against whole replica set and let the mongoprocess pick the primary node and execute on it?  \nThanks!  \nThe mongo shell can connect directly to a replica set – this works with 2.4 (current), 2.2 (previous) and 2.0 (the version before that).  \nAssuming you have a replica set called myrs and your hosts are host1:27017, etc. use the following syntax:  \nmongo -host myrs/host1:27017  \nThe shell will figure out the rest, including connecting to the primary and if the primary steps down or goes away it will reconnect to the new primary once it’s elected.  \nUnfortunately, the ‘mongo’ shell does not support making connections to a replica set: only to individual nodes. You have two choices if you want to do this:  \n1\\. Use a different language, which supports making a connection to a replica set. (PHP, Python, Perl, Java and Ruby all support this.)  \n2\\. Have a driver script that runs an ‘rs.status()’ command, parses the output of that command, and determines the current primary. If it’s not the node you’ve connected to, then re-connect to the correct primary node  \nI wish I had a better answer for you.  \nI normally set priority for a node in replicaset. This gives me freedom to chose which node should get read and write load.  \nIn your case, I think, if you set priority for the nodes then you can always run your script against the highest priority node as that node will be primary almost all the time.  \nSetting priority is quite simple and straight forward. You can check this link https://docs.mongodb.org/manual/tutorial/force-member-to-be-primary/  \nI hope this will solve your problem.  \nOKAY…. Probably this is what you need..\n\n    #!/bin/bash\n    PRIMARY=`~/mongo/bin/mongo localhost:27017 –eval “printjson(rs.isMaster())” | grep “primary” | cut -d””” -f4`\n    echo “$PRIMARY”\n    ~/mongo/bin/mongo “$PRIMARY” cleanupScript.js\n\n  \nRun this on any node and it will give you the “server:port” of the primary. Give full path to the mongo executable.. just to avoid errors.  \nPS: the whole command is in the backticks. Somehow those are not visible so thought of telling you."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Couldn’t connect to server 127.0.0.1:27017",
      "explanation": "I’m getting the following error:\n\n    alex@alex-K43U:/$ mongo\n    MongoDB shell version: 2.2.0\n    connecting to: test\n    Thu Oct 11 11:46:53 Error: couldn’t connect to server 127.0.0.1:27017 src/mongo/shell/mongo.js:91\n    exception: connect failed\n    alex@alex-K43U:/$\n\n  \nThis is what happens when I try to start mongodb:  \n\\* Starting database mongodb                                             \\[fail\\]  \nI already tried mongo –repair  \nI made chown and chmod to var, lib, and data/db and log mongodb.  \nNot sure what else to do. Any suggestions?  \n**mongodb.log:**  \nSkip code block\n\n    ***** SERVER RESTARTED *****\n    Thu Oct 11 08:29:40\n    Thu Oct 11 08:29:40 warning: 32-bit servers don’t have journaling enabled by default. Please use –journal if you want durability.\n    Thu Oct 11 08:29:40\n    Thu Oct 11 08:29:41 [initandlisten] MongoDB starting : pid=1052 port=27017 dbpath=/var/lib/mongodb 32-bit host=alex-K43U\n    Thu Oct 11 08:29:41 [initandlisten]\n    Thu Oct 11 08:29:41 [initandlisten] ** NOTE: when using MongoDB 32 bit, you are limited to about 2 gigabytes of data\n    Thu Oct 11 08:29:41 [initandlisten] **       see https://blog.mongodb.org/post/137788967/32-bit-limitations\n    Thu Oct 11 08:29:41 [initandlisten] **       with –journal, the limit is lower\n    Thu Oct 11 08:29:41 [initandlisten]\n    Thu Oct 11 08:29:41 [initandlisten] db version v2.2.0, pdfile version 4.5\n    Thu Oct 11 08:29:41 [initandlisten] git version: f5e83eae9cfbec7fb7a071321928f00d1b0c5207\n    Thu Oct 11 08:29:41 [initandlisten] build info: Linux domU-12-31-39-01-70-B4 2.6.21.7-2.fc8xen #1 SMP Fri Feb 15 12:39:36 EST 2008 i686 BOOST_LIB_VERSION=1_49\n    Thu Oct 11 08:29:41 [initandlisten] options: { config: “/etc/mongodb.conf”, dbpath: “/var/lib/mongodb”, logappend: “true”, logpath: “/var/log/mongodb/mongodb.log” }\n    Thu Oct 11 08:29:41 [initandlisten] Unable to check for journal files due to: boost::filesystem::basic_directory_iterator constructor: No such file or directory: “/var/lib/mongodb/journal”\n    **************\n    Unclean shutdown detected.\n    Please visit https://dochub.mongodb.org/core/repair for recovery instructions.\n    *************\n    Thu Oct 11 08:29:41 [initandlisten] exception in initAndListen: 12596 old lock file, terminating\n    Thu Oct 11 08:29:41 dbexit:\n    Thu Oct 11 08:29:41 [initandlisten] shutdown: going to close listening sockets…\n    Thu Oct 11 08:29:41 [initandlisten] shutdown: going to flush diaglog…\n    Thu Oct 11 08:29:41 [initandlisten] shutdown: going to close sockets…\n    Thu Oct 11 08:29:41 [initandlisten] shutdown: waiting for fs preallocator…\n    Thu Oct 11 08:29:41 [initandlisten] shutdown: closing all files…\n    Thu Oct 11 08:29:41 [initandlisten] closeAllFiles() finished\n    Thu Oct 11 08:29:41 dbexit: really exiting now\n    EDIT:\n    I removed the lock then did mongod repair and got this error:\n    Thu Oct 11 12:05:37 [initandlisten] exception in initAndListen: 10309 Unable to create/openlock file: /data/db/mongod.lock errno:13 Permission denied Is a mongod instance already running?, terminating\n    so I did it with sudo:\n    Skip code block\n    alex@alex-K43U:~$ sudo mongod –repair\n    Thu Oct 11 12:05:42\n    Thu Oct 11 12:05:42 warning: 32-bit servers don’t have journaling enabled by default. Please use –journal if you want durability.\n    Thu Oct 11 12:05:42\n    Thu Oct 11 12:05:42 [initandlisten] MongoDB starting : pid=5129 port=27017 dbpath=/data/db/ 32-bit host=alex-K43U\n    Thu Oct 11 12:05:42 [initandlisten]\n    Thu Oct 11 12:05:42 [initandlisten] ** NOTE: when using MongoDB 32 bit, you are limited to about 2 gigabytes of data\n    Thu Oct 11 12:05:42 [initandlisten] **       see https://blog.mongodb.org/post/137788967/32-bit-limitations\n    Thu Oct 11 12:05:42 [initandlisten] **       with –journal, the limit is lower\n    Thu Oct 11 12:05:42 [initandlisten]\n    Thu Oct 11 12:05:42 [initandlisten] db version v2.2.0, pdfile version 4.5\n    Thu Oct 11 12:05:42 [initandlisten] git version: f5e83eae9cfbec7fb7a071321928f00d1b0c5207\n    Thu Oct 11 12:05:42 [initandlisten] build info: Linux domU-12-31-39-01-70-B4 2.6.21.7-2.fc8xen #1 SMP Fri Feb 15 12:39:36 EST 2008 i686 BOOST_LIB_VERSION=1_49\n    Thu Oct 11 12:05:42 [initandlisten] options: { repair: true }\n    Thu Oct 11 12:05:42 [initandlisten] Unable to check for journal files due to: boost::filesystem::basic_directory_iterator constructor: No such file or directory: “/data/db/journal”\n    Thu Oct 11 12:05:42 [initandlisten] finished checking dbs\n    Thu Oct 11 12:05:42 dbexit:\n    Thu Oct 11 12:05:42 [initandlisten] shutdown: going to close listening sockets…\n    Thu Oct 11 12:05:42 [initandlisten] shutdown: going to flush diaglog…\n    Thu Oct 11 12:05:42 [initandlisten] shutdown: going to close sockets…\n    Thu Oct 11 12:05:42 [initandlisten] shutdown: waiting for fs preallocator…\n    Thu Oct 11 12:05:42 [initandlisten] shutdown: closing all files…\n    Thu Oct 11 12:05:42 [initandlisten] closeAllFiles() finished\n    Thu Oct 11 12:05:42 [initandlisten] shutdown: removing fs lock…\n    Thu Oct 11 12:05:42 dbexit: really exiting now\n\n  \nBut still having the same problem.  \nThe log indicates that mongodb is terminating because there is an old lock file.  \nIf you are not and were not running with journaling, remove the lock file, run repair, and start mongodb again.  \nIf you are or were running with journaling turned on, see the relevant Mongo DB docs. Note that they say “If you are running with Journaling you should not do a repair to recover to a consistent state.” So if you were journaling, the repair may have made things worse.  \nSkip code block  \n**Step 1:** Remove lock file.  \nsudo rm /var/lib/mongodb/mongod.lock  \n**Step 2:** Repair mongodb.  \nmongod –repair  \n**Step 3:** start mongodb.  \nsudo start mongodb  \nor  \nsudo service mongodb start  \n**Step 4**: Check status of mongodb.  \nsudo status mongodb  \nor  \nsudo service mongodb status  \n**Step 5:** Start mongo console.  \nmongo  \nDid you run mongod before running mongo?  \nI followed installation instructions for mongodb from https://docs.mongodb.org/manual/tutorial/install-mongodb-on-os-x/ and I had the same error as you only when I ran mongo before actually running the mongo process with mongod. I thought installing mongodb would also launch it but you need to launch it manually with mongod before you do anything else that needs mongodb.  \ntry  \nsudo service mongodb start  \nI solved my problem by this"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Operate on all databases from the mongo shell",
      "explanation": "We have a system with many different mongo databases. I regularly want to write ad-hoc queries that will apply to all (or a subset) of them, without having a priori knowledge of what databases are there.  \nI can do show dbs, which will visually print a list, but is there a way to do something like:\n\n    var db_list = listDatabases();\n    for (i = 0; i < db_list.length; i++) {\n    do_something(db_list[i])\n    }\n    My problem with show dbs is that it doesn’t capture any return values, so I can’t do anything productive with the output.\n    You can use the ‘listDatabases’ admin command for that:\n    var db_list = db.adminCommand(‘listDatabases’);\n    That returns an object that looks like this:\n    Skip code block\n    {\n    “databases” : [\n    {\n    “name” : “test”,\n    “sizeOnDisk” : 2097152000,\n    “empty” : false\n    },\n    {\n    “name” : “local”,\n    “sizeOnDisk” : 1,\n    “empty” : true\n    }\n    ],\n    “totalSize” : 8487174144,\n    “ok” : 1\n    }"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "I accidentally named a collection “stats” in MongoDB and now cannot rename it",
      "explanation": "Oops.  \nI use mongoose, and accidentally created a collection “stats”. I didn’t realize this was going to be an issue until a few weeks later, so I now need to rename (rather than just delete) the collection…  \nHowever, my attempts have hit a predictable problem:\n\n    PRIMARY> db.stats.find();\n    \n\nThu Oct 18 10:39:43 TypeError: db.stats.find is not a function (shell):1\n\n    PRIMARY> db.stats.renameCollection(‘statssnapshots’);\n    Thu Oct 18 10:39:45 TypeError: db.stats.renameCollection is not a function (shell):1\n    Try db[“stats”].find() and db[“stats”].renameCollection(‘statssnapshots’).\n    You could also do db.getCollection(“stats”).find()."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Set max on mongo capped collection",
      "explanation": "I have an existing collection that I need to convert into a Capped Collection using the method listed:\n\n    > db.runCommand({“convertToCapped”: “mycoll”, size: 100000});\n    However, the max field is not accepted as a parameter\n    > db.mycol1.convertToCapped\n    function (bytes) {\n    if (!bytes) {\n    throw “have to specify # of bytes”;\n    }\n    return this._dbCommand({convertToCapped: this._shortName, size: bytes});\n    }\n\n  \nAny idea how to set this?  \nmax is only an option in the createCollection method, not convertToCapped:  \ndb.createCollection(“mycoll”, {capped:true, size:100000, max:100});  \nThere’s a cloneCollectionAsCapped, but it doesn’t look like there’s a max doc option there either:\n\nhttps://docs.mongodb.org/manual/reference/command/cloneCollectionAsCapped/\n\n  \nYou may need to create a new capped collection with the max parameter and transfer data and indices from the existing collection. See https://learnmongo.com/posts/easily-move-documents-between-collections-or-databases/"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Does MongoDB reuse deleted space?",
      "explanation": "First off, I know about this question:  \nAuto compact the deleted space in mongodb?  \nMy question is not about shrinking DB file sizes though, but more about the reuse of deleted space. Say I have 100K documents in a collection, I then delete 50K of those. Will Mongo reuse the space within its data file that the deleted documents have freed? Or are they simply “marked” as deleted?  \nI don’t care so much about the actual size of the file on disk, its more about “does it just grow and grow”.  \nWhen documents are deleted the space left behind is put into a free list, yes. However, to use the space there will need to be similarly sized documents inserted later, and MongoDB will need to find an appropriate space for that document within a certain time frame (once it times out looking at the list, it will just append) otherwise the space re-use is not going to happen very often. This deletion is done within the data files, so there is no disk space reclamation happening here – all of this is done internally within the existing data files.  \nIf you subsequently do a repair, or resync a secondary from scratch, the data files are rewritten and the space on disk will be reclaimed (any padding on docs is also removed). This is where you will see actual space reclamation on-disk. For any other actions (compact included) the on disk usage will not change and may even increase.  \nWith 2.2 you can now use the collMod command and the use PowersOf2Sizes option to make the re-use of deleted space more likely. This means that the initial space allocation for a document is a bit less efficient (512 bytes for a 400 byte doc for example) but means that when a new doc is inserted it is more likely to be able to re-use that space. If you are deleting (or growing and hence moving) documents a lot, then this will be more efficient in the long term.  \nFor anyone that is interested, one of the people that wrote a lot of the storage code (Mathias Stearn) has a great presentation about the storage internals, which can be found here"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Can not delete collection from mongodb",
      "explanation": "Can not delete the collection from the shell,  \nThe thing that the collection is available and my php script is accessing it (selecting|updating)  \nBut when I used:  \ndb.\\_registration.drop()  \nit gives me an error:  \nDate, JS Error: TypeErrorL db.\\_registration has no properties (shell): 1  \nThe problem is not with deleting the collection. The problem is with accessing the collection. So you would not be able to update, find or do anything with it from the shell. As it was pointed in mongodb JIRA, this is a bug when a collection has characters like \\_, – or .  \nNevertheless this type of names for collections is acceptable, but it cause a problem in shell.  \nYou can delete it in shell with this command:\n\n    db.getCollection(“_registration”).drop()\n\n  \nbut I would rather rename it (of course if it is possible and will not end up with a lot of changing)."
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "explain() in Mongodb: differences between “nscanned” and “nscannedObjects”",
      "explanation": "I cannot get the exact difference between “nscanned” and “nscannedObjects” in the Mongodb’s explain query output.  \nOn MongoDB Explain documentation I can read:  \n**nscanned** Number of items (documents or index entries) examined. Items might be objects or index keys. If a “covered index” is involved, nscanned may be higher than nscannedObjects.  \n**nscannedObjects** Number of documents scanned.  \nWhat’s the different between these two fields? And more specific what does exactly mean when I have a query, which uses a BtreeCursor (an index), and these two fields have two different \n\n    values, for example:\n    {\n    “cursor” : “BtreeCursor a_1_b_1”,\n    “isMultiKey” : false,\n    “n” : 5,\n    “nscannedObjects” : 5,\n    “nscanned” : 9,\n    (…)\n    }\n\n  \nI know what a “covered index” is. I would like to understand exactly what the query did in the example above. Did it pass through (“scanned”) 9 elements (nscanned = 9), where all of them are index entries and read (“examined”) the value of only 5 of them (nscannedObjects = 5) to produce the result set?  \nThis means that :  \nThe query returned 5 documents – n  \nscanned 9 documents from the index – nscanned  \nand then read 5 full documents from the collection – nscannedObjects  \nSimilar example is given at :\n\n  \nhttps://docs.mongodb.org/manual/core/read-operations/#measuring-index-use"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDb Database vs Collection",
      "explanation": "I am designing a system with MongoDb (64 bit version) to handle a large amount of users (around 100,000) and each user will have large amounts of data (around 1 million records).  \nWhat is the best strategy of design?  \nDump all records in single collection  \nHave a collection for each user  \nHave a database for each user.  \nMany Thanks,  \nSo you’re looking at somewhere in the region of 100 billion records (1 million records \\* 100,000 users).  \nThe preferred way to deal with large amounts of data is to create a sharded cluster that splits the data out over several servers that are presented as single logical unit via the mongo client.  \nTherefore the answer to your question is put all your records in a single sharded collection.  \nThe number of shards required and configuration of the cluster is related to the size of the data and other factors such as the quantity and distribution of reads and writes. The answers to those questions are probably very specific to your unique situation, so I won’t attempt to guess them.  \nI’d probably start by deciding how many shards you have the time and machines available to set up and testing the system on a cluster of that many machines. Based on the performance of that, you can decide whether you need more or fewer shards in your cluster"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "MongoDB upsert – insert or update",
      "explanation": "I can’t find this in the documentation in any of the obvious places. I’d like to know if it is possible to know if mongo executed an insert or update in the upsert operation? Thanks.  \nYes there is, on a safe call (or getLastError) the update function will return an array with an upsert field and a updatedExisting field.  \nYou can read the PHP version of this here:\n\n**https://php.net/manual/en/mongocollection.insert.phptowards**\n\nthe bottom.  \nAs it says within the documentation on upserted:  \nIf an upsert occured, this field will contain the new record’s \\_id field. For upserts, either this field or updatedExisting will be present (unless an error occurred).  \nSo upserted contains the \\_id of the new record if a insert was done or it will incrementupdatedExisting if it updated a record.  \nI am sure a similar thing appears in all drivers.  \n**Edit**  \nIt will actually be a boolean in the updatedExisting field of true or false  \nFor reference only, in node.js:\n\n    collection.update( source, target, { upsert: true }, function(err, result, upserted) {\n    …\n    });\n    For reference only, in node.js using Mongoose 3.6:\n    model.update( findquery, updatequery, { upsert: true }, function(err, numberAffected, rawResponse) {\n    …\n    });\n    Where rawResponse looks like this when it has updated an existing document:\n    { updatedExisting: true,\n    n: 1,\n    connectionId: 222,\n    err: null,\n    ok: 1 }\n    And it looks like this when it has created a new document:\n    { updatedExisting: false,\n    upserted: 51eebc080eb3e2208a630d8e,\n    n: 1,\n    connectionId: 222,\n    err: null,\n    (Both cases would return numberAffected = 1)"
    },
    {
      "type": "LONG",
      "source": "https://mindmajix.com/mongodb-interview-questions",
      "statement": "Fastest way to remove duplicate documents in mongodb",
      "explanation": "I have approximately 1.7M documents in mongodb (in future 10m+). Some of them represent duplicate entry which I do not want. Structure of document is something like this:\n\n    {\n    _id: 14124412,\n    nodes: [\n    12345,\n    54321\n    ],\n    name: “Some beauty”\n    }\n\n  \nDocument is duplicate if it has at **least one node same** as another document with **same name**. What is the fastest way to remove duplicates?  \nAssuming you want to permanently delete docs that contain a duplicate name + nodes entry from the collection, you can add a unique index with the dropDups: true option:  \ndb.test.ensureIndex({name: 1, nodes: 1}, {unique: true, dropDups: true})  \nAs the docs say, use extreme caution with this as it will delete data from your database. Back up your database first in case it doesn’t do exactly as you’re expecting."
    }
  ]
}
